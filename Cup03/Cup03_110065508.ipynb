{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efbf59f1",
   "metadata": {},
   "source": [
    "# Cup03 Reverse Image Caption\n",
    "110065508 李丞恩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6877f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL)\n",
    "dictionary_path = 'dictionary'\n",
    "data_path = 'dataset'\n",
    "BATCH_SIZE = 64\n",
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LR': 2e-5,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 600,\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 1                           # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5f5eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 23:04:18.695070: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.applications.resnet import ResNet101, ResNet50\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, \\\n",
    "    BatchNormalization, Input, Conv2D, MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abab4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable warnings, info and errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c87e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 23:04:22.275796: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-12-28 23:04:22.329104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:04:22.329572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.852GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2021-12-28 23:04:22.329592: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-12-28 23:04:22.339885: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-12-28 23:04:22.339915: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-12-28 23:04:22.345096: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-12-28 23:04:22.347143: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-12-28 23:04:22.350555: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-12-28 23:04:22.354138: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-12-28 23:04:22.354395: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-12-28 23:04:22.354541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:04:22.355941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:04:22.357416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-12-28 23:04:22.358974: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-28 23:04:22.359904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:04:22.360792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.852GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2021-12-28 23:04:22.360831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:04:22.361267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:04:22.361665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-12-28 23:04:22.361924: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-12-28 23:04:22.923649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-12-28 23:04:22.923672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-12-28 23:04:22.923676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-12-28 23:04:22.923815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:04:22.924260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:04:22.924678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:04:22.925084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9476 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0520a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('samples/demo'): os.makedirs('samples/demo')\n",
    "if not os.path.exists('checkpoints/demo'): os.makedirs('checkpoints/demo')\n",
    "if not os.path.exists('inference/demo'): os.makedirs('inference/demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a454df",
   "metadata": {},
   "source": [
    "## 一. 資料前處理\n",
    "### 1. 文字前處理\n",
    "感謝助教已經幫我們處理好了以下的部分：\n",
    "\n",
    "1. Delete text over MAX_SEQ_LENGTH (20).\n",
    "2. Delete all puntuation in the texts.\n",
    "3. Encode each vocabulary in dictionary/vocab.npy.\n",
    "4. Represent texts by a sequence of integer IDs.\n",
    "5. Replace rare words by $<$RARE$>$ token to reduce vocabulary size for more efficient training.\n",
    "6. Add padding as $<$PAD$>$ to each text to make sure all of them have equal length to MAX_SEQ_LENGTH (20).\n",
    "    \n",
    "There is no necessary to append $<$ST$>$ and $<$ED$>$ to each text because we don't need to generate any sequence in this task.\n",
    "    \n",
    "We can decode sequence vocabulary IDs by looking up the vocabulary dictionary:\n",
    "\n",
    "1. dictionary/word2Id.npy is a numpy array mapping word to id.\n",
    "2. dictionary/id2Word.npy is a numpy array mapping id back to word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe0d1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8071352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77327296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids2textlist(index_lists):\n",
    "    result = []\n",
    "    for index_list in index_lists:\n",
    "        text = ''\n",
    "        for idx in index_list:\n",
    "            if idx == word2Id_dict['<PAD>']:\n",
    "                break\n",
    "            elif idx == word2Id_dict['<RARE>']:\n",
    "                pass\n",
    "            else:\n",
    "                text += id2word_dict[idx] + ' '\n",
    "        if text != '':\n",
    "            result.append(text.strip())\n",
    "    return result\n",
    "\n",
    "def ids2text(index):\n",
    "    text = ''\n",
    "    for idx in index:\n",
    "        if idx == word2Id_dict['<PAD>']:\n",
    "            break\n",
    "        elif idx == word2Id_dict['<RARE>']:\n",
    "            pass\n",
    "        else:\n",
    "            text += id2word_dict[idx] + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc8329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06649728",
   "metadata": {},
   "source": [
    "### 2. 將圖片與文字對應"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6582b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(data_path + '/text2img_cls_embedding.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "458faf5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>texts</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "      <td>[[0.18257322907447815, 0.7088410258293152, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "      <td>[[0.1587948352098465, 0.7034167051315308, 0.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "      <td>[[0.18289019167423248, 0.7226691246032715, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "      <td>[[0.17855443060398102, 0.7165486812591553, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "      <td>[[0.17934608459472656, 0.7307049632072449, 0.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \\\n",
       "ID                                   \n",
       "6734  ./102flowers/image_06734.jpg   \n",
       "6736  ./102flowers/image_06736.jpg   \n",
       "6737  ./102flowers/image_06737.jpg   \n",
       "6738  ./102flowers/image_06738.jpg   \n",
       "6739  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                                  texts  \\\n",
       "ID                                                        \n",
       "6734  [the petals of the flower are pink in color an...   \n",
       "6736  [this flower has white petals and yellow pisti...   \n",
       "6737  [the petals on this flower are pink with white...   \n",
       "6738  [the flower has a smooth purple petal with whi...   \n",
       "6739  [this white flower has bright yellow stamen wi...   \n",
       "\n",
       "                                             embeddings  \n",
       "ID                                                       \n",
       "6734  [[0.18257322907447815, 0.7088410258293152, 0.4...  \n",
       "6736  [[0.1587948352098465, 0.7034167051315308, 0.48...  \n",
       "6737  [[0.18289019167423248, 0.7226691246032715, 0.4...  \n",
       "6738  [[0.17855443060398102, 0.7165486812591553, 0.4...  \n",
       "6739  [[0.17934608459472656, 0.7307049632072449, 0.4...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5165edfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7370"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df['embeddings'].values\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f7ccf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7370"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df['Captions'].values\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0881f3f",
   "metadata": {},
   "source": [
    "### 3. 生成dataset\n",
    "in this competition, you have to generate image in size 64x64x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90718b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "def training_data_generator(caption, image_path, embedding):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "    return img, caption, embedding\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "    # load the training data into two NumPy arrays\n",
    "    df = pd.read_pickle(filenames)\n",
    "    embeddings = df['embeddings'].values\n",
    "    captions = df['Captions'].values\n",
    "    caption = []\n",
    "    embedding = []\n",
    "    # each image has 1 to 10 corresponding captions\n",
    "    # we choose one of them randomly for training\n",
    "\n",
    "    for i in range(len(captions)):\n",
    "        assert len(captions[i]) == len(embeddings[i])\n",
    "        rand_idx = randrange(len(captions[i]))\n",
    "        print(rand_idx)\n",
    "#         caption.append(random.choice(captions[i]))\n",
    "        caption.append(captions[i][rand_idx])\n",
    "        embedding.append(embeddings[i][rand_idx])\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    embedding = np.array(embedding)\n",
    "    image_path = df['ImagePath'].values\n",
    "    \n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path, embedding))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f728ee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "2\n",
      "6\n",
      "8\n",
      "6\n",
      "2\n",
      "2\n",
      "8\n",
      "3\n",
      "7\n",
      "1\n",
      "8\n",
      "6\n",
      "8\n",
      "7\n",
      "0\n",
      "5\n",
      "8\n",
      "2\n",
      "8\n",
      "7\n",
      "6\n",
      "6\n",
      "8\n",
      "3\n",
      "7\n",
      "3\n",
      "6\n",
      "9\n",
      "2\n",
      "5\n",
      "0\n",
      "9\n",
      "6\n",
      "8\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "7\n",
      "4\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "8\n",
      "6\n",
      "7\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "6\n",
      "6\n",
      "8\n",
      "6\n",
      "9\n",
      "4\n",
      "0\n",
      "0\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "7\n",
      "0\n",
      "2\n",
      "8\n",
      "9\n",
      "3\n",
      "2\n",
      "9\n",
      "7\n",
      "6\n",
      "1\n",
      "6\n",
      "6\n",
      "0\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "5\n",
      "6\n",
      "0\n",
      "7\n",
      "8\n",
      "5\n",
      "4\n",
      "9\n",
      "3\n",
      "4\n",
      "2\n",
      "7\n",
      "6\n",
      "0\n",
      "6\n",
      "0\n",
      "2\n",
      "7\n",
      "3\n",
      "8\n",
      "1\n",
      "0\n",
      "2\n",
      "9\n",
      "1\n",
      "9\n",
      "1\n",
      "7\n",
      "8\n",
      "8\n",
      "7\n",
      "7\n",
      "4\n",
      "2\n",
      "9\n",
      "5\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "7\n",
      "7\n",
      "1\n",
      "4\n",
      "0\n",
      "9\n",
      "4\n",
      "5\n",
      "9\n",
      "0\n",
      "0\n",
      "3\n",
      "4\n",
      "6\n",
      "7\n",
      "4\n",
      "7\n",
      "5\n",
      "2\n",
      "1\n",
      "4\n",
      "4\n",
      "4\n",
      "0\n",
      "2\n",
      "2\n",
      "7\n",
      "7\n",
      "2\n",
      "9\n",
      "9\n",
      "0\n",
      "4\n",
      "6\n",
      "6\n",
      "1\n",
      "7\n",
      "2\n",
      "6\n",
      "7\n",
      "5\n",
      "8\n",
      "8\n",
      "8\n",
      "3\n",
      "5\n",
      "8\n",
      "2\n",
      "4\n",
      "3\n",
      "0\n",
      "2\n",
      "5\n",
      "3\n",
      "1\n",
      "4\n",
      "8\n",
      "9\n",
      "2\n",
      "7\n",
      "6\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "5\n",
      "4\n",
      "1\n",
      "1\n",
      "5\n",
      "4\n",
      "9\n",
      "6\n",
      "5\n",
      "7\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n",
      "5\n",
      "1\n",
      "4\n",
      "1\n",
      "7\n",
      "3\n",
      "6\n",
      "7\n",
      "7\n",
      "4\n",
      "7\n",
      "2\n",
      "4\n",
      "0\n",
      "4\n",
      "0\n",
      "7\n",
      "6\n",
      "0\n",
      "7\n",
      "3\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "6\n",
      "3\n",
      "2\n",
      "7\n",
      "3\n",
      "2\n",
      "8\n",
      "7\n",
      "4\n",
      "8\n",
      "2\n",
      "6\n",
      "8\n",
      "3\n",
      "1\n",
      "1\n",
      "8\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "8\n",
      "3\n",
      "5\n",
      "1\n",
      "1\n",
      "9\n",
      "0\n",
      "6\n",
      "4\n",
      "1\n",
      "1\n",
      "6\n",
      "4\n",
      "0\n",
      "3\n",
      "7\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "8\n",
      "6\n",
      "3\n",
      "3\n",
      "7\n",
      "8\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "7\n",
      "4\n",
      "2\n",
      "0\n",
      "8\n",
      "8\n",
      "7\n",
      "9\n",
      "8\n",
      "8\n",
      "1\n",
      "3\n",
      "9\n",
      "7\n",
      "3\n",
      "7\n",
      "4\n",
      "9\n",
      "7\n",
      "9\n",
      "6\n",
      "7\n",
      "0\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "5\n",
      "7\n",
      "4\n",
      "9\n",
      "1\n",
      "0\n",
      "6\n",
      "7\n",
      "2\n",
      "7\n",
      "6\n",
      "5\n",
      "1\n",
      "1\n",
      "0\n",
      "9\n",
      "8\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "2\n",
      "4\n",
      "3\n",
      "8\n",
      "1\n",
      "0\n",
      "4\n",
      "3\n",
      "9\n",
      "6\n",
      "4\n",
      "3\n",
      "8\n",
      "1\n",
      "0\n",
      "6\n",
      "8\n",
      "3\n",
      "6\n",
      "8\n",
      "2\n",
      "4\n",
      "5\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "8\n",
      "7\n",
      "5\n",
      "4\n",
      "1\n",
      "9\n",
      "9\n",
      "3\n",
      "1\n",
      "8\n",
      "8\n",
      "9\n",
      "3\n",
      "8\n",
      "1\n",
      "9\n",
      "6\n",
      "1\n",
      "8\n",
      "1\n",
      "7\n",
      "0\n",
      "6\n",
      "6\n",
      "5\n",
      "7\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "8\n",
      "6\n",
      "0\n",
      "7\n",
      "0\n",
      "2\n",
      "1\n",
      "8\n",
      "2\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "8\n",
      "8\n",
      "9\n",
      "8\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "7\n",
      "9\n",
      "3\n",
      "1\n",
      "6\n",
      "8\n",
      "3\n",
      "6\n",
      "2\n",
      "4\n",
      "8\n",
      "5\n",
      "7\n",
      "6\n",
      "7\n",
      "1\n",
      "7\n",
      "7\n",
      "3\n",
      "2\n",
      "8\n",
      "6\n",
      "7\n",
      "1\n",
      "7\n",
      "2\n",
      "7\n",
      "5\n",
      "1\n",
      "3\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "2\n",
      "5\n",
      "1\n",
      "9\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "1\n",
      "1\n",
      "1\n",
      "9\n",
      "7\n",
      "6\n",
      "7\n",
      "6\n",
      "3\n",
      "7\n",
      "4\n",
      "4\n",
      "2\n",
      "8\n",
      "1\n",
      "5\n",
      "0\n",
      "6\n",
      "3\n",
      "1\n",
      "7\n",
      "3\n",
      "3\n",
      "4\n",
      "1\n",
      "3\n",
      "2\n",
      "4\n",
      "5\n",
      "2\n",
      "0\n",
      "9\n",
      "7\n",
      "3\n",
      "5\n",
      "8\n",
      "6\n",
      "3\n",
      "4\n",
      "8\n",
      "4\n",
      "4\n",
      "3\n",
      "9\n",
      "9\n",
      "5\n",
      "1\n",
      "4\n",
      "7\n",
      "6\n",
      "4\n",
      "0\n",
      "7\n",
      "8\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "5\n",
      "2\n",
      "9\n",
      "1\n",
      "9\n",
      "2\n",
      "8\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "9\n",
      "0\n",
      "6\n",
      "5\n",
      "1\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "8\n",
      "8\n",
      "5\n",
      "0\n",
      "8\n",
      "7\n",
      "7\n",
      "7\n",
      "4\n",
      "9\n",
      "7\n",
      "8\n",
      "2\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "7\n",
      "2\n",
      "7\n",
      "5\n",
      "4\n",
      "6\n",
      "0\n",
      "7\n",
      "1\n",
      "4\n",
      "1\n",
      "9\n",
      "6\n",
      "1\n",
      "1\n",
      "4\n",
      "6\n",
      "2\n",
      "7\n",
      "2\n",
      "5\n",
      "7\n",
      "1\n",
      "3\n",
      "9\n",
      "0\n",
      "3\n",
      "3\n",
      "1\n",
      "4\n",
      "4\n",
      "2\n",
      "5\n",
      "8\n",
      "4\n",
      "5\n",
      "7\n",
      "1\n",
      "2\n",
      "8\n",
      "4\n",
      "9\n",
      "4\n",
      "7\n",
      "3\n",
      "7\n",
      "3\n",
      "1\n",
      "6\n",
      "5\n",
      "0\n",
      "6\n",
      "0\n",
      "5\n",
      "6\n",
      "3\n",
      "9\n",
      "0\n",
      "4\n",
      "1\n",
      "0\n",
      "4\n",
      "7\n",
      "1\n",
      "3\n",
      "6\n",
      "7\n",
      "0\n",
      "1\n",
      "1\n",
      "8\n",
      "6\n",
      "5\n",
      "6\n",
      "5\n",
      "3\n",
      "7\n",
      "0\n",
      "0\n",
      "4\n",
      "8\n",
      "1\n",
      "7\n",
      "2\n",
      "6\n",
      "8\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "8\n",
      "0\n",
      "7\n",
      "1\n",
      "3\n",
      "8\n",
      "0\n",
      "3\n",
      "0\n",
      "8\n",
      "8\n",
      "8\n",
      "2\n",
      "2\n",
      "8\n",
      "9\n",
      "8\n",
      "3\n",
      "7\n",
      "4\n",
      "1\n",
      "6\n",
      "4\n",
      "3\n",
      "1\n",
      "8\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "6\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "7\n",
      "3\n",
      "3\n",
      "7\n",
      "1\n",
      "6\n",
      "8\n",
      "3\n",
      "1\n",
      "2\n",
      "6\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "4\n",
      "1\n",
      "7\n",
      "2\n",
      "6\n",
      "4\n",
      "9\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "5\n",
      "7\n",
      "8\n",
      "4\n",
      "7\n",
      "9\n",
      "4\n",
      "1\n",
      "3\n",
      "8\n",
      "3\n",
      "4\n",
      "1\n",
      "7\n",
      "8\n",
      "2\n",
      "8\n",
      "2\n",
      "4\n",
      "4\n",
      "2\n",
      "7\n",
      "9\n",
      "7\n",
      "4\n",
      "0\n",
      "5\n",
      "3\n",
      "9\n",
      "7\n",
      "7\n",
      "4\n",
      "6\n",
      "9\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "7\n",
      "8\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "8\n",
      "7\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "6\n",
      "2\n",
      "8\n",
      "6\n",
      "3\n",
      "6\n",
      "7\n",
      "3\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_984697/1083829426.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/text2img_cls_embedding.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_984697/4049773159.py\u001b[0m in \u001b[0;36mdataset_generator\u001b[0;34m(filenames, batch_size, data_generator)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mrand_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = dataset_generator(data_path + '/text2img_cls_embedding.pkl', BATCH_SIZE, training_data_generator)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a92ee",
   "metadata": {},
   "source": [
    "## 二. Conditional GAN Model\n",
    "### 1. Text Encoder\n",
    "A RNN encoder that captures the meaning of input text.\n",
    "1. Input: text, which is a list of ids.\n",
    "2. Output: embedding, or hidden representation of input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5170e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "        \n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(self.hparas['RNN_HIDDEN_SIZE'],\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, text, hidden):\n",
    "        text = self.embedding(text)\n",
    "        output, state = self.gru(text, initial_state = hidden)\n",
    "        return output[:, -1, :], state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas['RNN_HIDDEN_SIZE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3452d8",
   "metadata": {},
   "source": [
    "### 2. Generator\n",
    "A image generator which generates the target image illustrating the input text.\n",
    "\n",
    "1. Input: hidden representation of input text and random noise z with random seed.\n",
    "2. Output: target image, which is conditioned on the given text, in size 64x64x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten1 = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d2 = tf.keras.layers.Dense(64*3)\n",
    "        self.resnet101 = ResNet101(include_top=False, \n",
    "                                   weights='imagenet', \n",
    "                                   input_tensor=None, \n",
    "                                   input_shape=INPUT_SHAPE)\n",
    "        self.flatten2 = tf.keras.layers.Flatten()\n",
    "        self.d3 = tf.keras.layers.Dense(64*64*3)\n",
    "        \n",
    "    def call(self, text, noise_z):\n",
    "        text = self.flatten1(text)\n",
    "        text = self.d1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        # concatenate input text and random noise\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        text_concat = self.d2(text_concat)\n",
    "        text_concat = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "        text_concat = self.resnet101(text_concat)\n",
    "        text_concat = self.flatten1(text_concat)\n",
    "        text_concat = self.d3(text_concat)\n",
    "        logits = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "        output = tf.nn.tanh(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154aa9c",
   "metadata": {},
   "source": [
    "### 3. Discriminator\n",
    "A binary classifier which can discriminate the real and fake image:\n",
    "\n",
    "1. Real image\n",
    "\n",
    "    Input: real image and the paired text\n",
    "    \n",
    "    Output: a floating number representing the result, which is expected to be 1.\n",
    "    \n",
    "2. Fake Image\n",
    "\n",
    "    Input: generated image and paired text\n",
    "    \n",
    "    Output: a floating number representing the result, which is expected to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        \n",
    "        # text\n",
    "        self.flatten_text = tf.keras.layers.Flatten()\n",
    "        self.d_text = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        \n",
    "        # image\n",
    "        self.resnet50 = ResNet50(include_top=False, \n",
    "                                   weights='imagenet', \n",
    "                                   input_tensor=None, \n",
    "                                   input_shape=INPUT_SHAPE)\n",
    "        self.flatten_img = tf.keras.layers.Flatten()\n",
    "        self.d_img = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        \n",
    "        # concat\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, img, text):\n",
    "        text = self.flatten_text(text)\n",
    "        text = self.d_text(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        img = self.resnet50(img)\n",
    "        img = self.flatten_img(img)\n",
    "        img = self.d_img(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        \n",
    "        # concatenate image with paired text\n",
    "        img_text = tf.concat([text, img], axis=0)\n",
    "        \n",
    "        logits = self.d(img_text)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb53cf",
   "metadata": {},
   "source": [
    "### 4. 組裝Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377aeff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas['N_SAMPLE'] = num_training_sample # size of training data\n",
    "hparas['VOCAB_SIZE'] = len(word2Id_dict) # size of dictionary of captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b38ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(hparas)\n",
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910c8ca",
   "metadata": {},
   "source": [
    "## 三. Conditional GAN的訓練設定\n",
    "### 1. Loss Function\n",
    "This method returns a helper function to compute cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ad19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # output value of real image should be 1\n",
    "    real_loss = cross_entropy(tf.ones_like(real_logits), real_logits)\n",
    "    # output value of fake image should be 0\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_logits), fake_logits)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7bafa",
   "metadata": {},
   "source": [
    "### 2. Optimization\n",
    "we use seperated optimizers for training generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978746cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb361625",
   "metadata": {},
   "source": [
    "### 3. checkpoint\n",
    "one benefit of tf.train.Checkpoint() API is we can save everything seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 text_encoder=text_encoder,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ef974",
   "metadata": {},
   "source": [
    "### 4. 定義訓練函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee86c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, caption, hidden):\n",
    "    # random noise for generator\n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        text_embed, hidden = text_encoder(caption, hidden)\n",
    "        _, fake_image = generator(text_embed, noise)\n",
    "        real_logits, real_output = discriminator(real_image, text_embed)\n",
    "        fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "        g_loss = generator_loss(fake_logits)\n",
    "        d_loss = discriminator_loss(real_logits, fake_logits)\n",
    "\n",
    "    grad_g = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    grad_d = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(grad_d, discriminator.trainable_variables))\n",
    "    \n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7bdf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption, noise, hidden):\n",
    "    text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(text_embed, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18782549",
   "metadata": {},
   "source": [
    "## 五. 訓練Conditional GAN\n",
    "### 1. 視覺化訓練過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bf2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4620aae",
   "metadata": {},
   "source": [
    "### 2. Random seed設定\n",
    "We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5709e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2IdList(sent)\n",
    "sample_sentence = sample_generator(sample_sentence, hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d6b3d",
   "metadata": {},
   "source": [
    "### 3. 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c34b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    hidden = text_encoder.initialize_hidden_state() # hidden state of RNN\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    for epoch in tqdm(range(hparas['N_EPOCH'])):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        for image, caption in dataset:\n",
    "            g_loss, d_loss = train_step(image, caption, hidden)\n",
    "            g_total_loss += g_loss\n",
    "            d_total_loss += d_loss\n",
    "            \n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "            \n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        \n",
    "        # save the model\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            for caption in sample_sentence:\n",
    "                fake_image = test_step(caption, sample_seed, hidden)\n",
    "            save_images(fake_image, [ni, ni], 'samples/demo/train_{:02d}.jpg'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f31c34",
   "metadata": {},
   "source": [
    "## 六. Evaluation\n",
    "dataset/testData.pkl is a pandas dataframe containing testing text with attributes 'ID' and 'Captions'.\n",
    "1. 'ID': text ID used to name generated image.\n",
    "2. 'Captions': text used as condition to generate image.\n",
    "For each captions, you need to generate inference_ID.png to evaluate quality of generated image. You must name the generated image in this format, otherwise we cannot evaluate your images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a657c63",
   "metadata": {},
   "source": [
    "### 1. Testing Dataset\n",
    "If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption, index):\n",
    "    caption = tf.cast(caption, tf.float32)\n",
    "    return caption, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "    data = pd.read_pickle('./dataset/testData_cls_embedding.pkl')\n",
    "    captions = data['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8db92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData_cls_embedding.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614e75d4",
   "metadata": {},
   "source": [
    "### 2. Inferece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for captions, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(captions, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave('./inference/demo/inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy()*0.5 + 0.5)\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(checkpoint_dir + '/ckpt-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d5a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6310339d",
   "metadata": {},
   "source": [
    "### 3. 計算Inception Score & Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ./testing/inception_score.py ./inference/demo ./score_team2.csv 39"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
