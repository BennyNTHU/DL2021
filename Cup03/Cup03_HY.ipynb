{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b44db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 23:32:23.079284: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable warnings, info and errors \n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "# config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7  #占用85%显存\n",
    "# session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import imageio\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c8a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def utPuzzle(imgs, row, col, path=None):\n",
    "    h, w, c = imgs[0].shape\n",
    "    out = np.zeros((h * row, w * col, c), np.uint8)\n",
    "    for n, img in enumerate(imgs):\n",
    "        j, i = divmod(n, col)\n",
    "        out[j * h : (j + 1) * h, i * w : (i + 1) * w, :] = img\n",
    "    if path is not None : imageio.imwrite(path, out)\n",
    "    return out\n",
    "  \n",
    "def utMakeGif(imgs, fname, duration):\n",
    "    n = float(len(imgs)) / duration\n",
    "    clip = mpy.VideoClip(lambda t : imgs[int(n * t)], duration = duration)\n",
    "    clip.write_gif(fname, fps = n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a56134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 23:32:24.759735: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-12-28 23:32:24.829675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:32:24.831910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.852GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2021-12-28 23:32:24.831982: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-12-28 23:32:24.850149: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-12-28 23:32:24.850271: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-12-28 23:32:24.857561: I tensorflow/stream_execut"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "or/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-12-28 23:32:24.860550: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-12-28 23:32:24.864789: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-12-28 23:32:24.868573: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-12-28 23:32:24.869668: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-12-28 23:32:24.869810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:32:24.871063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:32:24.872453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-12-28 23:32:24.874145: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-28 23:32:24.874889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:32:24.875359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.852GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2021-12-28 23:32:24.875427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:32:24.875871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:32:24.876300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-12-28 23:32:24.876579: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-12-28 23:32:25.453250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-12-28 23:32:25.453278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-12-28 23:32:25.453282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-12-28 23:32:25.453559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:32:25.454069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:32:25.454502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-28 23:32:25.454920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9476 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279350f",
   "metadata": {},
   "source": [
    "## Preprocess Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694b7f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ddfc3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52e9c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "# df = pd.read_pickle(data_path+ '/testData.pkl' )\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2807a9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29af59f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids2textlist(index_lists):\n",
    "    result = []\n",
    "    for index_list in index_lists:\n",
    "        text = ''\n",
    "        for idx in index_list:\n",
    "            if idx == word2Id_dict['<PAD>']:\n",
    "                break\n",
    "            elif idx == word2Id_dict['<RARE>']:\n",
    "                pass\n",
    "            else:\n",
    "                text += id2word_dict[idx] + ' '\n",
    "        if text != '':\n",
    "            result.append(text.strip())\n",
    "    return result\n",
    "\n",
    "def ids2text(index):\n",
    "    text = ''\n",
    "    for idx in index:\n",
    "        if idx == word2Id_dict['<PAD>']:\n",
    "            break\n",
    "        elif idx == word2Id_dict['<RARE>']:\n",
    "            pass\n",
    "        else:\n",
    "            text += id2word_dict[idx] + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40612acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texts'] = df['Captions'].apply(lambda x: ids2textlist(x))\n",
    "# df['texts'] = df['Captions'].apply(lambda x: ids2text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a90b639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the petals of the flower are pink in color and have a yellow center',\n",
       " 'this flower is pink and white in color with petals that are multi colored',\n",
       " 'the purple petals have shades of white with white anther and filament',\n",
       " 'this flower has large pink petals and a white stigma in the center',\n",
       " 'this flower has petals that are pink and has a yellow stamen',\n",
       " 'a flower with short and wide petals that is light purple',\n",
       " 'this flower has small pink petals with a yellow center',\n",
       " 'this flower has large rounded pink petals with curved edges and purple veins',\n",
       " 'this flower has purple petals as well as a white stamen']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8798b",
   "metadata": {},
   "source": [
    "## GET captions embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89fc7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, TFBertModel\n",
    "# from transformers import AutoTokenizer, TFAutoModel\n",
    "# import transformers\n",
    "# import logging\n",
    "# transformers.logging.get_verbosity = lambda: logging.NOTSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3ee6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n",
    "# bert_model = TFAutoModel.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ecb3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_caption_embedding(string_list):\n",
    "#     inputs = bert_tokenizer(string_list, return_tensors=\"tf\", padding='max_length',max_length=30)\n",
    "# #     print(inputs)\n",
    "#     bert_outputs = bert_model(inputs)\n",
    "#     caption_embedding = bert_outputs.pooler_output\n",
    "#     caption_embedding = caption_embedding.numpy()\n",
    "# #     print(caption_embedding.shape)\n",
    "#     return caption_embedding.tolist()\n",
    "\n",
    "# # test_string = ['this flower is white and pink in color with petals that have small veins', 'the flower shown has a purple and white petal with white anther', 'the four heart shaped pink petals of this flower are striped with fuchsia and their centers are yellow and white']  \n",
    "# # print(len(get_caption_embedding(test_string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab9e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# print(\"{}, start infering.\".format(datetime.now()))\n",
    "# df['embeddings'] = df['texts'].apply(lambda x : get_caption_embedding(x))\n",
    "# print(\"{}, end infering.\".format(datetime.now()))\n",
    "# # df.to_pickle(\"./dataset/text2img_cls_embedding.pkl\")\n",
    "# df.to_pickle(\"./dataset/testData_cls_embedding.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f1897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_list_of_list(caption):\n",
    "#     return [caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e199c8aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_pickle(\"./dataset/testData_cls_embedding.pkl\")\n",
    "# df['Captions'] = df['Captions'].apply(lambda x: make_list_of_list(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf203db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle(\"./dataset/testData_cls_embedding.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd226f",
   "metadata": {},
   "source": [
    "## start to build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc27bb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7370, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"./dataset/text2img_cls_embedding.pkl\")\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85dfe254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a57db1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "    raw_img = tf.io.read_file(img_path)\n",
    "    return raw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b975111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(img):\n",
    "    feature = {\n",
    "        'img': _bytes_feature(img)\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    \n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a13a408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(img):\n",
    "    tf_string = tf.py_function(\n",
    "        serialize_example,\n",
    "        [img],  \n",
    "        tf.string)      \n",
    "    return tf.reshape(tf_string, ()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a63475f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = df['ImagePath'].values\n",
    "image_paths = np.asarray(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edb54254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_record_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "# write_record_dataset = write_record_dataset.map(load_img,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# write_record_dataset = write_record_dataset.map(tf_serialize_example,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# filename = f'train.tfrecord'\n",
    "# writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "# writer.write(write_record_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f1dec3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset_train = tf.data.TFRecordDataset(['train.tfrecord'])\n",
    "raw_dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f705fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'img': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    return parsed['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb5a9c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset_train = raw_dataset_train.map(_parse_function)\n",
    "raw_dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "825370e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BATCH_SIZE = 75\n",
    "BUFFER_SIZE = 2000\n",
    "DATASET_SIZE = 211485\n",
    "\n",
    "SAMPLE_COL = 8\n",
    "SAMPLE_ROW = 8\n",
    "SAMPLE_NUM = SAMPLE_COL * SAMPLE_ROW\n",
    "\n",
    "CAPTION_NUM = 70495\n",
    "\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "# hparas = {\n",
    "# #     'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "#     'EMBED_DIM': 1024,                         # word embedding dimension\n",
    "#     'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "# #     'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "#     'Z_DIM': 256,                             # random noise z dimension\n",
    "#     'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "#     'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "#     'BATCH_SIZE': 64,\n",
    "#     'LR': 1e-4,\n",
    "#     'LR_DECAY': 0.5,\n",
    "#     'BETA_1': 0.5,\n",
    "#     'N_EPOCH': 600,\n",
    "#     'N_SAMPLE': num_training_sample,          # size of training data\n",
    "#     'CHECKPOINTS_DIR': './checkpoints/train',  # checkpoint path\n",
    "#     'PRINT_FREQ': 1,                           # printing frequency of loss\n",
    "#     'TEST_Z':(SAMPLE_NUM,256),\n",
    "# }\n",
    "\n",
    "hparas = {\n",
    "    'EMBED_DIM': 1024,                         # word embedding dimension\n",
    "    'Z_DIM': 256,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': 100,\n",
    "    'LR': 1e-4,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 600,\n",
    "    'N_SAMPLE': DATASET_SIZE // BATCH_SIZE,          # size of training data\n",
    "    'rs_Train': float(BATCH_SIZE) / float(DATASET_SIZE), \n",
    "    'CHECKPOINTS_DIR': './checkpoints/train',  # checkpoint path\n",
    "    'PRINT_FREQ': 1,                       # printing frequency of loss\n",
    "    'BZ':(BATCH_SIZE,256),\n",
    "    'TEST_Z':(SAMPLE_NUM,256),\n",
    "    'TEST_BATCH_SIZE':91\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2447a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processing(img):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img = tf.cast(img,tf.float32)\n",
    "    img = img / 255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83f43f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_train = raw_dataset_train.map(processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "844bf770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 23:32:29.240168: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-28 23:32:29.264393: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2592000000 Hz\n"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "for img in raw_dataset_train:\n",
    "    imgs.append(img.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1abea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data_generator(img, embedding):\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    img = img*2. - 1.\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "\n",
    "    return img, embedding\n",
    "\n",
    "def flip_right_left_data_generator(img, embedding):\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    img = img*2. -1.\n",
    "    img = tf.image.flip_left_right(img)\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "\n",
    "    return img, embedding\n",
    "\n",
    "\n",
    "def adjust_brightness_data_generator(img, embedding):\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.random_brightness(img, 0.2, 2)\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    img = img*2. -1.\n",
    "    img = tf.image.flip_up_down(img)\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "\n",
    "    return img, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dea24a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./dataset/text2img_cls_embedding.pkl\")\n",
    "\n",
    "embeddings = df['embeddings'].values\n",
    "\n",
    "embedding = []\n",
    "\n",
    "img_for_dataset = []\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for emb in embeddings[i]:\n",
    "        embedding.append(emb)\n",
    "        img_for_dataset.append(imgs[i])\n",
    "embedding = np.asarray(embedding)\n",
    "img_for_dataset = np.asarray(img_for_dataset)\n",
    "\n",
    "assert embedding.shape[0] == img_for_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_for_dataset, embedding))\n",
    "dataset = dataset.map(training_data_generator,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#flip_right_left_dataset = tf.data.Dataset.from_tensor_slices((img_for_dataset, embedding))\n",
    "#flip_right_left_dataset = flip_right_left_dataset.map(flip_right_left_data_generator,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#adjust_brightness_dataset = tf.data.Dataset.from_tensor_slices((img_for_dataset, embedding))\n",
    "#adjust_brightness_dataset = adjust_brightness_dataset.map(adjust_brightness_data_generator,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#dataset = dataset.concatenate(flip_right_left_dataset)\n",
    "#dataset = dataset.concatenate(adjust_brightness_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45697538",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.compress = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.to_4_4_1024 = tf.keras.layers.Dense(4*4*1024)\n",
    "        \n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.lr1 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr2 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr3 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr4 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr5 = tf.keras.layers.LeakyReLU()\n",
    "        \n",
    "        self.dc1 = tf.keras.layers.Conv2DTranspose(\n",
    "            filters = 512,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = \"SAME\"\n",
    "        )\n",
    "        self.dc2 = tf.keras.layers.Conv2DTranspose(\n",
    "            filters = 256,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = \"SAME\"\n",
    "        )\n",
    "        self.dc3 = tf.keras.layers.Conv2DTranspose(\n",
    "            filters = 128,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = \"SAME\"\n",
    "        )\n",
    "        self.dc4 = tf.keras.layers.Conv2DTranspose(\n",
    "            filters = 3,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = \"SAME\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def call(self, noise_z, text, training):\n",
    "        # compress the embedding\n",
    "        text = self.compress(text)\n",
    "        text = self.lr1(text)\n",
    "        \n",
    "        # concatenate input text and random noise\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        \n",
    "        # To 4*4*1024\n",
    "        text_concat = self.to_4_4_1024(text_concat)\n",
    "        text_concat = tf.reshape(text_concat, [-1, 4, 4, 1024])\n",
    "        text_concat = self.bn1(text_concat,training=training)\n",
    "        text_concat = self.lr2(text_concat)\n",
    "        \n",
    "        # To 8*8*512\n",
    "        text_concat = self.dc1(text_concat)\n",
    "        text_concat = self.bn2(text_concat,training=training)\n",
    "        text_concat = self.lr3(text_concat)\n",
    "        \n",
    "        # To 16*16*256\n",
    "        text_concat = self.dc2(text_concat)\n",
    "        text_concat = self.bn3(text_concat,training=training)\n",
    "        text_concat = self.lr4(text_concat)\n",
    "        \n",
    "        # To 32*32*128\n",
    "        text_concat = self.dc3(text_concat)\n",
    "        text_concat = self.bn4(text_concat,training=training)\n",
    "        text_concat = self.lr5(text_concat)\n",
    "        \n",
    "        # To 64*64*3\n",
    "        text_concat = self.dc4(text_concat)\n",
    "        \n",
    "        output = tf.nn.tanh(text_concat)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84274df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.compress = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "        \n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn5 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.lr1 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr2 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr3 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr4 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr5 = tf.keras.layers.LeakyReLU()\n",
    "        \n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters = 128,\n",
    "            kernel_size = 5,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\",\n",
    "            input_shape = (64,64,3))\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters = 256,\n",
    "            kernel_size = 5,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\")\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filters = 512,\n",
    "            kernel_size = 5,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\")\n",
    "        \n",
    "        self.conv4 = tf.keras.layers.Conv2D(\n",
    "            filters = 1024,\n",
    "            kernel_size = 5,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\")\n",
    "        \n",
    "        self.conv5 = tf.keras.layers.Conv2D(\n",
    "            filters = 1024,\n",
    "            kernel_size = 1,\n",
    "            strides = (1, 1),\n",
    "            padding = \"SAME\")\n",
    "    \n",
    "    def call(self, img, text, training):\n",
    "        # Conpress embedding\n",
    "        text = self.compress(text)\n",
    "        text = self.relu(text)\n",
    "        # To 32*32*128\n",
    "        img = self.conv1(img)\n",
    "        #img = self.bn1(img,training=training)\n",
    "        img = self.lr1(img)\n",
    "        # To 16*16*256\n",
    "        img = self.conv2(img)\n",
    "        #img = self.bn2(img,training=training)\n",
    "        img = self.lr2(img)\n",
    "        # To 8*8*512\n",
    "        img = self.conv3(img)\n",
    "        #img = self.bn3(img,training=training)\n",
    "        img = self.lr3(img)\n",
    "        # To 4*4*1024\n",
    "        img = self.conv4(img)\n",
    "        #img = self.bn4(img,training=training)\n",
    "        img = self.lr4(img)\n",
    "        \n",
    "        # concatenate image with paired text\n",
    "        text = tf.expand_dims(text,axis=1)\n",
    "        text = tf.expand_dims(text,axis=1)\n",
    "        text = tf.tile(text,multiples=[1,4,4,1])\n",
    "        img_text = tf.concat([img, text], axis=-1)\n",
    "        \n",
    "        img_text = self.conv5(img_text)\n",
    "        #img_text = self.bn5(img_text,training=training)\n",
    "        img_text = self.relu2(img_text)\n",
    "        \n",
    "        img_text = tf.reshape(img_text, [-1, 4*4*1024])\n",
    "        \n",
    "        \n",
    "        score = self.d(img_text)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ba7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2792965",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = hparas['CHECKPOINTS_DIR']\n",
    "ckpt = tf.train.Checkpoint(generator = generator,\n",
    "                           discriminator = discriminator,\n",
    "                           generator_optimizer = generator_optimizer,\n",
    "                           discriminator_optimizer = discriminator_optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32010876",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def DC_D_Train(c1,embed,noise_decay):\n",
    "    z = tf.random.normal(hparas['BZ']) \n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_2:\n",
    "            x_bar = generator(z, embed, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = c1\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise_decay * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise_decay * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise_decay * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0 = discriminator(x_bar, embed, training = True)\n",
    "            z1 = discriminator(x, embed, training = True)\n",
    "            z2 = discriminator(x_hat, embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_2.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_d = tp.gradient(ld, discriminator.trainable_variables)\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "\n",
    "    return lg, ld\n",
    "\n",
    "@tf.function\n",
    "def DC_G_Train(c1,embed,noise_decay):\n",
    "    \n",
    "    z = tf.random.normal(hparas['BZ'])\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_2:\n",
    "            x_bar = generator(z, embed, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = c1\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise_decay * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise_decay * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise_decay * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0 = discriminator(x_bar, embed, training = True)\n",
    "            z1 = discriminator(x, embed, training = True)\n",
    "            z2 = discriminator(x_hat, embed, training = True)\n",
    "            gradient_penalty = tp_2.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_g = tp.gradient(lg, generator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "\n",
    "    return lg, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(noise, embed):\n",
    "    fake_image = generator(noise, embed, training = False)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = df['embeddings'].values\n",
    "ids = df['Captions'].values\n",
    "\n",
    "test_embed = []\n",
    "\n",
    "for i in range(8):\n",
    "    if len(embeddings[i]) >= 8:\n",
    "        for j in range(8):\n",
    "            test_embed.append(embeddings[i][j])\n",
    "            \n",
    "test_noise = tf.random.normal(hparas['TEST_Z'])\n",
    "test_embed = tf.Variable(test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36770df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = (\n",
    "    DC_D_Train,\n",
    "    DC_D_Train,\n",
    "    DC_D_Train,\n",
    "    DC_D_Train,\n",
    "    DC_D_Train,\n",
    "    DC_G_Train\n",
    ")\n",
    "\n",
    "Critic = len(Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf58b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlg = [None] * hparas['N_EPOCH'] #record loss of g for each epoch\n",
    "wld = [None] * hparas['N_EPOCH']  #record loss of d for each epoch\n",
    "wsp = [None] * hparas['N_EPOCH']  #record sample images for each epoch\n",
    "\n",
    "rsTrain = hparas['rs_Train']\n",
    "ctr = 0\n",
    "for ep in range(hparas['N_EPOCH']):\n",
    "    print(\"Epoch: \" + str(ep+1), end='\\r')\n",
    "    print('')\n",
    "    lgt = 0.0\n",
    "    ldt = 0.0\n",
    "    if ep < 200:\n",
    "        noise_decay = 1.0 / float(ep+1)\n",
    "    else:\n",
    "        noise_decay = 0.0\n",
    "        \n",
    "    for idx, (real_img,embed) in enumerate(dataset):\n",
    "        print(str(idx+1) + '/' + str(hparas['N_SAMPLE']), end='\\r')\n",
    "        lg, ld = Train[ctr](real_img, embed, noise_decay)\n",
    "        ctr += 1\n",
    "        lgt += lg.numpy()\n",
    "        ldt += ld.numpy()\n",
    "        if ctr == Critic : ctr = 0\n",
    "    print('')\n",
    "    wlg[ep] = lgt * rsTrain\n",
    "    wld[ep] = ldt * rsTrain\n",
    "    with open('./wlg_v2.txt','a') as f:\n",
    "        f.write(str(lgt * rsTrain) + '\\n')\n",
    "    f.close()\n",
    "    with open('./wld_v2.txt','a') as f:\n",
    "        f.write(str(ldt * rsTrain) + '\\n')\n",
    "    f.close()\n",
    "    \n",
    "    out = test_step(test_noise, test_embed)\n",
    "    img = utPuzzle(\n",
    "        ((out+1) / 2. * 255.0).numpy().astype(np.uint8),\n",
    "        SAMPLE_COL,\n",
    "        SAMPLE_ROW,\n",
    "        \"imgs_v2/w_%04d.png\" % ep\n",
    "    )\n",
    "    wsp[ep] = img\n",
    "    if (ep+1) % 10 == 0: \n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Epoch %d\" % (ep+1))\n",
    "        plt.show()\n",
    "    if (ep+1) % 10 == 0: \n",
    "        ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddf108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
