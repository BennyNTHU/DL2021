{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eae24e3",
   "metadata": {},
   "source": [
    "# Lab17 Deep Reinforcement Learning\n",
    "110065508 李丞恩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52709978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-24 00:01:34.233998: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import moviepy.editor as mpy\n",
    "import skimage.transform\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.losses as kls\n",
    "import os\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from tqdm.notebook import tqdm\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3295404",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # this line make pop-out window not appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef718a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-24 00:01:35.031819: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-12-24 00:01:35.060387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-24 00:01:35.060880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.852GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2021-12-24 00:01:35.060915: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-12-24 00:01:35.062670: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-12-24 00:01:35.062711: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-12-24 00:01:35.063261: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-12-24 00:01:35.063401: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-12-24 00:01:35.063950: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-12-24 00:01:35.064408: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-12-24 00:01:35.064494: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-12-24 00:01:35.064563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-24 00:01:35.065065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-24 00:01:35.065514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-12-24 00:01:35.066331: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-24 00:01:35.066712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-24 00:01:35.067177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.852GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2021-12-24 00:01:35.067230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-24 00:01:35.067716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-24 00:01:35.068168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-12-24 00:01:35.068209: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-12-24 00:01:35.358856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-12-24 00:01:35.358875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-12-24 00:01:35.358880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-12-24 00:01:35.359037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-24 00:01:35.359512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-24 00:01:35.359954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-24 00:01:35.360395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10245 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\") \n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974a2d0",
   "metadata": {},
   "source": [
    "## 一. 遊戲環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36fa519",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = FlappyBird()\n",
    "env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n",
    "env.reset_game()\n",
    "\n",
    "test_game = FlappyBird()\n",
    "test_env = PLE(test_game, fps=30, display_screen=False)\n",
    "test_env.reset_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'image_size': 84,\n",
    "    'num_stack': 4,\n",
    "    'action_dim': len(env.getActionSet()),\n",
    "    'hidden_size': 256,\n",
    "    'lr': 0.0001,\n",
    "    'gamma': 0.99,\n",
    "    'lambda': 0.95,\n",
    "    'clip_val': 0.2,\n",
    "    'ppo_epochs': 8,\n",
    "    'test_epochs': 1,\n",
    "    'num_steps': 512,\n",
    "    'mini_batch_size': 64,\n",
    "    'target_reward': 200,\n",
    "    'max_episode': 30000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4332b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not modify this method\n",
    "def make_anim(images, fps=60, true_image=False):\n",
    "    duration = len(images) / fps\n",
    "\n",
    "    def make_frame(t):\n",
    "        try:\n",
    "            x = images[int(len(images) / duration * t)]\n",
    "        except:\n",
    "            x = images[-1]\n",
    "\n",
    "        if true_image:\n",
    "            return x.astype(np.uint8)\n",
    "        else:\n",
    "            return ((x + 1) / 2 * 255).astype(np.uint8)\n",
    "\n",
    "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
    "    clip.fps = fps\n",
    "    \n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_screen(screen):\n",
    "    screen = skimage.transform.rotate(screen, -90, resize=True)\n",
    "    screen = screen[:400, :]\n",
    "    screen = skimage.transform.resize(screen, [hparas['image_size'], hparas['image_size'], 1])\n",
    "    return screen.astype(np.float32)\n",
    "\n",
    "def frames_to_state(input_frames):\n",
    "    if(len(input_frames) == 1):\n",
    "        state = np.concatenate(input_frames*4, axis=-1)\n",
    "    elif(len(input_frames) == 2):\n",
    "        state = np.concatenate(input_frames[0:1]*2 + input_frames[1:]*2, axis=-1)\n",
    "    elif(len(input_frames) == 3):\n",
    "        state = np.concatenate(input_frames + input_frames[2:], axis=-1)\n",
    "    else:\n",
    "        state = np.concatenate(input_frames[-4:], axis=-1)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28688e6e",
   "metadata": {},
   "source": [
    "## 二. 深度強化學習的Agent設定\n",
    "### 1. 深度強化學習網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, hparas):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = tf.keras.Sequential([\n",
    "          # Convolutional Layers\n",
    "          tf.keras.layers.Conv2D(filters=32, kernel_size=8, strides=4),\n",
    "          tf.keras.layers.ReLU(),\n",
    "          tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=2),\n",
    "          tf.keras.layers.ReLU(),\n",
    "          tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1),\n",
    "          tf.keras.layers.ReLU(),\n",
    "          # Embedding Layers\n",
    "          tf.keras.layers.Flatten(),\n",
    "          tf.keras.layers.Dense(hparas['hidden_size']),\n",
    "          tf.keras.layers.ReLU(),\n",
    "        ])\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor = tf.keras.layers.Dense(hparas['action_dim'], activation='softmax')\n",
    "        # Critic Network\n",
    "        self.critic = tf.keras.layers.Dense(1, activation = None)\n",
    "\n",
    "    def call(self, input):\n",
    "        x = self.feature_extractor(input)\n",
    "        action_logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return action_logits, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42a1e58",
   "metadata": {},
   "source": [
    "### 2. 定義Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ae51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, hparas):\n",
    "        self.gamma = hparas['gamma']\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=hparas['lr'])\n",
    "        self.actor_critic = ActorCriticNetwork(hparas)\n",
    "        self.clip_pram = hparas['clip_val']\n",
    "    \n",
    "    def ppo_iter(self, mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "        batch_size = states.shape[0]\n",
    "        for _ in range(batch_size // mini_batch_size):\n",
    "            rand_ids = tf.convert_to_tensor(np.random.randint(0, batch_size, mini_batch_size), dtype=tf.int32)\n",
    "            yield tf.gather(states, rand_ids), tf.gather(actions, rand_ids), tf.gather(log_probs, rand_ids), \\\n",
    "             tf.gather(returns, rand_ids), tf.gather(advantage, rand_ids)\n",
    "    \n",
    "    def ppo_update(self, ppo_epochs, mini_batch_size, states, actions, log_probs, discount_rewards, advantages):       \n",
    "        total_actor_loss = 0\n",
    "        total_critic_loss = 0\n",
    "        for _ in range(ppo_epochs):\n",
    "            for state, action, old_log_probs, reward, advantage in self.ppo_iter(mini_batch_size, states, actions, log_probs, discount_rewards, advantages):\n",
    "                reward = tf.expand_dims(reward, axis=-1)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    prob, value = self.actor_critic(state, training=True)\n",
    "                    dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
    "                    entropy = tf.math.reduce_mean(dist.entropy())\n",
    "                    new_log_probs = dist.log_prob(action)\n",
    "\n",
    "                    # PPO ratio\n",
    "                    ratio = tf.math.exp(new_log_probs - old_log_probs)\n",
    "                    surr1 = ratio * advantage\n",
    "                    surr2 = tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram) * advantage\n",
    "\n",
    "                    actor_loss = tf.math.negative(tf.math.reduce_mean(tf.math.minimum(surr1, surr2))) \\\n",
    "                                 - 0.1 * entropy\n",
    "                    critic_loss = 0.5 * tf.math.reduce_mean(kls.mean_squared_error(reward, value))\n",
    "\n",
    "                    total_loss = actor_loss + critic_loss\n",
    "            \n",
    "                # single optimizer\n",
    "                grads = tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.actor_critic.trainable_variables))\n",
    "      \n",
    "                total_actor_loss += actor_loss\n",
    "                total_critic_loss += critic_loss\n",
    "        return total_actor_loss, total_critic_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1506.02438.pdf\n",
    "# Equation 16\n",
    "def compute_gae(rewards, masks, values, gamma, LAMBDA):\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
    "        gae = delta + gamma * LAMBDA * masks[i] * gae\n",
    "        returns.append(gae + values[i])\n",
    "\n",
    "    returns.reverse()\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1725686",
   "metadata": {},
   "source": [
    "### 3. Testing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5617bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward(test_env, agent):\n",
    "    total_reward = 0\n",
    "    # Reset the environment\n",
    "    test_env.reset_game()\n",
    "    input_frames = [preprocess_screen(test_env.getScreenGrayscale())]\n",
    "\n",
    "    while not test_env.game_over():\n",
    "\n",
    "        state = frames_to_state(input_frames)\n",
    "        state = tf.expand_dims(state, axis=0)\n",
    "        prob, value = agent.actor_critic(state)\n",
    "\n",
    "        action = np.argmax(prob[0].numpy())\n",
    "        reward = test_env.act(test_env.getActionSet()[action])\n",
    "        total_reward += reward\n",
    "\n",
    "        input_frames.append(preprocess_screen(test_env.getScreenGrayscale()))\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5599bc",
   "metadata": {},
   "source": [
    "## 三. Agent的訓練與成果展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8254c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(hparas)\n",
    "max_episode = hparas['max_episode']\n",
    "test_per_n_episode = 10\n",
    "force_save_per_n_episode = 1000\n",
    "early_stop_reward = 10\n",
    "\n",
    "start_s = 0\n",
    "best_reward = -5.0\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    actor_critic = agent.actor_critic,\n",
    "    optimizer = agent.optimizer,\n",
    ")\n",
    "\n",
    "# Load from old checkpoint\n",
    "# checkpoint.restore('ckpt_dir/ckpt-?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_reward = []\n",
    "total_avgr = []\n",
    "early_stop = False\n",
    "avg_rewards_list = []\n",
    "\n",
    "env.reset_game()\n",
    "\n",
    "for s in tqdm(range(0, max_episode)):\n",
    "    if early_stop == True:\n",
    "        break\n",
    "\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    masks = []\n",
    "    values = []\n",
    "\n",
    "    display_frames = [env.getScreenRGB()]\n",
    "    input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
    "\n",
    "    for step in range(hparas['num_steps']):\n",
    "\n",
    "        state = frames_to_state(input_frames)\n",
    "        state = tf.expand_dims(state, axis=0)\n",
    "        prob, value = agent.actor_critic(state)\n",
    "\n",
    "        dist = tfp.distributions.Categorical(probs=prob[0], dtype=tf.float32)\n",
    "        action = dist.sample(1)\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        reward = env.act(env.getActionSet()[int(action.numpy())])\n",
    "\n",
    "        done = env.game_over()\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        values.append(value[0])\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(tf.convert_to_tensor(reward, dtype=tf.float32))\n",
    "        masks.append(tf.convert_to_tensor(1-int(done), dtype=tf.float32))\n",
    "\n",
    "        display_frames.append(env.getScreenRGB())\n",
    "        input_frames.append(preprocess_screen(env.getScreenGrayscale()))\n",
    "\n",
    "        if done:\n",
    "            env.reset_game()\n",
    "            input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
    "  \n",
    "    _, next_value = agent.actor_critic(state)\n",
    "    values.append(next_value[0])\n",
    "\n",
    "    returns = compute_gae(rewards, masks, values, hparas['gamma'], hparas['lambda'])\n",
    "\n",
    "    returns = tf.concat(returns, axis=0)\n",
    "    log_probs = tf.concat(log_probs, axis=0)\n",
    "    values = tf.concat(values, axis=0)\n",
    "    states = tf.concat(states, axis=0)\n",
    "    actions = tf.concat(actions, axis=0)\n",
    "    advantage = returns - values[:-1]\n",
    "\n",
    "    a_loss, c_loss = agent.ppo_update(hparas['ppo_epochs'], hparas['mini_batch_size'], \\\n",
    "                                      states, actions, log_probs, returns, advantage)\n",
    "    print('[Episode %d]  Actor loss: %.5f, Critic loss: %.5f' % (s, a_loss, c_loss))\n",
    "\n",
    "    if s % test_per_n_episode == 0:\n",
    "        # test agent hparas['test_epochs'] times to get the average reward\n",
    "        avg_reward = np.mean([test_reward(test_env, agent) for _ in range(hparas['test_epochs'])])\n",
    "        print(\"Test average reward is %.1f, Current best average reward is %.1f\\n\" % (avg_reward, best_reward))\n",
    "        avg_rewards_list.append(avg_reward)\n",
    "\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            agent.actor_critic.save('./save/Actor/model_actor_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
    "            checkpoint.save(file_prefix = './save/checkpoints/ckpt')\n",
    "\n",
    "    if s % force_save_per_n_episode == 0:\n",
    "        agent.actor_critic.save('./save/Actor/model_actor_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
    "        checkpoint.save(file_prefix = './save/checkpoints/ckpt')\n",
    "        clip = make_anim(display_frames, fps=60, true_image=True).rotate(-90)\n",
    "        clip.write_videofile(\"movie_f/{}_demo-{}.webm\".format('Lab17', s), fps=60)\n",
    "        display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))\n",
    "\n",
    "    if best_reward >= early_stop_reward:\n",
    "        early_stop = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
