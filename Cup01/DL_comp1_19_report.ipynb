{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import _pickle as pkl\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Clean </h2>\n",
    "\n",
    "### 定義清除html的tag以及將辭意重複的字詞作處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    stop = stopwords.words('english')\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Feature engineering</h3> \n",
    "\n",
    "挑選出我們需要的feature，包括:\n",
    "    1. author: 作者姓名\n",
    "    2. channel: 文章類別\n",
    "    3. title: 文章標題\n",
    "    4. topic: 文章細項分類\n",
    "    5. weekday: 文章在星期幾發布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datetime(text=[]):\n",
    "\n",
    "    day = []\n",
    "    \n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        if soup.time.has_attr('datetime'):\n",
    "            date = soup.time.attrs['datetime']\n",
    "            day.append(' '+ date[0:3])\n",
    "        else:\n",
    "            day.append(' fuckday')\n",
    "    return day\n",
    "\n",
    "def fetch_channel(text=[]):\n",
    "    \n",
    "    channels = []\n",
    "    \n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        channel = soup.article['data-channel']\n",
    "        channels.append(channel)\n",
    "    return channels\n",
    "\n",
    "def fetch_img_count(text=[]):\n",
    "\n",
    "    count = []\n",
    "    \n",
    "    for tx in text:\n",
    "        \n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        c = 0\n",
    "        find_all_images = soup.find_all('img')\n",
    "        \n",
    "        for i in find_all_images:\n",
    "            c = c+1\n",
    "        count.append(c)\n",
    "    return count\n",
    "\n",
    "def fetch_topics(text=[]):\n",
    "\n",
    "    topics = []\n",
    "    \n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        footer = soup.footer\n",
    "        ta = footer.find_all('a')\n",
    "        topic = []\n",
    "\n",
    "        for t in ta:\n",
    "            topic.append(t.get_text())\n",
    "        ff = ' '.join(topic)\n",
    "        topics.append(ff)\n",
    "\n",
    "    return topics\n",
    "\n",
    "def fetch_authors(text=[]):\n",
    "\n",
    "    authors = []\n",
    "    \n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        footer = soup.span\n",
    "        if footer != None:\n",
    "            ta = footer.findAll('a')\n",
    "            author = []\n",
    "            for t in ta:\n",
    "                author.append(t.get_text())\n",
    "            if len(author) == 0:\n",
    "                ff = 'NaN'\n",
    "            else:\n",
    "                ff = ''.join(author)\n",
    "        else:\n",
    "            ff = 'NaN'\n",
    "        authors.append(ff)\n",
    "\n",
    "    return authors\n",
    "\n",
    "def fetch_titles(text=[]):\n",
    "    \n",
    "    titles = []\n",
    "    \n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        footer = soup.h1\n",
    "        if footer != None:\n",
    "            ff = footer.get_text()\n",
    "        else:\n",
    "            ff = 'NaN'\n",
    "        titles.append(ff)\n",
    "        \n",
    "    return titles\n",
    "def fetch_social_media_count(text=[]):\n",
    "    count = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        c = 0\n",
    "        for frame in soup(\"iframe\"):\n",
    "#             print(frame.get('src').split(\".\"))\n",
    "            if frame.get('src').find(\"youtube\") != None:\n",
    "                c = c+1\n",
    "            elif frame.get('src').find(\"instagram\") != None:\n",
    "                c = c+1\n",
    "            elif frame.get('src').find(\"vine\") != None:\n",
    "                c= c+1\n",
    "            # apply new media here\n",
    "\n",
    "        count.append(c)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15760/1583152599.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./input/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# display(df.head(5))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch_social_media_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Page content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Popularity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15760/2108624577.py\u001b[0m in \u001b[0;36mfetch_social_media_count\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iframe\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mbuilder_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilder_registry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuilder_class\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m                 raise FeatureNotFound(\n\u001b[0m\u001b[0;32m    246\u001b[0m                     \u001b[1;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m                     \u001b[1;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# read training data\n",
    "df = pd.read_csv('./input/train.csv')\n",
    "# display(df.head(5))\n",
    "print(fetch_social_media_count(df[0:10]['Page content']))\n",
    "print(df[0:10]['Popularity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fatch deature\n",
    "topic_batch   = fetch_topics            (df[:]['Page content'])\n",
    "channel_batch = fetch_channel           (df[:]['Page content'])\n",
    "weekday_batch = fetch_datetime          (df[:]['Page content'])\n",
    "author_batch  = fetch_authors           (df[:]['Page content'])\n",
    "img_batch     = fetch_img_count         (df[:]['Page content'])\n",
    "title_batch   = fetch_titles            (df[:]['Page content'])\n",
    "media_batch   = fetch_social_media_count(df[:]['Page content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-consturct training data\n",
    "df_feature = pd.DataFrame({'Id':df.Id[:],\n",
    "                           'Popularity':df.Popularity[:],\n",
    "                           'topic':topic_batch,\n",
    "                           'channel':channel_batch,\n",
    "                           'weekday':weekday_batch,\n",
    "                           'author':author_batch,\n",
    "                           'img count':img_batch,\n",
    "                           'title':title_batch,\n",
    "                           'media count': media_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>author</th>\n",
       "      <th>channel</th>\n",
       "      <th>img count</th>\n",
       "      <th>media count</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>world</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>Wed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>tech</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>Thu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>Wed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>Fri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>Thu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity            author        channel  img count  media count  \\\n",
       "0   0          -1               NaN          world          1            0   \n",
       "1   1           1  Christina Warren           tech          2            0   \n",
       "2   2           1         Sam Laird  entertainment          2           25   \n",
       "3   3          -1         Sam Laird    watercooler          1           21   \n",
       "4   4          -1   Connor Finnegan  entertainment         52            1   \n",
       "\n",
       "                                               title  \\\n",
       "0  NASA's Grand Challenge: Stop Asteroids From De...   \n",
       "1  Google's New Open Source Patent Pledge: We Won...   \n",
       "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
       "3        Cameraperson Fails Deliver Slapstick Laughs   \n",
       "4  NFL Star Helps Young Fan Prove Friendship With...   \n",
       "\n",
       "                                               topic weekday  \n",
       "0  Asteroid Asteroids challenge Earth Space U.S. ...     Wed  \n",
       "1  Apps and Software Google open source opn pledg...     Thu  \n",
       "2      Entertainment NFL NFL Draft Sports Television     Wed  \n",
       "3                    Sports Video Videos Watercooler     Fri  \n",
       "4  Entertainment instagram instagram video NFL Sp...     Thu  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_feature.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = StandardScaler()\n",
    "# print(np.array(df_feature[:]['media count'].values.reshape(-1, 1)))\n",
    "# sc.fit(np.array(df_feature[:]['media count'].values.reshape(-1, 1)))\n",
    "# media_feature = sc.transform(df_feature['media count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 對挑選出來的feature根據性質做onehot encoding 或是 featurre hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 4000)\n",
      "(27643, 33)\n",
      "(27643, 7)\n",
      "(27643, 428)\n",
      "(27643, 4000)\n"
     ]
    }
   ],
   "source": [
    "# feature pre-processing\n",
    "\n",
    "# topic to 4000-dim vector\n",
    "hashvec = HashingVectorizer(n_features=4000,\n",
    "                            preprocessor=preprocessor,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "topic_hash = hashvec.transform(df_feature['topic'])\n",
    "print(topic_hash.shape)\n",
    "\n",
    "# ohe channel\n",
    "onehotencoderchannel = OneHotEncoder(handle_unknown='ignore')\n",
    "data_str_ohe_channel = onehotencoderchannel.fit_transform(df_feature['channel'].values.reshape(-1,1)).toarray()\n",
    "print(data_str_ohe_channel.shape)\n",
    "\n",
    "# ohe weekday\n",
    "onehotencoderweekday = OneHotEncoder(handle_unknown='ignore')\n",
    "data_str_ohe_weekday = onehotencoderweekday.fit_transform(df_feature['weekday'].values.reshape(-1,1)).toarray()\n",
    "print(data_str_ohe_weekday.shape)\n",
    "\n",
    "# ohe author\n",
    "onehotencoderauthor = OneHotEncoder(handle_unknown='ignore')\n",
    "data_str_ohe_author = onehotencoderauthor.fit_transform(df_feature['author'].values.reshape(-1,1)).toarray()\n",
    "print(data_str_ohe_author.shape)\n",
    "\n",
    "title_hash = hashvec.transform(df_feature['title'])\n",
    "\n",
    "\n",
    "print(title_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_X_train = np.concatenate([topic_hash.toarray(), data_str_ohe_channel,\n",
    "                             data_str_ohe_weekday, data_str_ohe_author, title_hash.toarray()], axis=1)\n",
    "\n",
    "df_y_train = df_feature['Popularity'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X_train, df_y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用SGDClassifier 對處理好的資料training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 0.6108992752641363\n",
      "epoch  1 0.6558416782037642\n",
      "epoch  2 0.6597213930803631\n",
      "epoch  3 0.6680220271703503\n",
      "epoch  4 0.6709409469509022\n",
      "epoch  5 0.6792859240376927\n",
      "epoch  6 0.6841521800972586\n",
      "epoch  7 0.6770870996196645\n",
      "epoch  8 0.6901256213182667\n",
      "epoch  9 0.6877716545873693\n",
      "epoch  10 0.6959724652225988\n",
      "epoch  11 0.6918381013140646\n",
      "epoch  12 0.6935495729268448\n",
      "epoch  13 0.6964670124528386\n",
      "epoch  14 0.6955382270536095\n",
      "epoch  15 0.6862720920801592\n",
      "epoch  16 0.6904274445762474\n",
      "epoch  17 0.6966094013061314\n",
      "epoch  18 0.6921931943441529\n",
      "epoch  19 0.6947486179028024\n",
      "epoch  20 0.6916715306603285\n",
      "epoch  21 0.6891888205668456\n",
      "epoch  22 0.6921299764857363\n",
      "epoch  23 0.6925094840200412\n",
      "epoch  24 0.6927043217182413\n",
      "epoch  25 0.6929294690829153\n",
      "epoch  26 0.6925108349947252\n",
      "epoch  27 0.6978375730377153\n",
      "epoch  28 0.6931457349202412\n",
      "epoch  29 0.6925063877670573\n",
      "epoch  30 0.6960193162489592\n",
      "epoch  31 0.6924378435778293\n",
      "epoch  32 0.6933631836684148\n",
      "epoch  33 0.6954631865507097\n",
      "epoch  34 0.6921208751826022\n",
      "epoch  35 0.6955108455762828\n",
      "epoch  36 0.6950838147602724\n",
      "epoch  37 0.696302193541396\n",
      "epoch  38 0.6914338172918955\n",
      "epoch  39 0.6933103986958349\n",
      "epoch  40 0.6955140840371281\n",
      "epoch  41 0.6951840131266647\n",
      "epoch  42 0.6963893734244702\n",
      "epoch  43 0.693957185905771\n",
      "epoch  44 0.6970548027922462\n",
      "epoch  45 0.6935735995914394\n",
      "epoch  46 0.6967614732219916\n",
      "epoch  47 0.6947115727548898\n",
      "epoch  48 0.6948957642554607\n",
      "epoch  49 0.6959421814120996\n",
      "training finished\n",
      "Accuracy: 0.549729\n"
     ]
    }
   ],
   "source": [
    "SGDLR = SGDClassifier(loss='log', max_iter=1000, alpha=0.0002)\n",
    "\n",
    "total_epoch = 50\n",
    "for epoch in range(total_epoch):\n",
    "    SGDLR.partial_fit(X_train, y_train, classes=np.array([-1, 1]))\n",
    "    print('epoch ', epoch, roc_auc_score(y_train, SGDLR.predict_proba(X_train)[:,1]))\n",
    "print(\"training finished\")\n",
    "\n",
    "SGDLR_pred_decision = SGDLR.predict(X_test)\n",
    "acc = accuracy_score(y_test, SGDLR_pred_decision)\n",
    "print('Accuracy: %f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用LogisticRegression對處理好的資料training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC : 0.563354\n",
      "Accuracy: 0.546474\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(max_iter=1000, C=0.3)\n",
    "LR.fit(X_train, y_train)\n",
    "\n",
    "LR_pred_probablity = LR.predict_proba(X_test)[:,1]\n",
    "LR_pred_decision   = LR.predict(X_test)\n",
    "\n",
    "roc = roc_auc_score(y_test, LR_pred_probablity)\n",
    "print('ROC_AUC : %f' % roc)\n",
    "acc = accuracy_score(y_test, LR_pred_decision)\n",
    "print('Accuracy: %f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>author</th>\n",
       "      <th>channel</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27643</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1</td>\n",
       "      <td>Soccer Star Gets Twitter Death Threats After T...</td>\n",
       "      <td>Entertainment Music One Direction soccer Sports</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27644</td>\n",
       "      <td>Stan Schroeder</td>\n",
       "      <td>tech</td>\n",
       "      <td>3</td>\n",
       "      <td>Google Glass Gets an Accessory Store</td>\n",
       "      <td>Gadgets glass Google Google Glass Google Glass...</td>\n",
       "      <td>Thu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27645</td>\n",
       "      <td>Todd Wasserman</td>\n",
       "      <td>business</td>\n",
       "      <td>2</td>\n",
       "      <td>OUYA Gaming Console Already Sold Out on Amazon</td>\n",
       "      <td>amazon amazon kindle Business Gaming</td>\n",
       "      <td>Tue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27646</td>\n",
       "      <td>Neha Prakash</td>\n",
       "      <td>film</td>\n",
       "      <td>1</td>\n",
       "      <td>'Between Two Ferns' Mocks Oscar Nominees</td>\n",
       "      <td>Between Two Ferns Movies The Oscars Oscars 201...</td>\n",
       "      <td>Wed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27647</td>\n",
       "      <td>Josh Dickey</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1</td>\n",
       "      <td>'American Sniper' Trailer: Looks Like Eastwood...</td>\n",
       "      <td>American Sniper Awards Bradley Cooper clint ea...</td>\n",
       "      <td>Fri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id          author        channel  img count  \\\n",
       "0  27643       Sam Laird  entertainment          1   \n",
       "1  27644  Stan Schroeder           tech          3   \n",
       "2  27645  Todd Wasserman       business          2   \n",
       "3  27646    Neha Prakash           film          1   \n",
       "4  27647     Josh Dickey  entertainment          1   \n",
       "\n",
       "                                               title  \\\n",
       "0  Soccer Star Gets Twitter Death Threats After T...   \n",
       "1               Google Glass Gets an Accessory Store   \n",
       "2     OUYA Gaming Console Already Sold Out on Amazon   \n",
       "3           'Between Two Ferns' Mocks Oscar Nominees   \n",
       "4  'American Sniper' Trailer: Looks Like Eastwood...   \n",
       "\n",
       "                                               topic weekday  \n",
       "0    Entertainment Music One Direction soccer Sports     Mon  \n",
       "1  Gadgets glass Google Google Glass Google Glass...     Thu  \n",
       "2               amazon amazon kindle Business Gaming     Tue  \n",
       "3  Between Two Ferns Movies The Oscars Oscars 201...     Wed  \n",
       "4  American Sniper Awards Bradley Cooper clint ea...     Fri  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read testing data\n",
    "df_test = pd.read_csv('./test.csv')\n",
    "\n",
    "test_topic_batch   = fetch_topics   (df_test[:]['Page content'])\n",
    "test_channel_batch = fetch_channel  (df_test[:]['Page content'])\n",
    "test_weekday_batch = fetch_datetime (df_test[:]['Page content'])\n",
    "test_author_batch  = fetch_authors  (df_test[:]['Page content'])\n",
    "test_img_batch     = fetch_img_count(df_test[:]['Page content'])\n",
    "test_title_batch   = fetch_titles   (df_test[:]['Page content'])\n",
    "\n",
    "df_test_feature = pd.DataFrame({'Id'        : df_test.Id[:],\n",
    "                                'topic'     : test_topic_batch,\n",
    "                                'channel'   : test_channel_batch,\n",
    "                                'weekday'   : test_weekday_batch,\n",
    "                                'author'    : test_author_batch,\n",
    "                                'img count' : test_img_batch,\n",
    "                                'title'     : test_title_batch})\n",
    "\n",
    "display(df_test_feature.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11847, 4000)\n",
      "(11847, 33)\n",
      "(11847, 7)\n",
      "(11847, 428)\n",
      "(11847, 4000)\n"
     ]
    }
   ],
   "source": [
    "topic_test_hash = hashvec.transform(df_test_feature['topic'])\n",
    "print(topic_test_hash.shape)\n",
    "\n",
    "test_data_str_ohe_channel = onehotencoderchannel.transform(df_test_feature['channel'].values.reshape(-1,1)).toarray()\n",
    "print(test_data_str_ohe_channel.shape)\n",
    "\n",
    "test_data_str_ohe_weekday = onehotencoderweekday.transform(df_test_feature['weekday'].values.reshape(-1,1)).toarray()  \n",
    "print(test_data_str_ohe_weekday.shape)\n",
    "\n",
    "test_data_str_ohe_author = onehotencoderauthor.transform(df_test_feature['author'].values.reshape(-1,1)).toarray()  \n",
    "print(test_data_str_ohe_author.shape)\n",
    "\n",
    "title_test_hash = hashvec.transform(df_test_feature['title'])\n",
    "print(title_test_hash.shape)\n",
    "\n",
    "X_test = np.concatenate([topic_test_hash.toarray(), test_data_str_ohe_channel,\n",
    "                         test_data_str_ohe_weekday, test_data_str_ohe_author, title_test_hash.toarray()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  對處理好的資料，使用SGDClassifier、LogisticRegression預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDLR Done.\n"
     ]
    }
   ],
   "source": [
    "df_out = pd.DataFrame({\"Id\":df_test.Id, \"Popularity\":SGDLR.predict_proba(X_test)[:,1]})\n",
    "df_out.to_csv(\"./SGDLR.csv\", index=None)\n",
    "print(\"SGDLR Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Done.\n"
     ]
    }
   ],
   "source": [
    "df_out = pd.DataFrame({\"Id\":df_test.Id, \"Popularity\":LR.predict_proba(X_test)[:,1]})\n",
    "df_out.to_csv(\"./LR.csv\", index=None)\n",
    "print(\"LR Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mix_LR_SGDLR Done.\n"
     ]
    }
   ],
   "source": [
    "s = (SGDLR.predict_proba(X_test)[:,1] + LR.predict_proba(X_test)[:,1]) / 2\n",
    "df_out = pd.DataFrame({\"Id\":df_test.Id, \"Popularity\":s})\n",
    "df_out.to_csv(\"./mix_LR_SGDLR.csv\", index=None)\n",
    "print(\"mix_LR_SGDLR Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 後來決定多使用RandomForest來做預測，並將結果和logistic regession做結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  49 0.8032993820467267\n",
      "training finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForest = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=0)\n",
    "# batch_size = 1000\n",
    "# stream = get_stream(path='./train.csv', size=batch_size)\n",
    "# classes = np.array([-1, 1])\n",
    "# train_auc, val_auc = [], []\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "# per_round_iters = int((27000+batch_size-1)/batch_size)\n",
    "\n",
    "# total_epoch = 25\n",
    "\n",
    "# X_train, y_train = cat, df_feature[:]['Popularity']\n",
    "\n",
    "RandomForest.fit(X_train, y_train)\n",
    "print('epoch ', epoch, roc_auc_score(y_train, RandomForest.predict_proba(X_train)[:,1]))\n",
    "\n",
    "print(\"training finished\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LR = LogisticRegression(max_iter=1000, C=0.3)\n",
    "# LR.fit(X_train, y_train)\n",
    "\n",
    "# LR_pred_probablity = LR.predict_proba(X_test)[:,1]\n",
    "# LR_pred_decision   = LR.predict(X_test)\n",
    "\n",
    "# roc = roc_auc_score(y_test, LR_pred_probablity)\n",
    "# print('ROC_AUC : %f' % roc)\n",
    "# acc = accuracy_score(y_test, LR_pred_decision)\n",
    "# print('Accuracy: %f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mix_LR_RandomForest Done.\n"
     ]
    }
   ],
   "source": [
    "s = (RandomForest.predict_proba(X_test)[:,1] + LR.predict_proba(X_test)[:,1]) / 2\n",
    "df_out = pd.DataFrame({\"Id\":df_test.Id, \"Popularity\":s})\n",
    "df_out.to_csv(\"./mix_LR_RandomForest.csv\", index=None)\n",
    "print(\"mix_LR_RandomForest Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUANGRandomForest Done.\n"
     ]
    }
   ],
   "source": [
    "df_out = pd.DataFrame({\"Id\":df_test.Id, \"Popularity\":RandomForest.predict_proba(X_test)[:,1]})\n",
    "df_out.to_csv(\"./HUANGRandomForest.csv\", index=None)\n",
    "print(\"HUANGRandomForest Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "### 組別: 育榮我大哥\n",
    "### 成員: 108062648 黃彥儒、108062510 劉志容、108133505 朱信承、108062630 楊書瑋\n",
    "\n",
    "<h2> Date Perprocessing </h2>\n",
    "<h3> Clean </h3>  \n",
    "\n",
    "    我們將整篇文章內容透過 beautiful soup 來 parse html tag，\n",
    "    並將文字一併轉成小寫後進行 tonkenize。\n",
    "\n",
    "<h3> Feature engineering</h3> \n",
    "\n",
    "    在資料前處理的方面，我們挑選了以下幾個feature 作為我們training、testing dataset 的 feature:\n",
    "    \n",
    "    1. author: 作者姓名\n",
    "    2. channel: 文章類別\n",
    "    3. title: 文章標題\n",
    "    4. topic: 文章細項分類\n",
    "    5. weekday: 文章在星期幾發布\n",
    "    \n",
    "    其中考慮到資料的特性，我們將author、channel、weekday 做了 one-hot encoding，並且將title以及topic分別做Feature hashing之後，將這些vector concate起來，產生training set、testing set。\n",
    "    \n",
    "<h2>How did you build the classifier, e.g. model, training algorithm, special techniques, etc?</h2>\n",
    "\n",
    "    在挑選classifer 方面，我們使用了logistic regression跟randomforest，將這兩種classifer 得到的 predict_proba 取平均。這邊想特別提一下logistic regression，原本我們使用的是scikit learn 的 SGDClassifier，但是經過比較發現logistic regression 的表現較好，我們猜測可能是因為兩個得到的結果都是線性，加上SGDClassifier是用SGD，在兩種classifer都已經收斂的情形下，logistic regression表現會較好。\n",
    "    \n",
    "<h2>Conclusions</h2>\n",
    "\n",
    "    我們這次在kaggle的競賽上，在public跟private的表現落差不小，探究原因，我們猜測可能是我們最後在極致追求在public的測資上取得些微的進步，造成classifer 跟 feature的挑選實際上都是去 fit public data，但實際上還是應該要使用例如cross_validation 之類的方法來挑選才恰當。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
