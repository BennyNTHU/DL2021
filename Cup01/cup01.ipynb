{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, roc_auc_score\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('./datalabcup1-predicting-news-popularity/train_no_html.csv')\n",
    "# df_test = pd.read_csv('./datalabcup1-predicting-news-popularity/test_no_html.csv')\n",
    "\n",
    "df_train = pd.read_csv('./train.csv')\n",
    "df_test = pd.read_csv('./test.csv')\n",
    "\n",
    "# df_train = pd.read_csv('./train_no_html.csv')\n",
    "# df_test = pd.read_csv('./test_no_html.csv')\n",
    "\n",
    "# print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "11847\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hongyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    #  lower case    \n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text).strip()\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = re.split('\\s+', text.strip())\n",
    "    \n",
    "    \n",
    "    stem_tokens = [porter.stem(w) for w in tokens if w not in stop and len(w) > 1]\n",
    "    \n",
    "    stem_text = ' '.join(stem_tokens)\n",
    "    \n",
    "    return stem_text\n",
    "\n",
    "def process_text(text):\n",
    "    words = word_tokenize(text.strip())\n",
    "    filtered_words = [w for w in words if not w.isdigit() and w not in stop and len(w)>1]\n",
    "    \n",
    "    lemmatized_word = [wnl.lemmatize(w) for w in filtered_words]\n",
    "    \n",
    "#     tags = pos_tag(filtered_words)\n",
    "#     acceptable = ['NN', 'NNS', 'VBZ', 'JJ', 'RB' , 'NNP', 'NNPS', 'RBR']\n",
    "    \n",
    "#     clean_text = ''\n",
    "#     for word , tag in tags:\n",
    "#         if tag in acceptable:\n",
    "#             clean_text = clean_text + wnl.lemmatize(word) + ' '\n",
    "    \n",
    "    return ' '.join(lemmatized_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['Page content'].apply(preprocessor)\n",
    "df_test['text'] = df_test['Page content'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_processed'] = df_train['text'].apply(process_text)\n",
    "df_test['text_processed'] = df_test['text'].apply(process_text)\n",
    "df_train = df_train.drop(columns=['Page content'])\n",
    "df_test = df_test.drop(columns=['Page content'])\n",
    "df_train.head()\n",
    "df_test.head()\n",
    "df_train.to_csv('./train_no_html.csv', index=False)\n",
    "df_test.to_csv('./test_no_html.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train['text_processed'].values\n",
    "y_train = df_train['Popularity'].values\n",
    "x_test = df_test['text_processed'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i , text in enumerate(x_train):\n",
    "#     processed_text = process_text(text)\n",
    "#     x_train[i] = processed_text\n",
    "    \n",
    "# for i , text in enumerate(x_test):\n",
    "#     processed_text = process_text(text)\n",
    "#     x_test[i] = processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(x_train))\n",
    "print(type(x_train[0]))\n",
    "x_train = list(x_train)\n",
    "x_test =  list(x_test)\n",
    "x = x_train + x_test\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorize = TfidfVectorizer(max_df=0.6, min_df=0.0001)\n",
    "tfidf_data = tfidf_vectorize.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorize.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorize.get_feature_names()\n",
    "x_train_tfidf = tfidf_vectorize.transform(x_train)\n",
    "x_test_tfidf = tfidf_vectorize.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "(27643, 65537) (27643,)\n",
      "(11847, 65537)\n"
     ]
    }
   ],
   "source": [
    "y_train[y_train==-1] = 0\n",
    "print(np.unique(y_train))\n",
    "print(x_train_tfidf.shape, y_train.shape)\n",
    "print(x_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_val , y_train, y_val = train_test_split(x_train_tfidf, y_train, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22114, 65537) (22114,) 0.4922221217328389\n",
      "(5529, 65537) (5529,) 0.4968348706818593\n",
      "(11847, 65537)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, y_train.sum()/len(y_train))\n",
    "print(x_val.shape, y_val.shape, y_val.sum()/len(y_val))\n",
    "print(x_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5161862741839539\n"
     ]
    }
   ],
   "source": [
    "#SVM classifier\n",
    "SVM = SVC(kernel = 'linear')\n",
    "SVMClassifier = SVM.fit(x_train, y_train)\n",
    "y_val_pred = SVMClassifier.predict(x_val)\n",
    "auc = roc_auc_score(y_val, y_val_pred)\n",
    "print(f'AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5092433363682543\n"
     ]
    }
   ],
   "source": [
    "#Multilayer Perceptron classfier\n",
    "NN = MLPClassifier(solver = 'lbfgs', alpha = 0.00095, learning_rate = 'adaptive', learning_rate_init = 0.005, max_iter = 300, random_state = 0)\n",
    "Perceptron = NN.fit(x_train, y_train)\n",
    "y_val_pred = Perceptron.predict(x_val)\n",
    "auc = roc_auc_score(y_val, y_val_pred)\n",
    "print(f'AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5275867903211582\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes classifier\n",
    "MNB = MultinomialNB()\n",
    "NBClassifier = MNB.fit(x_train, y_train)\n",
    "y_val_pred = NBClassifier.predict(x_val)\n",
    "auc = roc_auc_score(y_val, y_val_pred)\n",
    "print(f'AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5285473179420357\n"
     ]
    }
   ],
   "source": [
    "SVM = SVC(kernel = 'linear', probability = True)\n",
    "MNB = MultinomialNB()\n",
    "EnsembleClassifier = VotingClassifier(estimators = [('mnb', MNB), ('svc', SVM)], voting = 'soft')\n",
    "EnsembleClassifier = EnsembleClassifier.fit(x_train, y_train)\n",
    "\n",
    "y_val_pred = EnsembleClassifier.predict(x_val)\n",
    "auc = roc_auc_score(y_val, y_val_pred)\n",
    "print(f'AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_threshold(model):\n",
    "    for th in np.arange(0.3, 0.8, 0.1):\n",
    "        y_val_pred = (model.predict_proba(x_val)[:,1] >= th).astype(int)\n",
    "        auc = roc_auc_score(y_val, y_val_pred)\n",
    "        print(f'Threshold: {th} AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.3 AUC: 0.5003457140486831\n",
      "Threshold: 0.4 AUC: 0.5120439211248556\n",
      "Threshold: 0.5 AUC: 0.5285473179420357\n",
      "Threshold: 0.6000000000000001 AUC: 0.504144054150178\n",
      "Threshold: 0.7000000000000002 AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "test_threshold(EnsembleClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_output(x_test, model, th, file_name):\n",
    "    y_pred = (model.predict_proba(x_test)[:,1] >= th).astype(int)\n",
    "    y_pred[y_pred==0] = -1\n",
    "    df_submission = pd.read_csv('./sample_submission.csv')\n",
    "    df_submission['Popularity'] = y_pred\n",
    "    df_submission.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './ensemble_svm_mnb.csv'\n",
    "predict_and_output(x_test_tfidf, EnsembleClassifier, 0.5, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
