{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cup01\n",
    "組員名單：110065508李丞恩 109062676劉廷哲 110062592姜宏昀 110062539古之恒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "debug = False\n",
    "MIN = 1 # n-gram\n",
    "MAX = 1 # n-gram\n",
    "MAX_DF = 0.6\n",
    "HASH_POWER = 10 # hash to 2**HASH_POWER features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作業有部分使用到的函數寫在cup01.py中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from summa import keywords\n",
    "from summa.summarizer import summarize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from time import strptime\n",
    "from cup01 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"output/\") : os.mkdir(\"output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. 資料前處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 清掉所有的html tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 定義tokenize+波特詞幹還原演算法+刪除停用字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runner', 'like', 'run', 'thu', 'run']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/benny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "stop = stop + extra_stopwords()\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(tokenizer_stem_nostop('runners like running and thus they run')) # Test if it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 分析文章基本性質\n",
    "原本的dataset中其實隱含了非常多的資訊[1]。包含發布日期，時間，連結數量等等。因此我們利用分析html語法的方法取出這些資訊。至於抽取資訊的演算法我們實作在cup01.py之中。首先我們先將training set和較submit的test.csv引入，並疊在一起方便操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./input/train.csv')\n",
    "df2 = pd.read_csv('./input/test.csv')\n",
    "df2['Popularity'] = np.zeros(len(df2))\n",
    "df = pd.concat([df1,df2])\n",
    "train_length = len(df1)\n",
    "test_length = len(df2)\n",
    "del df1, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "檢查一下是否正常引入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt; &lt;span c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content\n",
       "0   0        -1.0  <html><head><div class=\"article-info\"> <span c...\n",
       "1   1         1.0  <html><head><div class=\"article-info\"><span cl...\n",
       "2   2         1.0  <html><head><div class=\"article-info\"><span cl...\n",
       "3   3        -1.0  <html><head><div class=\"article-info\"><span cl...\n",
       "4   4        -1.0  <html><head><div class=\"article-info\"><span cl..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_contents = df['Page content'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page content一欄中存著原本的html文件，根據[1]中的說明，抽取各種特徵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "days, pub_days, channels, img_counts, topics, authors, titles, social_media_counts, \\\n",
    "contents, num_hrefs, num_self_hrefs \\\n",
    "= get_all_datas(df_train_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_positive_words, rate_negative_words, avg_positive_polarity, min_positive_polarity, max_positive_polarity, \\\n",
    "avg_negative_polarity, min_negative_polarity, max_negative_polarity \\\n",
    "= get_word_sentiment_features(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n"
     ]
    }
   ],
   "source": [
    "n_tokens_titles, n_tokens_contents, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens \\\n",
    "= get_some_n_features(titles, contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_sentiment_polarity, global_subjectivity, title_subjectivity_list, title_sentiment_polarity_list, \\\n",
    "abs_title_subjectivity, abs_title_sentiment_polarity \\\n",
    "= get_sentiment_features(titles, contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們抽取的特徵如下所示，除了Page content，Id以及Popularity外都是我們得到的特徵，接著我們存到一個dataframe裡面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Page content':df_train_contents,\n",
    "                   'Id':df.Id[:],\n",
    "                   'Popularity':df.Popularity[:],\n",
    "                   'topic':topics,\n",
    "                   'channel':channels,\n",
    "                   'weekday':days,\n",
    "                   'pub_date' : pub_days,\n",
    "                   'author':authors,\n",
    "                   'img count':img_counts,\n",
    "                   'title':titles,\n",
    "                   'content':contents,\n",
    "                   'media count': social_media_counts,\n",
    "                   'n_tokens_title' : n_tokens_titles,\n",
    "                   'n_tokens_content': n_tokens_contents,\n",
    "                   'n_unique_tokens' : n_unique_tokens,\n",
    "                   'n_non_stop_words': n_non_stop_words,\n",
    "                   'n_non_stop_unique_tokens': n_non_stop_unique_tokens,\n",
    "                   'num_hrefs' : num_hrefs,\n",
    "                   'num_self_hrefs' : num_self_hrefs,\n",
    "                   'global_sentiment_polarity' : global_sentiment_polarity,\n",
    "                   'global_subjectivity' : global_subjectivity,\n",
    "                   'title_subjectivity' : title_subjectivity_list,\n",
    "                   'title_sentiment_polarity' : title_sentiment_polarity_list,\n",
    "                   'abs_title_subjectivity' : abs_title_subjectivity,\n",
    "                   'abs_title_sentiment_polarity' : abs_title_sentiment_polarity,\n",
    "                   'rate_positive_words' : rate_positive_words,\n",
    "                   'rate_negative_words' : rate_negative_words,\n",
    "                   'avg_positive_polarity' : avg_positive_polarity,\n",
    "                   'min_positive_polarity' : min_positive_polarity,\n",
    "                   'max_positive_polarity' : max_positive_polarity,\n",
    "                   'avg_negative_polarity' : avg_negative_polarity,\n",
    "                   'min_negative_polarity' : min_negative_polarity,\n",
    "                   'max_negative_polarity' : max_negative_polarity})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 時間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於pub_date裡面寫的是發佈時間，資料型別是字串，因此我們先將其分割成月份，日期以及發布時間。因為我們認為周末發布的新聞可能會比較多人看，因此增加一欄位判斷是否為周末。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_of_month'] = df['pub_date'].apply(lambda x: int(x.split()[1]) if x != ' noneday' else 0)\n",
    "df['month'] = df['pub_date'].apply(lambda x: strptime(x.split()[2], '%b').tm_mon if x  != ' noneday' else 0)\n",
    "df['hour'] = df['pub_date'].apply(lambda x: strptime(x.split()[4], '%X')[3] if x != ' noneday' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_weekend'] = df['weekday'].apply(lambda x: 1 if x ==' Sat' or x == ' Sun' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train_contents, df['pub_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 產生關鍵字\n",
    "由於一整篇文章太長，如果直接拿去計算tf-idf向量，記憶體會不夠。我們有嘗試將一整篇文章拿去hash，但是後來發現提取關鍵字後再把關鍵字拿去hash會表現更好。具體而言，我們使用的是summa這個套件[2]。它是基於TextRank演算法[3]找出文件裡的關鍵字。首先，我們要先清除原始文件中的html語法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 39490/39490 [01:46<00:00, 370.38it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['Page content'] = df['Page content'].progress_apply(preprocessor) # 此步驟約要花五分鐘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，使用1.2節中的函式，使用波特詞幹還原演算法並刪除停用字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 39490/39490 [02:39<00:00, 248.36it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['keywords'] = df['Page content'].progress_apply(tokenizer_stem_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 39490/39490 [00:00<00:00, 228611.07it/s]\n"
     ]
    }
   ],
   "source": [
    "df['keywords'] = df['keywords'].progress_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算出關鍵字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 39490/39490 [26:58<00:00, 24.40it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x: keywords.keywords(x).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 作者評價"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著我們計算每個作者平均得到的分數。(
    幫我找一下這邊的參考資料)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'author'\n",
    "df2 = df.iloc[:train_length].groupby(f'{col}').mean().reset_index().sort_values(by='Popularity', ascending=False)\\\n",
    "    [[f'{col}', 'Popularity']]\n",
    "df2.columns=[f'{col}', 'avg_popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_avg_score = {}\n",
    "for i, row in df2.iterrows():\n",
    "    author_name = row['author']\n",
    "    score = row['avg_popularity']\n",
    "    author_avg_score[author_name] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加到dataframe裡面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_popularity'] = df['author'].apply(lambda x: author_avg_score[x] if x in author_avg_score else 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df就是我們最後得到的dataframe。不包含原始文章以及ID，popularity，一共有35個特徵。但是有些特徵還必須另外再處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page content</th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>keywords</th>\n",
       "      <th>author_popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clara moskowitz for space com 2013 06 19 15 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>There may be killer asteroids headed for Eart...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.153571</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>nasa space mission said challeng stop asteroid...</td>\n",
       "      <td>-0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by christina warren2013 03 28 17 40 55 utcgoog...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>Google took a stand of sorts against patent-l...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.130000</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>googl softwar new open sourc patent pledg sue ...</td>\n",
       "      <td>-0.002506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>by sam laird2014 05 07 19 15 20 utcballin 2014...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>You've spend countless hours training to be a...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.433992</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>bonu youtub funniest sport fail win nfl draft ...</td>\n",
       "      <td>-0.131474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>by sam laird2013 10 11 02 26 50 utccameraperso...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>1</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>Tired of the same old sports fails and ne...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.432727</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>fail sport video second youtub laugh entertain...</td>\n",
       "      <td>-0.131474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by connor finnegan2014 04 17 03 31 43 utcnfl s...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>52</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>At 6-foot-5 and 298 pounds, All-Pro NFL star ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.303175</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>stadium imag getti game watt world championshi...</td>\n",
       "      <td>0.149425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Page content  Id  Popularity  \\\n",
       "0   clara moskowitz for space com 2013 06 19 15 0...   0        -1.0   \n",
       "1  by christina warren2013 03 28 17 40 55 utcgoog...   1         1.0   \n",
       "2  by sam laird2014 05 07 19 15 20 utcballin 2014...   2         1.0   \n",
       "3  by sam laird2013 10 11 02 26 50 utccameraperso...   3        -1.0   \n",
       "4  by connor finnegan2014 04 17 03 31 43 utcnfl s...   4        -1.0   \n",
       "\n",
       "                                               topic        channel weekday  \\\n",
       "0  Asteroid Asteroids challenge Earth Space U.S. ...          world     Wed   \n",
       "1  Apps and Software Google open source opn pledg...           tech     Thu   \n",
       "2      Entertainment NFL NFL Draft Sports Television  entertainment     Wed   \n",
       "3                    Sports Video Videos Watercooler    watercooler     Fri   \n",
       "4  Entertainment instagram instagram video NFL Sp...  entertainment     Thu   \n",
       "\n",
       "             author  img count  \\\n",
       "0               NaN          1   \n",
       "1  Christina Warren          2   \n",
       "2         Sam Laird          2   \n",
       "3         Sam Laird          1   \n",
       "4   Connor Finnegan         52   \n",
       "\n",
       "                                               title  \\\n",
       "0  NASA's Grand Challenge: Stop Asteroids From De...   \n",
       "1  Google's New Open Source Patent Pledge: We Won...   \n",
       "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
       "3        Cameraperson Fails Deliver Slapstick Laughs   \n",
       "4  NFL Star Helps Young Fan Prove Friendship With...   \n",
       "\n",
       "                                             content  ...  \\\n",
       "0   There may be killer asteroids headed for Eart...  ...   \n",
       "1   Google took a stand of sorts against patent-l...  ...   \n",
       "2   You've spend countless hours training to be a...  ...   \n",
       "3       Tired of the same old sports fails and ne...  ...   \n",
       "4   At 6-foot-5 and 298 pounds, All-Pro NFL star ...  ...   \n",
       "\n",
       "   max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
       "0                    0.8              -0.153571                  -0.25   \n",
       "1                    0.7              -0.130000                  -0.25   \n",
       "2                    1.0              -0.433992                  -1.00   \n",
       "3                    0.5              -0.432727                  -0.60   \n",
       "4                    1.0              -0.303175                  -0.40   \n",
       "\n",
       "   max_negative_polarity  day_of_month  month  hour  is_weekend  \\\n",
       "0                 -0.125            19      6    15           0   \n",
       "1                 -0.050            28      3    17           0   \n",
       "2                 -0.050             7      5    19           0   \n",
       "3                 -0.150            11     10     2           0   \n",
       "4                 -0.050            17      4     3           0   \n",
       "\n",
       "                                            keywords  author_popularity  \n",
       "0  nasa space mission said challeng stop asteroid...          -0.034483  \n",
       "1  googl softwar new open sourc patent pledg sue ...          -0.002506  \n",
       "2  bonu youtub funniest sport fail win nfl draft ...          -0.131474  \n",
       "3  fail sport video second youtub laugh entertain...          -0.131474  \n",
       "4  stadium imag getti game watt world championshi...           0.149425  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存成一個csv檔案方便使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./input/input_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二. 特徵選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/input_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 找出頻率最高的詞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先先將經過詞幹還原以及刪除停用字的文章用CountVectorizer，計算每個單字出現在一文件裡的次數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(preprocessor=<function preprocessor at 0x7f1e906a21f0>,\n",
       "                tokenizer=<function tokenizer_stem_nostop at 0x7f1e906a2670>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range=(1, 1), # (MIN, MAX)\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "count.fit([\"YEAH TIGER\", \"FIBER WIPER\"]) # need to fit something first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本來想找出最頻繁出現的幾個詞，可惜記憶體不夠QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df['Page content']\n",
    "doc_bag = count.fit_transform(doc).toarray()\n",
    "\n",
    "print(\"[most frequent vocabularies]\")\n",
    "bag_cnts = np.sum(doc_bag, axis=0)\n",
    "top = 10\n",
    "# [::-1] reverses a list since sort is in ascending order\n",
    "for tok, v in zip(count.inverse_transform(np.ones(bag_cnts.shape[0]).reshape(1, -1))[0][bag_cnts.argsort()[::-1][:top]], \n",
    "                  np.sort(bag_cnts)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 基於整個文本的TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用前面所定義的前處理方法產生tf-idf向量。不過一樣因為記憶體不夠無法執行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df['Page content']\n",
    "tfidf = TfidfVectorizer(ngram_range=(MIN, MAX), \n",
    "                        preprocessor=preprocessor, \n",
    "                        tokenizer=tokenizer_stem_nostop, \n",
    "                        max_df=MAX_DF, \n",
    "                        min_df=0.0001)\n",
    "tfidf.fit(doc)\n",
    "doc_tfidf = tfidf.transform(doc).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡原本預計要調查idf分數以及tf-idf值最大的10個單字。不過一樣因為記憶體不夠無法執行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones(tfidf_sum.shape[0]).reshape(1, -1))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del doc_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 基於關鍵字的tf-idf\n",
    "我們原本想說如果只從關鍵字來計算tf-idf會不會好一點，但後來我們發現還是會遇到記憶體不夠的問題，只好作罷。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df['keywords']\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                        preprocessor=preprocessor, \n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "tfidf.fit(doc)\n",
    "doc_tfidf = tfidf.transform(doc).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原本預計要找出idf分數以及tf-idf值最大的10個關鍵字。不過一樣因為記憶體不夠無法執行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones(tfidf_sum.shape[0]).reshape(1, -1))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del doc_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature Hashing\n",
    "將關鍵字hash到1024個bucket裡面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df['keywords']\n",
    "hashvec = HashingVectorizer(n_features=2**HASH_POWER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39490, 1024)\n"
     ]
    }
   ],
   "source": [
    "doc_hash = hashvec.transform(doc).toarray() # HashingVectorizer不需要先.fit過一次。\n",
    "print(doc_hash.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將title也hash到1024個bucket裡面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df['title']\n",
    "hashvec = HashingVectorizer(n_features=2**HASH_POWER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39490, 1024)\n"
     ]
    }
   ],
   "source": [
    "doc_hash_title = hashvec.transform(doc).toarray()\n",
    "print(doc_hash_title.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 One hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著我們對新聞的類別(channel)，發布的星期以及作者做one-hot encoding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39490, 34)\n",
      "(39490, 8)\n",
      "(39490, 470)\n"
     ]
    }
   ],
   "source": [
    "# channel\n",
    "channel_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "channel_str = channel_ohe.fit_transform(df['channel'].values.reshape(-1,1)).toarray()\n",
    "print(channel_str.shape)\n",
    "\n",
    "# weekday\n",
    "weekday_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "weekday_str = weekday_ohe.fit_transform(df['weekday'].values.reshape(-1,1)).toarray()\n",
    "print(weekday_str.shape)\n",
    "\n",
    "# ohe author\n",
    "author_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "author_str = author_ohe.fit_transform(df['author'].values.reshape(-1,1)).toarray()\n",
    "print(author_str.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把所有特徵給組合起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_count = df['img count'].values.reshape(-1,1)\n",
    "media_count = df['media count'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 39490/39490 [00:07<00:00, 5353.98it/s]\n"
     ]
    }
   ],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "df_X = []\n",
    "for i in tqdm(range(len(channel_str))):\n",
    "    temp = []\n",
    "    temp.append(img_count[i])\n",
    "    temp.append(media_count[i])\n",
    "    temp.append(channel_str[i])\n",
    "    temp.append(weekday_str[i])\n",
    "    temp.append(author_str[i])\n",
    "    temp.append(doc_hash[i])\n",
    "    temp.append(doc_hash_title[i])\n",
    "    temp = flatten(temp)\n",
    "    temp.append(df['day_of_month'][i])\n",
    "    temp.append(df['month'][i])\n",
    "    temp.append(df['hour'][i])\n",
    "    temp.append(df['n_tokens_title'][i])\n",
    "    temp.append(df['n_tokens_content'][i])\n",
    "    temp.append(df['n_unique_tokens'][i])\n",
    "    temp.append(df['n_non_stop_words'][i])\n",
    "    temp.append(df['n_non_stop_unique_tokens'][i])\n",
    "    temp.append(df['num_hrefs'][i])\n",
    "    temp.append(df['num_self_hrefs'][i])\n",
    "    temp.append(df['global_sentiment_polarity'][i])\n",
    "    temp.append(df['global_subjectivity'][i])\n",
    "    temp.append(df['title_subjectivity'][i])\n",
    "    temp.append(df['title_sentiment_polarity'][i])\n",
    "    temp.append(df['abs_title_subjectivity'][i])\n",
    "    temp.append(df['abs_title_sentiment_polarity'][i])\n",
    "    temp.append(df['rate_positive_words'][i])\n",
    "    temp.append(df['rate_negative_words'][i])\n",
    "    temp.append(df['avg_positive_polarity'][i])\n",
    "    temp.append(df['min_positive_polarity'][i])\n",
    "    temp.append(df['max_positive_polarity'][i])\n",
    "    temp.append(df['avg_negative_polarity'][i])\n",
    "    temp.append(df['min_negative_polarity'][i])\n",
    "    temp.append(df['max_negative_polarity'][i])\n",
    "    temp.append(df['is_weekend'][i])\n",
    "    temp.append(df['author_popularity'][i])\n",
    "    df_X.append(temp)\n",
    "    del temp\n",
    "\n",
    "df_y = df['Popularity'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, doc_hash, img_count, media_count, channel_str, weekday_str, author_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39490 2588\n"
     ]
    }
   ],
   "source": [
    "print(len(df_X),len(df_X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存成csv檔案就不用再重跑之前的code了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_X).to_csv(\"./input/X_data.csv\", index=False, header=False)\n",
    "pd.DataFrame(df_y).to_csv(\"./input/y_data.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成training set和testing set。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三. 模型訓練\n",
    "訓練模型時掛掉的話直接從這裡開始跑。首先先引入已經經過處理的dataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.read_csv('./input/X_data.csv', header=None).values\n",
    "df_y = pd.read_csv('./input/y_data.csv', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先切出要做預測的部分X_submit。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submit = df_X[train_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割成training set與testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X[:train_length], \n",
    "                                                    df_y[:train_length], \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_X\n",
    "del df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 繪製解釋變異數圖形\n",
    "為了判斷是否要進行PCA降維，我們先繪製解釋變異數比率的圖形。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = np.cov(X_train_std.T) # 計算特徵值\n",
    "eigen_vals, _ = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 把explained variance(lambda i / lambda和)由大排到小\n",
    "tot = sum(eigen_vals)\n",
    "var_exp = np.array([(i / tot) for i in sorted(eigen_vals, reverse=True)])\n",
    "cum_var_exp = np.cumsum(var_exp) # 計算解釋變異數\n",
    "\n",
    "# 繪圖\n",
    "plt.bar(range(len(eigen_vals)), var_exp, alpha=0.5, align='center',\n",
    "        label='individual explained variance')\n",
    "plt.step(range(len(eigen_vals)), cum_var_exp, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_02.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果取解釋變異數比率0.6的話那麼可以降維到1200維，不過我們實驗後發現PCA不會有效增加AUC。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_std, X_test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Logistic Regression\n",
    "我們使用邏輯斯回歸以及江湖人稱kaggle神器的XGboost[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                        #PCA(n_components=1000),\n",
    "                        LogisticRegression(random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_xgb = np.array(X_train)\n",
    "X_test_xgb = np.array(X_test)\n",
    "y_train_xgb = y_train\n",
    "y_train_xgb[y_train==-1] = 0\n",
    "y_test_xgb = y_test\n",
    "y_test_xgb[y_test==-1] = 0\n",
    "d_train = xgb.DMatrix(X_train_xgb, y_train_xgb)\n",
    "d_test = xgb.DMatrix(X_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'eta': 0.05, \n",
    "              'max_depth': 4, \n",
    "              'subsample': 0.7 ,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'min_child_weight' : 4,\n",
    "              'objective': 'binary:logistic', \n",
    "              'eval_metric': 'auc',\n",
    "              'alpha': 0.005,\n",
    "             }\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_params = xgb_model.get_xgb_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四. 參數調整\n",
    "我們利用GridSearchCV來尋找兩個分類的最佳參數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5699055260256226\n",
      "{'logisticregression__C': 0.0001, 'logisticregression__class_weight': None, 'logisticregression__penalty': 'l2', 'logisticregression__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] \n",
    "param_grid = [{'logisticregression__C': param_range, \n",
    "               'logisticregression__penalty': ['l2'],\n",
    "               'logisticregression__class_weight': ['balanced', None],\n",
    "               'logisticregression__solver': ['saga', 'lbfgs']}]\n",
    "\n",
    "# 使用網格搜尋法\n",
    "gs = GridSearchCV(estimator=pipe_lr, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc', \n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train.ravel())\n",
    "\n",
    "# 輸出結果\n",
    "print(gs.best_score_) \n",
    "print(gs.best_params_) # 最佳參數組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=0.0001, random_state=0))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = gs.best_estimator_ # 取得剛剛算出的最佳參數\n",
    "clf.fit(X_train, y_train) # 用這個參數train出model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvresult = xgb.cv(xgb_params, \n",
    "                  d_train, \n",
    "                  num_boost_round=1000, \n",
    "                  verbose_eval=10, \n",
    "                  nfold=5, \n",
    "                  metrics=['auc'],\n",
    "                  early_stopping_rounds=50, \n",
    "                  stratified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.005, base_score=None, booster=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, eta=0.05, eval_metric='auc', gamma=None,\n",
       "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=4,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=71, n_jobs=None, num_parallel_tree=None,\n",
       "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "              scale_pos_weight=None, subsample=0.7, tree_method=None,\n",
       "              validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.005, base_score=0.5, booster='gbtree',\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
       "              eta=0.05, eval_metric='auc', gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.0500000007, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=71, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0.00499999989, reg_lambda=1, scale_pos_weight=1,\n",
       "              subsample=0.7, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X_train_xgb, y_train_xgb, eval_metric='auc', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五. 結果預測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.560\n",
      "AUC score: 0.535 (+/-0.022)\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "scores = cross_val_score(estimator=clf, \n",
    "                         X=X_test, \n",
    "                         y=y_test, \n",
    "                         cv=10, \n",
    "                         verbose = 0,\n",
    "                         scoring='roc_auc')\n",
    "print('AUC score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.574\n",
      "AUC score: 0.583 (+/-0.014)\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_model.predict_proba(X_test_xgb)[:,1]\n",
    "print('Test Accuracy: %.3f' % xgb_model.score(X_test_xgb, y_test_xgb))\n",
    "scores = cross_val_score(estimator=xgb_model, \n",
    "                         X=X_test_xgb, \n",
    "                         y=y_test_xgb, \n",
    "                         cv=10, \n",
    "                         verbose = 0,\n",
    "                         scoring='roc_auc')\n",
    "print('AUC score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以發現XGboost的表現比邏輯斯回歸好。這是因為XGboost是一種組合學習[5]，理論上會比單一模型表現更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六. 繳交結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submit = np.array(X_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit = xgb_model.predict_proba(X_submit)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv('./output/sample_submission.csv')\n",
    "df_submission['Popularity'] = y_submit\n",
    "df_submission.to_csv('./output/submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "繳交到Kaggle後發現ROC-AUC分數為0.5616。不過若是不把關鍵字加到特徵裡train的話，分數可以達到0.5806。本組的實驗記錄位於[6]中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七. 結論\n",
    "\n",
    "我認為本cup最大的用意在於讓同學練習如何分析html的語法並提取出有用的資訊。如果能很熟練地完成這份工作的話，就可以額外發想一些feature。例如：文章關鍵字，或是可以調查在指定一個單字(例如Trump)下，文章popularity為1或-1的機率。另外也可以討論究竟要把關鍵字或是文章hash到多大的bucket才是最適合的。\n",
    "(可幫我補一些討論)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 八. 參考資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Online News Popularity Data Set, https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity?fbclid=IwAR0DxK_HXRkYxR_nGOUzKg4QBk62-FFluqgem9LqmOWCLCDVlyq3G_7j-Io#\n",
    "\n",
    "[2] Summa - Textrank: TextRank implementation in Python, https://summanlp.github.io/textrank/\n",
    "\n",
    "[3] R. Mihalcea and P. Tarau, TextRank: Bringing Order into Texts. https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "\n",
    "[4] https://github.com/benjamin-awd/NYT-Article-Popularity\n",
    "\n",
    "[5] T. Chen and C. Guestrin, XGBoost: A Scalable Tree Boosting System. https://arxiv.org/pdf/1603.02754.pdf\n",
    "\n",
    "[6] https://hackmd.io/rvdrCRN-R8-myReH8wldpw?view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
