{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee524c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "debug = False\n",
    "MIN = 1 # n-gram\n",
    "MAX = 1 # n-gram\n",
    "MAX_DF = 0.6\n",
    "HASH_POWER = 10 # hash to 2**HASH_POWER features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce311235",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from summa import keywords\n",
    "from summa.summarizer import summarize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b2afffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"output/\") : os.mkdir(\"output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60804d",
   "metadata": {},
   "source": [
    "## 一. 資料前處理\n",
    "首先先引入dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d62373",
   "metadata": {},
   "source": [
    "### 1.1 清掉所有的html tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd400ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aaabd5",
   "metadata": {},
   "source": [
    "### 1.2 定義tokenize+波特詞幹還原演算法+刪除停用字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d419e143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runner', 'like', 'run', 'thu', 'run']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/benny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(tokenizer_stem_nostop('runners like running and thus they run')) # Test if it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cadb6",
   "metadata": {},
   "source": [
    "### 1.3 html語法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd8b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datetime(text=[]):\n",
    "    day = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        if soup.time.has_attr('datetime'):\n",
    "            date = soup.time.attrs['datetime']\n",
    "            day.append(' '+ date[0:3])\n",
    "        else:\n",
    "            day.append(' fuckday')\n",
    "    return day\n",
    "\n",
    "def fetch_channel(text=[]):\n",
    "    channels = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        channel = soup.article['data-channel']\n",
    "        channels.append(channel)\n",
    "    return channels\n",
    "\n",
    "def fetch_img_count(text=[]):\n",
    "    count = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        c = 0\n",
    "        find_all_images = soup.find_all('img')\n",
    "        for i in find_all_images:\n",
    "            c = c+1\n",
    "        count.append(c)\n",
    "    return count\n",
    "\n",
    "def fetch_topics(text=[]):\n",
    "    topics = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        footer = soup.footer\n",
    "        ta = footer.find_all('a')\n",
    "        topic = []\n",
    "        for t in ta:\n",
    "            topic.append(t.get_text())\n",
    "        ff = ' '.join(topic)\n",
    "        topics.append(ff)\n",
    "    return topics\n",
    "\n",
    "def fetch_authors(text=[]):\n",
    "    authors = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        footer = soup.span\n",
    "        if footer != None:\n",
    "            ta = footer.findAll('a')\n",
    "            author = []\n",
    "            for t in ta:\n",
    "                author.append(t.get_text())\n",
    "            if len(author) == 0:\n",
    "                ff = 'NaN'\n",
    "            else:\n",
    "                ff = ''.join(author)\n",
    "        else:\n",
    "            ff = 'NaN'\n",
    "        authors.append(ff)\n",
    "    return authors\n",
    "\n",
    "def fetch_titles(text=[]):\n",
    "    titles = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        footer = soup.h1\n",
    "        if footer != None:\n",
    "            ff = footer.get_text()\n",
    "        else:\n",
    "            ff = 'NaN'\n",
    "        titles.append(ff) \n",
    "    return titles\n",
    "\n",
    "def fetch_social_media_count(text=[]):\n",
    "    count = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        c = 0\n",
    "        for frame in soup(\"iframe\"):\n",
    "#             print(frame.get('src').split(\".\"))\n",
    "            if frame.get('src').find(\"youtube\") != None:\n",
    "                c = c+1\n",
    "            elif frame.get('src').find(\"instagram\") != None:\n",
    "                c = c+1\n",
    "            elif frame.get('src').find(\"vine\") != None:\n",
    "                c= c+1\n",
    "            # apply new media here\n",
    "        count.append(c)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa74563",
   "metadata": {},
   "source": [
    "### 1.4 分析文章基本性質\n",
    "例如作者,圖片數量等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00422646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./input/train.csv')\n",
    "if debug:\n",
    "    df = df.iloc[:100] # debug\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74046a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fatch deature, 這裡可能需要15分鐘左右\n",
    "topic_batch = fetch_topics(df[:]['Page content'])\n",
    "channel_batch = fetch_channel(df[:]['Page content'])\n",
    "weekday_batch = fetch_datetime(df[:]['Page content'])\n",
    "author_batch = fetch_authors(df[:]['Page content'])\n",
    "img_batch = fetch_img_count(df[:]['Page content'])\n",
    "title_batch = fetch_titles(df[:]['Page content'])\n",
    "media_batch = fetch_social_media_count(df[:]['Page content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d90e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-consturct training data\n",
    "df_feature = pd.DataFrame({'topic':topic_batch,\n",
    "                           'channel':channel_batch,\n",
    "                           'weekday':weekday_batch,\n",
    "                           'author':author_batch,\n",
    "                           'img count':img_batch,\n",
    "                           'title':title_batch,\n",
    "                           'media count': media_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb439b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>media count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>1</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>52</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               topic        channel weekday  \\\n",
       "0  Asteroid Asteroids challenge Earth Space U.S. ...          world     Wed   \n",
       "1  Apps and Software Google open source opn pledg...           tech     Thu   \n",
       "2      Entertainment NFL NFL Draft Sports Television  entertainment     Wed   \n",
       "3                    Sports Video Videos Watercooler    watercooler     Fri   \n",
       "4  Entertainment instagram instagram video NFL Sp...  entertainment     Thu   \n",
       "\n",
       "             author  img count  \\\n",
       "0               NaN          1   \n",
       "1  Christina Warren          2   \n",
       "2         Sam Laird          2   \n",
       "3         Sam Laird          1   \n",
       "4   Connor Finnegan         52   \n",
       "\n",
       "                                               title  media count  \n",
       "0  NASA's Grand Challenge: Stop Asteroids From De...            0  \n",
       "1  Google's New Open Source Patent Pledge: We Won...            0  \n",
       "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...           25  \n",
       "3        Cameraperson Fails Deliver Slapstick Laughs           21  \n",
       "4  NFL Star Helps Young Fan Prove Friendship With...            1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_feature.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cad556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_feature, lsuffix='_caller', rsuffix='_other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ec1bea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 27643/27643 [01:21<00:00, 338.83it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['Page content'] = df['Page content'].progress_apply(preprocessor) # 此步驟約要花五分鐘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91747aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>media count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>clara moskowitz for space com 2013 06 19 15 0...</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>by christina warren2013 03 28 17 40 55 utcgoog...</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>by sam laird2014 05 07 19 15 20 utcballin 2014...</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>by sam laird2013 10 11 02 26 50 utccameraperso...</td>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>1</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>by connor finnegan2014 04 17 03 31 43 utcnfl s...</td>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>52</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content  \\\n",
       "0   0          -1   clara moskowitz for space com 2013 06 19 15 0...   \n",
       "1   1           1  by christina warren2013 03 28 17 40 55 utcgoog...   \n",
       "2   2           1  by sam laird2014 05 07 19 15 20 utcballin 2014...   \n",
       "3   3          -1  by sam laird2013 10 11 02 26 50 utccameraperso...   \n",
       "4   4          -1  by connor finnegan2014 04 17 03 31 43 utcnfl s...   \n",
       "\n",
       "                                               topic        channel weekday  \\\n",
       "0  Asteroid Asteroids challenge Earth Space U.S. ...          world     Wed   \n",
       "1  Apps and Software Google open source opn pledg...           tech     Thu   \n",
       "2      Entertainment NFL NFL Draft Sports Television  entertainment     Wed   \n",
       "3                    Sports Video Videos Watercooler    watercooler     Fri   \n",
       "4  Entertainment instagram instagram video NFL Sp...  entertainment     Thu   \n",
       "\n",
       "             author  img count  \\\n",
       "0               NaN          1   \n",
       "1  Christina Warren          2   \n",
       "2         Sam Laird          2   \n",
       "3         Sam Laird          1   \n",
       "4   Connor Finnegan         52   \n",
       "\n",
       "                                               title  media count  \n",
       "0  NASA's Grand Challenge: Stop Asteroids From De...            0  \n",
       "1  Google's New Open Source Patent Pledge: We Won...            0  \n",
       "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...           25  \n",
       "3        Cameraperson Fails Deliver Slapstick Laughs           21  \n",
       "4  NFL Star Helps Young Fan Prove Friendship With...            1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2798b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacfd2f8",
   "metadata": {},
   "source": [
    "### 1.5 產生關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38a682eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 27643/27643 [01:45<00:00, 261.81it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['keywords'] = df['Page content'].progress_apply(tokenizer_stem_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b40337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 27643/27643 [00:00<00:00, 224492.84it/s]\n"
     ]
    }
   ],
   "source": [
    "df['keywords'] = df['keywords'].progress_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71f27595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 27643/27643 [20:57<00:00, 21.99it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x: keywords.keywords(x).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6043734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>media count</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>clara moskowitz for space com 2013 06 19 15 0...</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>0</td>\n",
       "      <td>nasa space mission said challeng stop asteroid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>by christina warren2013 03 28 17 40 55 utcgoog...</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>0</td>\n",
       "      <td>googl softwar new open sourc patent pledg sue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>by sam laird2014 05 07 19 15 20 utcballin 2014...</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>25</td>\n",
       "      <td>bonu youtub funniest sport fail win nfl draft ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>by sam laird2013 10 11 02 26 50 utccameraperso...</td>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>1</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>21</td>\n",
       "      <td>fail video sport second youtub need littl laug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>by connor finnegan2014 04 17 03 31 43 utcnfl s...</td>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>52</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>1</td>\n",
       "      <td>stadium imag getti game watt world championshi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content  \\\n",
       "0   0          -1   clara moskowitz for space com 2013 06 19 15 0...   \n",
       "1   1           1  by christina warren2013 03 28 17 40 55 utcgoog...   \n",
       "2   2           1  by sam laird2014 05 07 19 15 20 utcballin 2014...   \n",
       "3   3          -1  by sam laird2013 10 11 02 26 50 utccameraperso...   \n",
       "4   4          -1  by connor finnegan2014 04 17 03 31 43 utcnfl s...   \n",
       "\n",
       "                                               topic        channel weekday  \\\n",
       "0  Asteroid Asteroids challenge Earth Space U.S. ...          world     Wed   \n",
       "1  Apps and Software Google open source opn pledg...           tech     Thu   \n",
       "2      Entertainment NFL NFL Draft Sports Television  entertainment     Wed   \n",
       "3                    Sports Video Videos Watercooler    watercooler     Fri   \n",
       "4  Entertainment instagram instagram video NFL Sp...  entertainment     Thu   \n",
       "\n",
       "             author  img count  \\\n",
       "0               NaN          1   \n",
       "1  Christina Warren          2   \n",
       "2         Sam Laird          2   \n",
       "3         Sam Laird          1   \n",
       "4   Connor Finnegan         52   \n",
       "\n",
       "                                               title  media count  \\\n",
       "0  NASA's Grand Challenge: Stop Asteroids From De...            0   \n",
       "1  Google's New Open Source Patent Pledge: We Won...            0   \n",
       "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...           25   \n",
       "3        Cameraperson Fails Deliver Slapstick Laughs           21   \n",
       "4  NFL Star Helps Young Fan Prove Friendship With...            1   \n",
       "\n",
       "                                            keywords  \n",
       "0  nasa space mission said challeng stop asteroid...  \n",
       "1  googl softwar new open sourc patent pledg sue ...  \n",
       "2  bonu youtub funniest sport fail win nfl draft ...  \n",
       "3  fail video sport second youtub need littl laug...  \n",
       "4  stadium imag getti game watt world championshi...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7545af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./input/input_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946c107",
   "metadata": {},
   "source": [
    "可以丟下去做onehot的有channel，weekday，img count，media count,因為topic和title已經包含在文章內了,而我們取關鍵字是連著他們一起取的."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f91df3",
   "metadata": {},
   "source": [
    "## 二. 特徵選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "536854fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/input_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491dfea",
   "metadata": {},
   "source": [
    "### 2.1 找出頻率最高的詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6057f13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(preprocessor=<function preprocessor at 0x00000149A614A8B0>,\n",
       "                tokenizer=<function tokenizer_stem_nostop at 0x00000149A1AE6940>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range=(1, 1), # (MIN, MAX)\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "count.fit([\"YEAH TIGER\", \"FIBER WIPER\"]) # need to fit something first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e6f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[most frequent vocabularies]\n",
      "imag: 116996\n",
      "also: 51161\n",
      "new: 44159\n",
      "one: 42492\n",
      "video: 41798\n",
      "see: 38955\n",
      "like: 36858\n",
      "time: 35997\n",
      "use: 33510\n",
      "app: 32685\n"
     ]
    }
   ],
   "source": [
    "doc = df['Page content']\n",
    "doc_bag = count.fit_transform(doc).toarray()\n",
    "\n",
    "print(\"[most frequent vocabularies]\")\n",
    "bag_cnts = np.sum(doc_bag, axis=0)\n",
    "top = 10\n",
    "# [::-1] reverses a list since sort is in ascending order\n",
    "for tok, v in zip(count.inverse_transform(np.ones(bag_cnts.shape[0]).reshape(1, -1))[0][bag_cnts.argsort()[::-1][:top]], \n",
    "                  np.sort(bag_cnts)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2fc53",
   "metadata": {},
   "source": [
    "### 2.2 基於整個文本的TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22fb742",
   "metadata": {},
   "source": [
    "利用前面所定義的前處理方法產生tf-idf向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b99c3a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "doc = df['Page content']\n",
    "tfidf = TfidfVectorizer(ngram_range=(MIN, MAX), \n",
    "                        preprocessor=preprocessor, \n",
    "                        tokenizer=tokenizer_stem_nostop, \n",
    "                        max_df=MAX_DF, \n",
    "                        min_df=0.0001)\n",
    "tfidf.fit(doc)\n",
    "doc_tfidf = tfidf.transform(doc).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4079cf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 43634)\n"
     ]
    }
   ],
   "source": [
    "print(doc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "528591d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"tfidf.csv\", doc_tfidf, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ac742",
   "metadata": {},
   "source": [
    "接著調查idf分數以及tf-idf值最大的10個單字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31ac39f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "new: 1.56\n",
      "like: 1.60\n",
      "time: 1.61\n",
      "make: 1.72\n",
      "year: 1.73\n",
      "world: 1.75\n",
      "use: 1.77\n",
      "get: 1.79\n",
      "first: 1.85\n",
      "take: 1.90\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "video: 733.5061786531768\n",
      "app: 602.2534993946733\n",
      "new: 500.9036433140395\n",
      "googl: 455.2371136820567\n",
      "game: 442.6232487233518\n",
      "twitter: 414.0212743230195\n",
      "facebook: 410.4951735218714\n",
      "compani: 403.128044066731\n",
      "appl: 401.1395615393847\n",
      "time: 400.9980301873785\n"
     ]
    }
   ],
   "source": [
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones(tfidf_sum.shape[0]).reshape(1, -1))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del doc_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7dd108",
   "metadata": {},
   "source": [
    "### 2.3 基於關鍵字的tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc1b5402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "doc = df['keywords']\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                        preprocessor=preprocessor, \n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "tfidf.fit(doc)\n",
    "doc_tfidf = tfidf.transform(doc).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "067e34da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 39431)\n"
     ]
    }
   ],
   "source": [
    "print(doc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea9abfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "imag: 1.94\n",
      "new: 2.23\n",
      "like: 2.35\n",
      "time: 2.47\n",
      "use: 2.55\n",
      "video: 2.59\n",
      "make: 2.61\n",
      "year: 2.66\n",
      "said: 2.70\n",
      "compani: 2.79\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "imag: 639.4314030667334\n",
      "new: 543.6068855046963\n",
      "video: 487.298715875056\n",
      "like: 460.2245497275674\n",
      "time: 431.4077628796134\n",
      "use: 427.29808775659825\n",
      "compani: 402.0364173369534\n",
      "make: 393.66282963562537\n",
      "year: 391.85262234595297\n",
      "said: 379.9360365379158\n"
     ]
    }
   ],
   "source": [
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones(tfidf_sum.shape[0]).reshape(1, -1))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c3fb1",
   "metadata": {},
   "source": [
    "### 2.4 Feature Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d5f53db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 1024)\n"
     ]
    }
   ],
   "source": [
    "# hash words to 1024 buckets\n",
    "#doc = df['keywords']\n",
    "doc = df['Page content']\n",
    "hashvec = HashingVectorizer(n_features=2**HASH_POWER,\n",
    "                            preprocessor=preprocessor,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "# no .fit needed for HashingVectorizer, since it's defined by the hash function\n",
    "# transform sentences to vectors of dimension 1024\n",
    "doc_hash = hashvec.transform(doc).toarray()\n",
    "print(doc_hash.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042fe77b",
   "metadata": {},
   "source": [
    "### 2.5 One hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26548fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 33)\n",
      "(27643, 7)\n",
      "(27643, 428)\n"
     ]
    }
   ],
   "source": [
    "# channel\n",
    "channel_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "channel_str = channel_ohe.fit_transform(df['channel'].values.reshape(-1,1)).toarray()\n",
    "print(channel_str.shape)\n",
    "\n",
    "# weekday\n",
    "weekday_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "weekday_str = weekday_ohe.fit_transform(df['weekday'].values.reshape(-1,1)).toarray()\n",
    "print(weekday_str.shape)\n",
    "\n",
    "# ohe author\n",
    "author_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "author_str = author_ohe.fit_transform(df['author'].values.reshape(-1,1)).toarray()\n",
    "print(author_str.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8ac17",
   "metadata": {},
   "source": [
    "把所有特徵給組合起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "700b85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_count = df['img count'].values.reshape(-1,1)\n",
    "media_count = df['media count'].values.reshape(-1,1)\n",
    "df_y_train = df['Popularity'].to_numpy()\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6c569cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 27643/27643 [00:01<00:00, 14629.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "df_X_train = []\n",
    "for i in tqdm(range(len(channel_str))):\n",
    "    temp = []\n",
    "    temp.append(img_count[i])\n",
    "    temp.append(media_count[i])\n",
    "    temp.append(channel_str[i])\n",
    "    temp.append(weekday_str[i])\n",
    "    temp.append(author_str[i])\n",
    "    temp.append(doc_hash[i])\n",
    "    #temp.append(doc_tfidf[i])\n",
    "    df_X_train.append(flatten(temp))\n",
    "    del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "608d09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del doc_hash\n",
    "#del doc_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3f872fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643 1494\n"
     ]
    }
   ],
   "source": [
    "print(len(df_X_train),len(df_X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcad4b",
   "metadata": {},
   "source": [
    "存成csv檔案就不用再重跑之前的code了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "922fab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_X_train).to_csv(\"./input/X_train.csv\", index=False, header=False)\n",
    "pd.DataFrame(df_y_train).to_csv(\"./input/y_train.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc08a55",
   "metadata": {},
   "source": [
    "生成training set和testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963df3e7",
   "metadata": {},
   "source": [
    "## 三. 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ca1eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_X_train = pd.read_csv('./input/df_X_train.csv')\n",
    "#df_y_train = pd.read_csv('./input/df_y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97e000e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X_train, df_y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f6f2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_X_train\n",
    "del df_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84844ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "026cd77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                        LogisticRegression(penalty='l2', \n",
    "                                           C=1e-3, \n",
    "                                           random_state=1, \n",
    "                                           solver='lbfgs', \n",
    "                                           multi_class='ovr', \n",
    "                                           verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8bb1691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=0.001, multi_class='ovr',\n",
       "                                    random_state=1))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f7297",
   "metadata": {},
   "source": [
    "## 四. 參數調整"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4eba3",
   "metadata": {},
   "source": [
    "## 五. 結果預測"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1d193",
   "metadata": {},
   "source": [
    "首先我們得把testing set做一模一樣的前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2d7cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.538\n",
      "AUC score: 0.521 (+/-0.023)\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe_lr.predict(X_test)\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))\n",
    "scores = cross_val_score(estimator=pipe_lr, \n",
    "                         X=X_test, \n",
    "                         y=y_test, \n",
    "                         cv=10, \n",
    "                         verbose = 0,\n",
    "                         scoring='roc_auc')\n",
    "print('AUC score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aeed88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
