{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee524c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "debug = False\n",
    "MIN = 1 # n-gram\n",
    "MAX = 1 # n-gram\n",
    "MAX_DF = 0.6\n",
    "HASH_POWER = 10 # hash to 2**HASH_POWER features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce311235",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from summa import keywords\n",
    "from summa.summarizer import summarize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from time import strptime\n",
    "from cup01 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b2afffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"output/\") : os.mkdir(\"output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60804d",
   "metadata": {},
   "source": [
    "## 一. 資料前處理\n",
    "首先先引入dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d62373",
   "metadata": {},
   "source": [
    "### 1.1 清掉所有的html tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd400ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aaabd5",
   "metadata": {},
   "source": [
    "### 1.2 定義tokenize+波特詞幹還原演算法+刪除停用字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d419e143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runner', 'like', 'run', 'thu', 'run']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/benny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "stop = stop + extra_stopwords()\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(tokenizer_stem_nostop('runners like running and thus they run')) # Test if it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa74563",
   "metadata": {},
   "source": [
    "### 1.3 分析文章基本性質\n",
    "例如作者,圖片數量等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00422646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/train.csv')\n",
    "if debug:\n",
    "    df = df.iloc[:100] # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2709e608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt; &lt;span c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content\n",
       "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
       "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
       "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
       "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
       "4   4          -1  <html><head><div class=\"article-info\"><span cl..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1184fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_contents = df['Page content'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2535413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "days, pub_days, channels, img_counts, topics, authors, titles, social_media_counts, \\\n",
    "contents, num_hrefs, num_self_hrefs \\\n",
    "= get_all_datas(df_train_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbcc92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_positive_words, rate_negative_words, avg_positive_polarity, min_positive_polarity, max_positive_polarity, \\\n",
    "avg_negative_polarity, min_negative_polarity, max_negative_polarity \\\n",
    "= get_word_sentiment_features(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f3ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens_titles, n_tokens_contents, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens \\\n",
    "= get_some_n_features(titles, contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b657c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_sentiment_polarity, global_subjectivity, title_subjectivity_list, title_sentiment_polarity_list, \\\n",
    "abs_title_subjectivity, abs_title_sentiment_polarity \\\n",
    "= get_sentiment_features(titles, contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f72172",
   "metadata": {},
   "source": [
    "combine the properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e1f7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Page content':df_train_contents,\n",
    "                   'Id':df.Id[:],\n",
    "                   'Popularity':df.Popularity[:],\n",
    "                   'topic':topics,\n",
    "                   'channel':channels,\n",
    "                   'weekday':days,\n",
    "                   'pub_date' : pub_days,\n",
    "                   'author':authors,\n",
    "                   'img count':img_counts,\n",
    "                   'title':titles,\n",
    "                   'content':contents,\n",
    "                   'media count': social_media_counts,\n",
    "                   'n_tokens_title' : n_tokens_titles,\n",
    "                   'n_tokens_content': n_tokens_contents,\n",
    "                   'n_unique_tokens' : n_unique_tokens,\n",
    "                   'n_non_stop_words': n_non_stop_words,\n",
    "                   'n_non_stop_unique_tokens': n_non_stop_unique_tokens,\n",
    "                   'num_hrefs' : num_hrefs,\n",
    "                   'num_self_hrefs' : num_self_hrefs,\n",
    "                   'global_sentiment_polarity' : global_sentiment_polarity,\n",
    "                   'global_subjectivity' : global_subjectivity,\n",
    "                   'title_subjectivity' : title_subjectivity_list,\n",
    "                   'title_sentiment_polarity' : title_sentiment_polarity_list,\n",
    "                   'abs_title_subjectivity' : abs_title_subjectivity,\n",
    "                   'abs_title_sentiment_polarity' : abs_title_sentiment_polarity,\n",
    "                   'rate_positive_words' : rate_positive_words,\n",
    "                   'rate_negative_words' : rate_negative_words,\n",
    "                   'avg_positive_polarity' : avg_positive_polarity,\n",
    "                   'min_positive_polarity' : min_positive_polarity,\n",
    "                   'max_positive_polarity' : max_positive_polarity,\n",
    "                   'avg_negative_polarity' : avg_negative_polarity,\n",
    "                   'min_negative_polarity' : min_negative_polarity,\n",
    "                   'max_negative_polarity' : max_negative_polarity})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41807ff",
   "metadata": {},
   "source": [
    "### 1.4 時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3adf3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_of_month'] = df['pub_date'].apply(lambda x: int(x.split()[1]))\n",
    "df['month'] = df['pub_date'].apply(lambda x: strptime(x.split()[2], '%b').tm_mon)\n",
    "df['hour'] = df['pub_date'].apply(lambda x: strptime(x.split()[4], '%X')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e15ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train_contents, df['pub_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacfd2f8",
   "metadata": {},
   "source": [
    "### 1.5 產生關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ec1bea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 27643/27643 [01:10<00:00, 389.72it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['Page content'] = df['Page content'].progress_apply(preprocessor) # 此步驟約要花五分鐘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38a682eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 27643/27643 [01:52<00:00, 246.39it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['keywords'] = df['Page content'].progress_apply(tokenizer_stem_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b40337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 27643/27643 [00:00<00:00, 236658.27it/s]\n"
     ]
    }
   ],
   "source": [
    "df['keywords'] = df['keywords'].progress_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f27595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████▌        | 21315/27643 [14:39<25:32,  4.13it/s]"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x: keywords.keywords(x).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6043734",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7545af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./input/input_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f91df3",
   "metadata": {},
   "source": [
    "## 二. 特徵選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd52b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/input_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491dfea",
   "metadata": {},
   "source": [
    "### 2.1 找出頻率最高的詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6057f13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(preprocessor=<function preprocessor at 0x00000149A614A8B0>,\n",
       "                tokenizer=<function tokenizer_stem_nostop at 0x00000149A1AE6940>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range=(1, 1), # (MIN, MAX)\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "count.fit([\"YEAH TIGER\", \"FIBER WIPER\"]) # need to fit something first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e6f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[most frequent vocabularies]\n",
      "imag: 116996\n",
      "also: 51161\n",
      "new: 44159\n",
      "one: 42492\n",
      "video: 41798\n",
      "see: 38955\n",
      "like: 36858\n",
      "time: 35997\n",
      "use: 33510\n",
      "app: 32685\n"
     ]
    }
   ],
   "source": [
    "doc = df['Page content']\n",
    "doc_bag = count.fit_transform(doc).toarray()\n",
    "\n",
    "print(\"[most frequent vocabularies]\")\n",
    "bag_cnts = np.sum(doc_bag, axis=0)\n",
    "top = 10\n",
    "# [::-1] reverses a list since sort is in ascending order\n",
    "for tok, v in zip(count.inverse_transform(np.ones(bag_cnts.shape[0]).reshape(1, -1))[0][bag_cnts.argsort()[::-1][:top]], \n",
    "                  np.sort(bag_cnts)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2fc53",
   "metadata": {},
   "source": [
    "### 2.2 基於整個文本的TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22fb742",
   "metadata": {},
   "source": [
    "利用前面所定義的前處理方法產生tf-idf向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b99c3a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "doc = df['Page content']\n",
    "tfidf = TfidfVectorizer(ngram_range=(MIN, MAX), \n",
    "                        preprocessor=preprocessor, \n",
    "                        tokenizer=tokenizer_stem_nostop, \n",
    "                        max_df=MAX_DF, \n",
    "                        min_df=0.0001)\n",
    "tfidf.fit(doc)\n",
    "doc_tfidf = tfidf.transform(doc).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4079cf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 43634)\n"
     ]
    }
   ],
   "source": [
    "print(doc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "528591d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"tfidf.csv\", doc_tfidf, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ac742",
   "metadata": {},
   "source": [
    "接著調查idf分數以及tf-idf值最大的10個單字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31ac39f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "new: 1.56\n",
      "like: 1.60\n",
      "time: 1.61\n",
      "make: 1.72\n",
      "year: 1.73\n",
      "world: 1.75\n",
      "use: 1.77\n",
      "get: 1.79\n",
      "first: 1.85\n",
      "take: 1.90\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "video: 733.5061786531768\n",
      "app: 602.2534993946733\n",
      "new: 500.9036433140395\n",
      "googl: 455.2371136820567\n",
      "game: 442.6232487233518\n",
      "twitter: 414.0212743230195\n",
      "facebook: 410.4951735218714\n",
      "compani: 403.128044066731\n",
      "appl: 401.1395615393847\n",
      "time: 400.9980301873785\n"
     ]
    }
   ],
   "source": [
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones(tfidf_sum.shape[0]).reshape(1, -1))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del doc_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7dd108",
   "metadata": {},
   "source": [
    "### 2.3 基於關鍵字的tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc1b5402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "doc = df['keywords']\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                        preprocessor=preprocessor, \n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "tfidf.fit(doc)\n",
    "doc_tfidf = tfidf.transform(doc).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "067e34da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 39431)\n"
     ]
    }
   ],
   "source": [
    "print(doc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea9abfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "imag: 1.94\n",
      "new: 2.23\n",
      "like: 2.35\n",
      "time: 2.47\n",
      "use: 2.55\n",
      "video: 2.59\n",
      "make: 2.61\n",
      "year: 2.66\n",
      "said: 2.70\n",
      "compani: 2.79\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "imag: 639.4314030667334\n",
      "new: 543.6068855046963\n",
      "video: 487.298715875056\n",
      "like: 460.2245497275674\n",
      "time: 431.4077628796134\n",
      "use: 427.29808775659825\n",
      "compani: 402.0364173369534\n",
      "make: 393.66282963562537\n",
      "year: 391.85262234595297\n",
      "said: 379.9360365379158\n"
     ]
    }
   ],
   "source": [
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones(tfidf_sum.shape[0]).reshape(1, -1))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56746307",
   "metadata": {},
   "outputs": [],
   "source": [
    "del doc_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c3fb1",
   "metadata": {},
   "source": [
    "### 2.4 Feature Hashing\n",
    "hash words to 1024 or 2048 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f53db",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df['keywords']\n",
    "hashvec = HashingVectorizer(n_features=2**HASH_POWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603aa3f",
   "metadata": {},
   "source": [
    "no .fit needed for HashingVectorizer, since it's defined by the hash function. transform sentences to vectors of dimension 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb757c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_hash = hashvec.transform(doc).toarray()\n",
    "print(doc_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d487b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df['title']\n",
    "hashvec = HashingVectorizer(n_features=2**HASH_POWER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b834f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_hash_title = hashvec.transform(doc).toarray()\n",
    "print(doc_hash_title.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042fe77b",
   "metadata": {},
   "source": [
    "### 2.5 One hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26548fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel\n",
    "channel_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "channel_str = channel_ohe.fit_transform(df['channel'].values.reshape(-1,1)).toarray()\n",
    "print(channel_str.shape)\n",
    "\n",
    "# weekday\n",
    "weekday_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "weekday_str = weekday_ohe.fit_transform(df['weekday'].values.reshape(-1,1)).toarray()\n",
    "print(weekday_str.shape)\n",
    "\n",
    "# ohe author\n",
    "author_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "author_str = author_ohe.fit_transform(df['author'].values.reshape(-1,1)).toarray()\n",
    "print(author_str.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8ac17",
   "metadata": {},
   "source": [
    "把所有特徵給組合起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_count = df['img count'].values.reshape(-1,1)\n",
    "media_count = df['media count'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c569cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "df_X_train = []\n",
    "for i in tqdm(range(len(channel_str))):\n",
    "    temp = []\n",
    "    temp.append(img_count[i])\n",
    "    temp.append(media_count[i])\n",
    "    temp.append(channel_str[i])\n",
    "    temp.append(weekday_str[i])\n",
    "    temp.append(author_str[i])\n",
    "    temp.append(doc_hash[i])\n",
    "    temp.append(doc_hash_title[i])\n",
    "    temp = flatten(temp)\n",
    "    temp.append(df['day_of_month'][i])\n",
    "    temp.append(df['month'][i])\n",
    "    temp.append(df['hour'][i])\n",
    "    temp.append(df['n_tokens_title'][i])\n",
    "    temp.append(df['n_tokens_content'][i])\n",
    "    temp.append(df['n_unique_tokens'][i])\n",
    "    temp.append(df['n_non_stop_words'][i])\n",
    "    temp.append(df['n_non_stop_unique_tokens'][i])\n",
    "    temp.append(df['num_hrefs'][i])\n",
    "    temp.append(df['num_self_hrefs'][i])\n",
    "    temp.append(df['global_sentiment_polarity'][i])\n",
    "    temp.append(df['global_subjectivity'][i])\n",
    "    temp.append(df['title_subjectivity'][i])\n",
    "    temp.append(df['title_sentiment_polarity'][i])\n",
    "    temp.append(df['abs_title_subjectivity'][i])\n",
    "    temp.append(df['abs_title_sentiment_polarity'][i])\n",
    "    temp.append(df['rate_positive_words'][i])\n",
    "    temp.append(df['rate_negative_words'][i])\n",
    "    temp.append(df['avg_positive_polarity'][i])\n",
    "    temp.append(df['min_positive_polarity'][i])\n",
    "    temp.append(df['max_positive_polarity'][i])\n",
    "    temp.append(df['avg_negative_polarity'][i])\n",
    "    temp.append(df['min_negative_polarity'][i])\n",
    "    temp.append(df['max_negative_polarity'][i])\n",
    "    df_X_train.append(temp)\n",
    "    del temp\n",
    "\n",
    "df_y_train = df['Popularity'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, doc_hash, img_count, media_count, channel_str, weekday_str, author_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f872fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_X_train),len(df_X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcad4b",
   "metadata": {},
   "source": [
    "存成csv檔案就不用再重跑之前的code了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922fab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_X_train).to_csv(\"./input/X_train.csv\", index=False, header=False)\n",
    "pd.DataFrame(df_y_train).to_csv(\"./input/y_train.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc08a55",
   "metadata": {},
   "source": [
    "生成training set和testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963df3e7",
   "metadata": {},
   "source": [
    "## 三. 模型訓練\n",
    "直接從這裡開始跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca1eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = pd.read_csv('./input/X_train.csv', header=None).values\n",
    "df_y_train = pd.read_csv('./input/y_train.csv', header=None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e000e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X_train, df_y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_X_train\n",
    "del df_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4320e",
   "metadata": {},
   "source": [
    "### 3.1 繪製解釋變異數圖形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae90ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = np.cov(X_train_std.T) # 計算特徵值\n",
    "eigen_vals, _ = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae596748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把explained variance(lambda i / lambda和)由大排到小\n",
    "tot = sum(eigen_vals)\n",
    "var_exp = np.array([(i / tot) for i in sorted(eigen_vals, reverse=True)])\n",
    "cum_var_exp = np.cumsum(var_exp) # 計算解釋變異數\n",
    "\n",
    "# 繪圖\n",
    "plt.bar(range(len(eigen_vals)), var_exp, alpha=0.5, align='center',\n",
    "        label='individual explained variance')\n",
    "plt.step(range(len(eigen_vals)), cum_var_exp, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_02.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bfbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_std, X_test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89312ce1",
   "metadata": {},
   "source": [
    "### 3.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5ef6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr2 = make_pipeline(StandardScaler(),\n",
    "                         #PCA(n_components=1200),\n",
    "                         LogisticRegression(penalty='l2',\n",
    "                                            C=1e-5, \n",
    "                                            random_state=1, \n",
    "                                            solver='lbfgs', \n",
    "                                            multi_class='ovr', \n",
    "                                            verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fbf9c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=1e-05, multi_class='ovr',\n",
       "                                    random_state=1))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr2.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "311cca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.600\n",
      "AUC score: nan (+/-nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 87, in __call__\n",
      "    score = scorer._score(cached_call, estimator,\n",
      "  File \"/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 362, in _score\n",
      "    return self._sign * self._score_func(y, y_pred, **self._kwargs)\n",
      "  File \"/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\", line 542, in roc_auc_score\n",
      "    return _average_binary_score(partial(_binary_roc_auc_score,\n",
      "  File \"/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/metrics/_base.py\", line 77, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\", line 327, in _binary_roc_auc_score\n",
      "    raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n",
      "ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe_lr2.predict(X_test)\n",
    "print('Test Accuracy: %.3f' % pipe_lr2.score(X_test, y_test.ravel()))\n",
    "scores = cross_val_score(estimator=pipe_lr2, \n",
    "                         X=X_test, \n",
    "                         y=y_test.ravel(), \n",
    "                         cv=10, \n",
    "                         verbose = 0,\n",
    "                         scoring='roc_auc')\n",
    "print('AUC score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dca78c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe_lr2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0256504",
   "metadata": {},
   "source": [
    "### 3.3 xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d39a0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_xgb = np.array(X_train)\n",
    "y_train_xgb = [0 if x==[-1] else x for x in y_train]\n",
    "X_test_xgb = np.array(X_test)\n",
    "#y_test_xgb = [0 if x==[-1] else x for x in y_test]\n",
    "d_train = xgb.DMatrix(X_train_xgb, y_train_xgb)\n",
    "d_test = xgb.DMatrix(X_test_xgb)\n",
    "xgb_params = {'eta': 0.05, \n",
    "              'max_depth': 4, \n",
    "              'subsample': 0.7 ,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'min_child_weight' : 4,\n",
    "              'objective': 'binary:logistic', \n",
    "              'eval_metric': 'auc',\n",
    "              'alpha': 0.005,\n",
    "             }\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_params = xgb_model.get_xgb_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d9b7",
   "metadata": {},
   "source": [
    "## 四. 參數調整"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb06979",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "026cd77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                        #PCA(n_components=1000),\n",
    "                        LogisticRegression(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9fdca61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70625\n",
      "{'logisticregression__C': 0.01, 'logisticregression__class_weight': 'balanced', 'logisticregression__penalty': 'l2', 'logisticregression__solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] \n",
    "param_grid = [{'logisticregression__C': param_range, \n",
    "               'logisticregression__penalty': ['l2'],\n",
    "               'logisticregression__class_weight': ['balanced', None],\n",
    "               'logisticregression__solver': ['saga', 'lbfgs']}]\n",
    "\n",
    "# 使用網格搜尋法\n",
    "gs = GridSearchCV(estimator=pipe_lr, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc', \n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train.ravel())\n",
    "\n",
    "# 輸出結果\n",
    "print(gs.best_score_) \n",
    "print(gs.best_params_) # 最佳參數組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8bb1691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=0.01, class_weight='balanced',\n",
       "                                    random_state=0, solver='saga'))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = gs.best_estimator_ # 取得剛剛算出的最佳參數\n",
    "clf.fit(X_train, y_train) # 用這個參數train出model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e2153",
   "metadata": {},
   "source": [
    "### 4.2 XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b328c4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.005, base_score=None, booster=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, eta=0.05, eval_metric='auc', gamma=None,\n",
       "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=4,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=2, n_jobs=None, num_parallel_tree=None,\n",
       "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "              scale_pos_weight=None, subsample=0.7, tree_method=None,\n",
       "              validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvresult = xgb.cv(xgb_params, \n",
    "                  d_train, \n",
    "                  num_boost_round=1000, \n",
    "                  verbose_eval=10, \n",
    "                  nfold=5, \n",
    "                  metrics=['auc'],\n",
    "                  early_stopping_rounds=50, \n",
    "                  stratified=True)\n",
    "xgb_model.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f8d220a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.005, base_score=0.5, booster='gbtree',\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
       "              eta=0.05, eval_metric='auc', gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.0500000007, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=2, n_jobs=12, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0.00499999989, reg_lambda=1, scale_pos_weight=1,\n",
       "              subsample=0.7, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X_train_xgb, y_train_xgb, eval_metric='auc', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4eba3",
   "metadata": {},
   "source": [
    "## 五. 結果預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2d7cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.650\n",
      "AUC score: nan (+/-nan)\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "scores = cross_val_score(estimator=clf, \n",
    "                         X=X_test, \n",
    "                         y=y_test, \n",
    "                         cv=10, \n",
    "                         verbose = 0,\n",
    "                         scoring='roc_auc')\n",
    "print('AUC score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7bdb5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.200\n",
      "AUC score: nan (+/-nan)\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_model.predict_proba(X_test_xgb)[:,1]\n",
    "print('Test Accuracy: %.3f' % xgb_model.score(X_test_xgb, y_test))\n",
    "scores = cross_val_score(estimator=xgb_model, \n",
    "                         X=X_test, \n",
    "                         y=y_test, \n",
    "                         cv=10, \n",
    "                         verbose = 0,\n",
    "                         scoring='roc_auc')\n",
    "print('AUC score: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221f897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
