{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee524c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "debug = False\n",
    "MIN = 1 # n-gram\n",
    "MAX = 1 # n-gram\n",
    "MAX_DF = 0.6\n",
    "HASH_POWER = 10 # hash to 2**HASH_POWER features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce311235",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60804d",
   "metadata": {},
   "source": [
    "## 一. 資料前處理\n",
    "首先先引入dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00422646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./input/train.csv')\n",
    "if debug:\n",
    "    df = df.iloc[:100] # debug\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d62373",
   "metadata": {},
   "source": [
    "### 1.1 清掉所有的html tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd400ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aaabd5",
   "metadata": {},
   "source": [
    "### 1.2 定義tokenize+波特詞幹還原演算法+刪除停用字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d419e143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runner', 'like', 'run', 'thu', 'run']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/benny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(tokenizer_stem_nostop('runners like running and thus they run')) # Test if it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cadb6",
   "metadata": {},
   "source": [
    "### 1.3 html語法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd8b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datetime(text=[]):\n",
    "    day = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        if soup.time.has_attr('datetime'):\n",
    "            date = soup.time.attrs['datetime']\n",
    "            day.append(' '+ date[0:3])\n",
    "        else:\n",
    "            day.append(' fuckday')\n",
    "    return day\n",
    "\n",
    "def fetch_channel(text=[]):\n",
    "    channels = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        channel = soup.article['data-channel']\n",
    "        channels.append(channel)\n",
    "    return channels\n",
    "\n",
    "def fetch_img_count(text=[]):\n",
    "    count = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        c = 0\n",
    "        find_all_images = soup.find_all('img')\n",
    "        for i in find_all_images:\n",
    "            c = c+1\n",
    "        count.append(c)\n",
    "    return count\n",
    "\n",
    "def fetch_topics(text=[]):\n",
    "    topics = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        footer = soup.footer\n",
    "        ta = footer.find_all('a')\n",
    "        topic = []\n",
    "        for t in ta:\n",
    "            topic.append(t.get_text())\n",
    "        ff = ' '.join(topic)\n",
    "        topics.append(ff)\n",
    "    return topics\n",
    "\n",
    "def fetch_authors(text=[]):\n",
    "    authors = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        footer = soup.span\n",
    "        if footer != None:\n",
    "            ta = footer.findAll('a')\n",
    "            author = []\n",
    "            for t in ta:\n",
    "                author.append(t.get_text())\n",
    "            if len(author) == 0:\n",
    "                ff = 'NaN'\n",
    "            else:\n",
    "                ff = ''.join(author)\n",
    "        else:\n",
    "            ff = 'NaN'\n",
    "        authors.append(ff)\n",
    "    return authors\n",
    "\n",
    "def fetch_titles(text=[]):\n",
    "    titles = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        footer = soup.h1\n",
    "        if footer != None:\n",
    "            ff = footer.get_text()\n",
    "        else:\n",
    "            ff = 'NaN'\n",
    "        titles.append(ff) \n",
    "    return titles\n",
    "\n",
    "def fetch_social_media_count(text=[]):\n",
    "    count = []\n",
    "    for tx in text:\n",
    "        soup = BeautifulSoup(tx, \"lxml\")\n",
    "        c = 0\n",
    "        for frame in soup(\"iframe\"):\n",
    "#             print(frame.get('src').split(\".\"))\n",
    "            if frame.get('src').find(\"youtube\") != None:\n",
    "                c = c+1\n",
    "            elif frame.get('src').find(\"instagram\") != None:\n",
    "                c = c+1\n",
    "            elif frame.get('src').find(\"vine\") != None:\n",
    "                c= c+1\n",
    "            # apply new media here\n",
    "        count.append(c)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0ecda",
   "metadata": {},
   "source": [
    "這裡以下不必重跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74046a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fatch deature, 這裡可能需要15分鐘左右\n",
    "topic_batch   = fetch_topics            (df[:]['Page content'])\n",
    "channel_batch = fetch_channel           (df[:]['Page content'])\n",
    "weekday_batch = fetch_datetime          (df[:]['Page content'])\n",
    "author_batch  = fetch_authors           (df[:]['Page content'])\n",
    "img_batch     = fetch_img_count         (df[:]['Page content'])\n",
    "title_batch   = fetch_titles            (df[:]['Page content'])\n",
    "media_batch   = fetch_social_media_count(df[:]['Page content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d90e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-consturct training data\n",
    "df_feature = pd.DataFrame({'Id':df.Id[:],\n",
    "                           'Popularity':df.Popularity[:],\n",
    "                           'topic':topic_batch,\n",
    "                           'channel':channel_batch,\n",
    "                           'weekday':weekday_batch,\n",
    "                           'author':author_batch,\n",
    "                           'img count':img_batch,\n",
    "                           'title':title_batch,\n",
    "                           'media count': media_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99b4f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_feature.drop(['Id', 'Popularity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb439b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>media count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>1</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>52</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               topic        channel weekday  \\\n",
       "0  Asteroid Asteroids challenge Earth Space U.S. ...          world     Wed   \n",
       "1  Apps and Software Google open source opn pledg...           tech     Thu   \n",
       "2      Entertainment NFL NFL Draft Sports Television  entertainment     Wed   \n",
       "3                    Sports Video Videos Watercooler    watercooler     Fri   \n",
       "4  Entertainment instagram instagram video NFL Sp...  entertainment     Thu   \n",
       "\n",
       "             author  img count  \\\n",
       "0               NaN          1   \n",
       "1  Christina Warren          2   \n",
       "2         Sam Laird          2   \n",
       "3         Sam Laird          1   \n",
       "4   Connor Finnegan         52   \n",
       "\n",
       "                                               title  media count  \n",
       "0  NASA's Grand Challenge: Stop Asteroids From De...            0  \n",
       "1  Google's New Open Source Patent Pledge: We Won...            0  \n",
       "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...           25  \n",
       "3        Cameraperson Fails Deliver Slapstick Laughs           21  \n",
       "4  NFL Star Helps Young Fan Prove Friendship With...            1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_feature.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cad556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_feature, lsuffix='_caller', rsuffix='_other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec1bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Page content'] = df['Page content'].apply(preprocessor) # 此步驟約要花五分鐘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91747aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>media count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>clara moskowitz for space com 2013 06 19 15 0...</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>by christina warren2013 03 28 17 40 55 utcgoog...</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>by sam laird2014 05 07 19 15 20 utcballin 2014...</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>by sam laird2013 10 11 02 26 50 utccameraperso...</td>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>1</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>by connor finnegan2014 04 17 03 31 43 utcnfl s...</td>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>52</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content  \\\n",
       "0   0          -1   clara moskowitz for space com 2013 06 19 15 0...   \n",
       "1   1           1  by christina warren2013 03 28 17 40 55 utcgoog...   \n",
       "2   2           1  by sam laird2014 05 07 19 15 20 utcballin 2014...   \n",
       "3   3          -1  by sam laird2013 10 11 02 26 50 utccameraperso...   \n",
       "4   4          -1  by connor finnegan2014 04 17 03 31 43 utcnfl s...   \n",
       "\n",
       "                                               topic        channel weekday  \\\n",
       "0  Asteroid Asteroids challenge Earth Space U.S. ...          world     Wed   \n",
       "1  Apps and Software Google open source opn pledg...           tech     Thu   \n",
       "2      Entertainment NFL NFL Draft Sports Television  entertainment     Wed   \n",
       "3                    Sports Video Videos Watercooler    watercooler     Fri   \n",
       "4  Entertainment instagram instagram video NFL Sp...  entertainment     Thu   \n",
       "\n",
       "             author  img count  \\\n",
       "0               NaN          1   \n",
       "1  Christina Warren          2   \n",
       "2         Sam Laird          2   \n",
       "3         Sam Laird          1   \n",
       "4   Connor Finnegan         52   \n",
       "\n",
       "                                               title  media count  \n",
       "0  NASA's Grand Challenge: Stop Asteroids From De...            0  \n",
       "1  Google's New Open Source Patent Pledge: We Won...            0  \n",
       "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...           25  \n",
       "3        Cameraperson Fails Deliver Slapstick Laughs           21  \n",
       "4  NFL Star Helps Young Fan Prove Friendship With...            1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946c107",
   "metadata": {},
   "source": [
    "可以丟下去的有channel，weekday，img count，media count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2798b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7545af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./input/input_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f91df3",
   "metadata": {},
   "source": [
    "## 二. 特徵選擇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491dfea",
   "metadata": {},
   "source": [
    "### 2.1 找出頻率最高的詞 (不需要重跑，不然要跑10多分鐘)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6057f13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(preprocessor=<function preprocessor at 0x00000149A614A8B0>,\n",
       "                tokenizer=<function tokenizer_stem_nostop at 0x00000149A1AE6940>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range=(1, 1), # (MIN, MAX)\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "count.fit([\"YEAH TIGER\", \"FIBER WIPER\"]) # need to fit something first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e6f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[most frequent vocabularies]\n",
      "imag: 116996\n",
      "also: 51161\n",
      "new: 44159\n",
      "one: 42492\n",
      "video: 41798\n",
      "see: 38955\n",
      "like: 36858\n",
      "time: 35997\n",
      "use: 33510\n",
      "app: 32685\n"
     ]
    }
   ],
   "source": [
    "doc = df['Page content']\n",
    "doc_bag = count.fit_transform(doc).toarray()\n",
    "\n",
    "print(\"[most frequent vocabularies]\")\n",
    "bag_cnts = np.sum(doc_bag, axis=0)\n",
    "top = 10\n",
    "# [::-1] reverses a list since sort is in ascending order\n",
    "for tok, v in zip(count.inverse_transform(np.ones(bag_cnts.shape[0]).reshape(1, -1))[0][bag_cnts.argsort()[::-1][:top]], \n",
    "                  np.sort(bag_cnts)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2fc53",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dc38411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/input_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22fb742",
   "metadata": {},
   "source": [
    "利用前面所定義的前處理方法產生tf-idf向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b99c3a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "doc = df['Page content']\n",
    "tfidf = TfidfVectorizer(ngram_range=(MIN, MAX), \n",
    "                        preprocessor=preprocessor, \n",
    "                        tokenizer=tokenizer_stem_nostop, \n",
    "                        max_df=MAX_DF, \n",
    "                        min_df=0.0001)\n",
    "tfidf.fit(doc)\n",
    "doc_tfidf = tfidf.transform(doc).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4079cf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 43634)\n"
     ]
    }
   ],
   "source": [
    "print(doc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ac742",
   "metadata": {},
   "source": [
    "接著調查idf分數以及tf-idf值最大的10個單字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ac39f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "new: 1.56\n",
      "like: 1.60\n",
      "time: 1.61\n",
      "make: 1.72\n",
      "year: 1.73\n",
      "world: 1.75\n",
      "use: 1.77\n",
      "get: 1.79\n",
      "first: 1.85\n",
      "take: 1.90\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "video: 733.5061786531768\n",
      "app: 602.2534993946733\n",
      "new: 500.9036433140395\n",
      "googl: 455.2371136820567\n",
      "game: 442.6232487233518\n",
      "twitter: 414.0212743230195\n",
      "facebook: 410.4951735218714\n",
      "compani: 403.128044066731\n",
      "appl: 401.1395615393847\n",
      "time: 400.9980301873785\n"
     ]
    }
   ],
   "source": [
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones(tfidf_sum.shape[0]).reshape(1, -1))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c3fb1",
   "metadata": {},
   "source": [
    "### 2.3 Feature Hashing (我記憶體很夠，應該不會使用這個方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d5f53db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1024)\n"
     ]
    }
   ],
   "source": [
    "# hash words to 1024 buckets\n",
    "hashvec = HashingVectorizer(n_features=2**HASH_POWER,\n",
    "                            preprocessor=preprocessor,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "# no .fit needed for HashingVectorizer, since it's defined by the hash function\n",
    "# transform sentences to vectors of dimension 1024\n",
    "doc_hash = hashvec.transform([\"YEAH TIGER\", \"FIBER WIPER\"]) # test\n",
    "print(doc_hash.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042fe77b",
   "metadata": {},
   "source": [
    "### 2.4 One hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26548fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 33)\n",
      "(27643, 7)\n",
      "(27643, 428)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# channel\n",
    "channel_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "channel_str = channel_ohe.fit_transform(df['channel'].values.reshape(-1,1)).toarray()\n",
    "print(channel_str.shape)\n",
    "\n",
    "# weekday\n",
    "weekday_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "weekday_str = weekday_ohe.fit_transform(df['weekday'].values.reshape(-1,1)).toarray()\n",
    "print(weekday_str.shape)\n",
    "\n",
    "# ohe author\n",
    "author_ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "author_str = author_ohe.fit_transform(df['author'].values.reshape(-1,1)).toarray()\n",
    "print(author_str.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8ac17",
   "metadata": {},
   "source": [
    "把所有特徵給組合起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf0f3ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_count = df['img count'].values.reshape(-1,1)\n",
    "media_count = df['media count'].values.reshape(-1,1)\n",
    "df_y_train = df['Popularity'].to_numpy()\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c569cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = np.concatenate([channel_str,\n",
    "                             weekday_str, \n",
    "                             author_str, \n",
    "                             img_count,\n",
    "                             media_count,\n",
    "                             doc_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcad4b",
   "metadata": {},
   "source": [
    "存成csv檔案就不用再重跑之前的code了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_X_train = pd.read_csv('./input/df_X_train.csv')\n",
    "#df_y_train = pd.read_csv('./input/df_y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922fab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train.to_csv('./input/X_train.csv')\n",
    "df_y_train.to_csv('./input/y_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc08a55",
   "metadata": {},
   "source": [
    "生成training set和testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97e000e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X_train, df_y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606965c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_X_train\n",
    "del df_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963df3e7",
   "metadata": {},
   "source": [
    "## 三. 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84844ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "026cd77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                        LogisticRegression(penalty='l2', C=0.08, random_state=1, solver='lbfgs', multi_class='ovr', verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8bb1691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=0.08, multi_class='ovr', random_state=1,\n",
       "                                    verbose=1))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2d7cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.558\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe_lr.predict(X_test)\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))\n",
    "scores = cross_val_score(estimator=clf, X=df_small['review'], y=df_small['sentiment'], cv=10, scoring='roc_auc')\n",
    "print('AUC score %s: %.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f7297",
   "metadata": {},
   "source": [
    "## 四. 參數調整"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4eba3",
   "metadata": {},
   "source": [
    "## 五. 結果預測"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
