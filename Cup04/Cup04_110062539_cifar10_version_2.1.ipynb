{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 110062539 chKoogoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT_SHAPE = (224, 224, 3)\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "PATIENCE = 13 # 如果過多少個EPOCHS沒改善就停止訓練\n",
    "CATGORICAL = 10\n",
    "LR = 1e-3\n",
    "LR_FACTOR = 0.5 # new_lr = lr * factor.\n",
    "LR_PATIENCE = 4 # umber of epochs with no improvement after which learning rate will be reduced\n",
    "MODEL_NAME = 'cifar10_Densenet121_current'\n",
    "MODEL_PATH = MODEL_NAME + '.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        # set memory limit\n",
    "        #tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit = 1024)])\n",
    "\n",
    "       \n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Try5: Data augmentation:\n",
    "\n",
    "https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.FancyPCA\n",
    "\n",
    "https://hackmd.io/@allen108108/SyCsOIkxB\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "\n",
    "https://keras.io/zh/preprocessing/image/\n",
    "\n",
    "http://www.tisv.cn/14/\n",
    "\n",
    "- 單純flip, rotate沒什麼用，像是pixelwise的operation效果顯著（試驗第一個連結的所有function）\n",
    "\n",
    "- 之後把有用的function全部放在一起訓練\n",
    "\n",
    "### ImageDataGenerator:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('./datalab-cup4-unlearnable-datasets-cifar-10/x_train_cifar10_unlearn.npy')\n",
    "y_train = np.load('./datalab-cup4-unlearnable-datasets-cifar-10/y_train_cifar10.npy')\n",
    "x_val = np.load('./datalab-cup4-unlearnable-datasets-cifar-10/x_val_cifar10.npy')\n",
    "y_val = np.load('./datalab-cup4-unlearnable-datasets-cifar-10/y_val_cifar10.npy')\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "# ohe labels\n",
    "y_train = tf.one_hot(y_train, 10)\n",
    "y_val = tf.one_hot(y_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=MODEL_PATH, \n",
    "                             monitor='val_acc',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='auto', \n",
    "                             save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                   factor=LR_FACTOR, \n",
    "                                   patience=LR_PATIENCE, \n",
    "                                   verbose=1, \n",
    "                                   mode='auto', \n",
    "                                   min_delta=1.25e-4)\n",
    "early = EarlyStopping(monitor='val_acc', \n",
    "                      mode=\"auto\", \n",
    "                      patience=PATIENCE)\n",
    "\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "#callbacks_list = [checkpoint, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding Image augmentation layers\n",
    "# https://keras.io/api/layers/preprocessing_layers/image_augmentation/\n",
    "\n",
    "\n",
    "rand_image_aug_layer = Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=None),\n",
    "    tf.keras.layers.RandomContrast(0.1, seed=None)\n",
    "],\n",
    "    name='aug_layer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding Image batchnormalization layer\n",
    "# \n",
    "\n",
    "batch_norm_layer = Sequential([\n",
    "    tf.keras.layers.BatchNormalization(\n",
    "    axis=-1,\n",
    "    momentum=0.99,\n",
    "    epsilon=0.001,\n",
    "    center=True,\n",
    "    scale=True,\n",
    "    beta_initializer=\"zeros\",\n",
    "    gamma_initializer=\"ones\",\n",
    "    moving_mean_initializer=\"zeros\",\n",
    "    moving_variance_initializer=\"ones\",\n",
    "    beta_regularizer=None,\n",
    "    gamma_regularizer=None,\n",
    "    beta_constraint=None,\n",
    "    gamma_constraint=None\n",
    "    )\n",
    "],\n",
    "    name='batch_norm_layer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10_Densenet121_current\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " aug_layer (Sequential)      (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " batch_norm_layer (Sequentia  (None, 32, 32, 3)        12        \n",
      " l)                                                              \n",
      "                                                                 \n",
      " densenet121 (Functional)    (None, 10)                7047754   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,047,766\n",
      "Trainable params: 6,964,112\n",
      "Non-trainable params: 83,654\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' try2: Densenet121 '''\n",
    "model = Sequential([\n",
    "    \n",
    "    # random_aug & batch_norm\n",
    "    rand_image_aug_layer,\n",
    "    batch_norm_layer,\n",
    "    \n",
    "    # main model\n",
    "    tf.keras.applications.DenseNet121(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    pooling=max,\n",
    "    classes=10\n",
    "    ),\n",
    "#     layers.Conv2D(2048, (1, 1), activation=\"relu\"),\n",
    "#     layers.Dropout(0.25),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(10, activation='softmax'),\n",
    "],\n",
    "    name=MODEL_NAME\n",
    ")\n",
    "model.build((None, INPUT_SHAPE[0], INPUT_SHAPE[1], INPUT_SHAPE[2]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.losses.CategoricalCrossentropy(from_logits=False) ##\n",
    "optimizer = tf.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define types of transformer of images '''\n",
    "\n",
    "transformer_try = A.Compose([   \n",
    "#    A.ChannelDropout (channel_drop_range=(1, 1), fill_value=0, always_apply=False, p=1)\n",
    "#    A.Downscale (scale_min=0.1, scale_max=0.9, interpolation=cv2.INTER_NEAREST, always_apply=False, p=1),\n",
    "#    A.Emboss (alpha=(0.2, 0.5), strength=(0.2, 0.7), always_apply=False, p=1),\n",
    "#    A.ColorJitter (brightness=0.1, contrast=0.2, saturation=0.3, hue=0.2, always_apply=False, p=1)\n",
    "#    A.Posterize (num_bits=4, always_apply=False, p=1) #不能用\n",
    "#    A.RandomBrightnessContrast (brightness_limit=0.1, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1)\n",
    "#    A.Equalize (mode='cv', by_channels=True, mask=None, mask_params=(), always_apply=False, p=1) #不能用\n",
    "#    A.ChannelShuffle(p=1.0)\n",
    "#    A.GridDistortion (num_steps=3, interpolation=cv2.INTER_NEAREST, border_mode=4, value=None, mask_value=None, always_apply=False, p=1)\n",
    "#    A.ISONoise (color_shift=(0.01, 0.05), intensity=(0.1, 0.5), always_apply=False, p=1.0) #不能用\n",
    "#    A.HorizontalFlip(p=1.0)\n",
    "#    A.RGBShift (r_shift_limit=(-40./255, 40./255), g_shift_limit=(-40./255, 40./255), b_shift_limit=(-40./255, 40./255), always_apply=False, p=1.0)\n",
    "#    A.ToGray(p=1.0),\n",
    "#    A.Sharpen (alpha=(0.3, 0.5), lightness=(0.8, 1.0), always_apply=False, p=1.0)\n",
    "#    A.ToSepia (always_apply=False, p=1.0)\n",
    "#    A.Superpixels (p_replace=0.1, n_segments=100, max_size=32, interpolation=1, always_apply=False, p=1.0)\n",
    "    \n",
    "    \n",
    "],  p=1)\n",
    "\n",
    "\n",
    "transformer1 = A.Compose([   \n",
    "    A.HueSaturationValue (hue_shift_limit=(-40./255, -40./255), sat_shift_limit=(-60./255, -60./255), val_shift_limit=(-40./255, -40./255), always_apply=False, p=1)\n",
    "],  p=1)\n",
    "\n",
    "transformer7 = A.Compose([   \n",
    "    A.ChannelShuffle(p=1.0)\n",
    "],  p=1)\n",
    "\n",
    "transformer11 = A.Compose([   \n",
    "    A.ToGray(p=1.0)\n",
    "],  p=1)\n",
    "\n",
    "transformer12 = A.Compose([   \n",
    "    A.ToSepia (always_apply=False, p=1.0)\n",
    "],  p=1)\n",
    "\n",
    "transformer5 = A.Compose([   \n",
    "    A.ColorJitter (brightness=0.1, contrast=0.2, saturation=0.3, hue=0.2, always_apply=False, p=1)\n",
    "],  p=1)\n",
    "\n",
    "# combined_?\n",
    "transformer7_11 = A.Compose([   \n",
    "    A.ChannelShuffle(p=1.0),\n",
    "    A.ToGray(p=1.0)\n",
    "],  p=1)\n",
    "\n",
    "# combined_5\n",
    "transformer5_7 = A.Compose([   \n",
    "    A.ColorJitter (brightness=0.1, contrast=0.2, saturation=0.3, hue=0.2, always_apply=False, p=1),\n",
    "    A.ChannelShuffle(p=1.0)\n",
    "],  p=1)\n",
    "\n",
    "\n",
    "\n",
    "transformer = A.Compose([   \n",
    "    #A.ChannelDropout (channel_drop_range=(1, 1), fill_value=0, always_apply=False, p=1),\n",
    "    #A.JpegCompression (quality_lower=99, quality_upper=100, always_apply=False, p=1),\n",
    "    A.HueSaturationValue (hue_shift_limit=(-40./255, -40./255), sat_shift_limit=(-60./255, -60./255), val_shift_limit=(-40./255, -40./255), always_apply=False, p=1),\n",
    "#    A.FancyPCA (alpha=0.1, always_apply=False, p=0.5)\n",
    "],  p=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transformer_a = A.Compose([   \n",
    "    #A.ChannelDropout (channel_drop_range=(1, 1), fill_value=0, always_apply=False, p=1),\n",
    "    #A.JpegCompression (quality_lower=99, quality_upper=100, always_apply=False, p=1),\n",
    "    A.HueSaturationValue (hue_shift_limit=(-40./255, -40./255), sat_shift_limit=(-60./255, -60./255), val_shift_limit=(-40./255, -40./255), always_apply=False, p=1),\n",
    "#    A.FancyPCA (alpha=0.1, always_apply=False, p=0.5)\n",
    "],  p=1)\n",
    "\n",
    "transformer_b = A.Compose([   \n",
    "    #A.HorizontalFlip(p=1),\n",
    "#    A.GaussianBlur (blur_limit=(3, 5), sigma_limit=0, always_apply=False, p=1)\n",
    "    A.Affine(p=1),\n",
    "    A.GaussNoise(var_limit=(1.0/255., 10.0/255.), mean=np.mean(x_train[0], axis=(0, 1, 2)), p=1),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1)\n",
    "    #A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1)\n",
    "],  p=1)\n",
    "\n",
    "transformer_c = A.Compose([   \n",
    "    A.HorizontalFlip(p=1),\n",
    "    A.GaussNoise(var_limit=(1.0/255., 10.0/255.), mean=np.mean(x_train, axis=(0, 1, 2)), p=1),\n",
    "#    A.MotionBlur(blur_limit=17, p=1)\n",
    "#    A.RGBShift(p=1),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1)\n",
    "],  p=1)\n",
    "\n",
    "\n",
    "\n",
    "image_size_aug = 36\n",
    "image_size = 32\n",
    "pad_size = int((image_size_aug-image_size)/2)\n",
    "\n",
    "transformer4 = A.Compose([   \n",
    "    #A.ChannelShuffle(p=0.5),\n",
    "    #RandomContrast (limit=0.5, always_apply=False, p=1),\n",
    "    #A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1),\n",
    "    #A.HorizontalFlip(p=1),\n",
    "    #A.Sharpen (alpha=(0.2, 0.5), lightness=(0.5, 1.0), always_apply=False, p=1)\n",
    "    A.CropAndPad (px=pad_size, percent=None, pad_mode=0, pad_cval=0, pad_cval_mask=0, \n",
    "                keep_size=True, sample_independently=True, interpolation=cv2.INTER_AREA, always_apply=False, p=1.0),\n",
    "    A.CropAndPad (px=-pad_size, percent=None, pad_mode=0, pad_cval=0, pad_cval_mask=0, \n",
    "                keep_size=True, sample_independently=True, interpolation=cv2.INTER_AREA, always_apply=False, p=1.0)\n",
    "],  p=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efc704790d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot transformer_try\n",
    "plt.imshow(transformer_try(image = x_train[0])['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_ds_try(img_ds):\n",
    "    num = len(img_ds)\n",
    "    transform_img_ds = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        img_dic = transformer_try(image = img_ds[i])\n",
    "        transform_img_ds.append(img_dic['image'])\n",
    "        \n",
    "    return np.array(transform_img_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create transformed training data & Concatenate those data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_ds(img_ds, transformer):\n",
    "    num = len(img_ds)\n",
    "    transform_img_ds = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        img_dic = transformer(image = img_ds[i])\n",
    "        transform_img_ds.append(img_dic['image'])\n",
    "        \n",
    "    return np.array(transform_img_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformed_data(x_train, y_train, transformer_list, num_data_apply_list):\n",
    "    x_trans_list = []\n",
    "    y_trans_list = []\n",
    "    \n",
    "    for i in range(len(transformer_list)):\n",
    "        x_trans_list.append(transformed_ds(x_train, transformer_list[i])[ : num_data_apply_list[i]])\n",
    "        y_trans_list.append(y_train[ : num_data_apply_list[i]])\n",
    "        \n",
    "#     for transformer in transformer_list:\n",
    "#         x_trans.append(transformed_ds(x_train, transformer))\n",
    "#         y_trans = np.concatenate((y_train , y_train))\n",
    "    \n",
    "    return np.array(np.concatenate(x_trans_list)), np.array(np.concatenate(y_trans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_1\n",
    "# transformer_list = [transformer1, transformer7, transformer7, transformer11, transformer12, transformer5]\n",
    "# num_data_apply_list = [len(x_train), len(x_train), len(x_train)//2, len(x_train), len(x_train), len(x_train)//2]\n",
    "\n",
    "# combined_2\n",
    "# transformer_list = [transformer1, transformer7]\n",
    "# num_data_apply_list = [len(x_train), len(x_train)]\n",
    "\n",
    "# combined_3\n",
    "# transformer_list = [transformer7_11]\n",
    "# num_data_apply_list = [len(x_train)]\n",
    "\n",
    "# combined_4\n",
    "# transformer_list = [transformer7, transformer7]\n",
    "# num_data_apply_list = [len(x_train), len(x_train)]\n",
    "\n",
    "# combined_5\n",
    "transformer_list = [transformer5_7]\n",
    "num_data_apply_list = [len(x_train)]\n",
    "\n",
    "x_train_trans, y_train_trans = create_transformed_data(x_train, y_train, transformer_list, num_data_apply_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_ds1(img_ds):\n",
    "    num = len(img_ds)\n",
    "    transform_img_ds = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        img_dic = transformer1(image = img_ds[i])\n",
    "        transform_img_ds.append(img_dic['image'])\n",
    "        \n",
    "    return np.array(transform_img_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(transformed_ds1(x_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the result:\n",
    "\n",
    "# x_train_trans4 = transformed_ds4(x_train)\n",
    "# x_train_trans3 = transformed_ds3(x_train)\n",
    "# #print(type(x_train_trans))\n",
    "# print(x_train_trans4.shape)\n",
    "# print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try\n",
    "\n",
    "# x_train_trans_try = transformed_ds_try(x_train)\n",
    "# x_train_trans = x_train_trans_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_trans = np.concatenate((x_train_trans4 , x_train3))\n",
    "# y_train = np.concatenate((y_train , y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('cifar10_Densenet121_combine_1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar10_Densenet121_current.hdf5\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('ChannelShuffle_data.npy', 'wb') as f:\n",
    "#     np.save(f, x_train_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.7707 - acc: 0.3644\n",
      "Epoch 00001: val_acc improved from -inf to 0.38940, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 220s 327ms/step - loss: 1.7707 - acc: 0.3644 - val_loss: 2.0179 - val_acc: 0.3894 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.2968 - acc: 0.5429\n",
      "Epoch 00002: val_acc improved from 0.38940 to 0.55600, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 197s 316ms/step - loss: 1.2968 - acc: 0.5429 - val_loss: 1.2625 - val_acc: 0.5560 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.1483 - acc: 0.6012\n",
      "Epoch 00003: val_acc improved from 0.55600 to 0.58080, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 196s 314ms/step - loss: 1.1483 - acc: 0.6012 - val_loss: 1.3160 - val_acc: 0.5808 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.0766 - acc: 0.6305\n",
      "Epoch 00004: val_acc improved from 0.58080 to 0.63740, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 199s 318ms/step - loss: 1.0766 - acc: 0.6305 - val_loss: 1.0394 - val_acc: 0.6374 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.9336 - acc: 0.6834\n",
      "Epoch 00005: val_acc did not improve from 0.63740\n",
      "625/625 [==============================] - 246s 394ms/step - loss: 0.9336 - acc: 0.6834 - val_loss: 5.9909 - val_acc: 0.4617 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.8440 - acc: 0.7099\n",
      "Epoch 00006: val_acc improved from 0.63740 to 0.64950, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 260s 416ms/step - loss: 0.8440 - acc: 0.7099 - val_loss: 1.0472 - val_acc: 0.6495 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.7920 - acc: 0.7304\n",
      "Epoch 00007: val_acc improved from 0.64950 to 0.66890, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 224s 359ms/step - loss: 0.7920 - acc: 0.7304 - val_loss: 0.9571 - val_acc: 0.6689 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.6981 - acc: 0.7597\n",
      "Epoch 00008: val_acc improved from 0.66890 to 0.71270, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 198s 316ms/step - loss: 0.6981 - acc: 0.7597 - val_loss: 0.8399 - val_acc: 0.7127 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.5965 - acc: 0.7944\n",
      "Epoch 00009: val_acc did not improve from 0.71270\n",
      "625/625 [==============================] - 198s 317ms/step - loss: 0.5965 - acc: 0.7944 - val_loss: 0.9060 - val_acc: 0.6966 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.5490 - acc: 0.8120\n",
      "Epoch 00010: val_acc improved from 0.71270 to 0.72220, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 205s 328ms/step - loss: 0.5490 - acc: 0.8120 - val_loss: 0.8788 - val_acc: 0.7222 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.5466 - acc: 0.8133\n",
      "Epoch 00011: val_acc did not improve from 0.72220\n",
      "625/625 [==============================] - 202s 324ms/step - loss: 0.5466 - acc: 0.8133 - val_loss: 0.8999 - val_acc: 0.7030 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.4681 - acc: 0.8394\n",
      "Epoch 00012: val_acc did not improve from 0.72220\n",
      "625/625 [==============================] - 200s 320ms/step - loss: 0.4681 - acc: 0.8394 - val_loss: 0.9597 - val_acc: 0.7037 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.4232 - acc: 0.8536\n",
      "Epoch 00013: val_acc improved from 0.72220 to 0.72420, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 201s 322ms/step - loss: 0.4232 - acc: 0.8536 - val_loss: 0.8706 - val_acc: 0.7242 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3638 - acc: 0.8742\n",
      "Epoch 00014: val_acc improved from 0.72420 to 0.74550, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 202s 323ms/step - loss: 0.3638 - acc: 0.8742 - val_loss: 0.8474 - val_acc: 0.7455 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3353 - acc: 0.8837\n",
      "Epoch 00015: val_acc did not improve from 0.74550\n",
      "625/625 [==============================] - 201s 322ms/step - loss: 0.3353 - acc: 0.8837 - val_loss: 1.0021 - val_acc: 0.7168 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3265 - acc: 0.8873\n",
      "Epoch 00016: val_acc did not improve from 0.74550\n",
      "625/625 [==============================] - 199s 318ms/step - loss: 0.3265 - acc: 0.8873 - val_loss: 0.8732 - val_acc: 0.7294 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2670 - acc: 0.9081\n",
      "Epoch 00017: val_acc did not improve from 0.74550\n",
      "625/625 [==============================] - 199s 319ms/step - loss: 0.2670 - acc: 0.9081 - val_loss: 0.9968 - val_acc: 0.7257 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2414 - acc: 0.9166\n",
      "Epoch 00018: val_acc did not improve from 0.74550\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "625/625 [==============================] - 201s 321ms/step - loss: 0.2414 - acc: 0.9166 - val_loss: 0.9801 - val_acc: 0.7322 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1304 - acc: 0.9555\n",
      "Epoch 00019: val_acc improved from 0.74550 to 0.75580, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 201s 321ms/step - loss: 0.1304 - acc: 0.9555 - val_loss: 0.9746 - val_acc: 0.7558 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.0934 - acc: 0.9688\n",
      "Epoch 00020: val_acc improved from 0.75580 to 0.76290, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 201s 322ms/step - loss: 0.0934 - acc: 0.9688 - val_loss: 1.0272 - val_acc: 0.7629 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.0852 - acc: 0.9709\n",
      "Epoch 00021: val_acc did not improve from 0.76290\n",
      "625/625 [==============================] - 199s 318ms/step - loss: 0.0852 - acc: 0.9709 - val_loss: 1.1476 - val_acc: 0.7565 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.0747 - acc: 0.9743\n",
      "Epoch 00022: val_acc did not improve from 0.76290\n",
      "625/625 [==============================] - 199s 319ms/step - loss: 0.0747 - acc: 0.9743 - val_loss: 1.1294 - val_acc: 0.7625 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.0645 - acc: 0.9780\n",
      "Epoch 00023: val_acc did not improve from 0.76290\n",
      "625/625 [==============================] - 200s 321ms/step - loss: 0.0645 - acc: 0.9780 - val_loss: 1.2386 - val_acc: 0.7502 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.0650 - acc: 0.9783\n",
      "Epoch 00024: val_acc did not improve from 0.76290\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "625/625 [==============================] - 201s 321ms/step - loss: 0.0650 - acc: 0.9783 - val_loss: 1.3111 - val_acc: 0.7393 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.0337 - acc: 0.9894\n",
      "Epoch 00025: val_acc improved from 0.76290 to 0.77020, saving model to cifar10_Densenet121_current.hdf5\n",
      "625/625 [==============================] - 202s 323ms/step - loss: 0.0337 - acc: 0.9894 - val_loss: 1.1452 - val_acc: 0.7702 - lr: 2.5000e-04\n",
      "Epoch 26/50\n",
      "235/625 [==========>...................] - ETA: 1:57 - loss: 0.0191 - acc: 0.9941"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=False,  \n",
    "                                             samplewise_center=False, \n",
    "                                             featurewise_std_normalization=False, \n",
    "                                             samplewise_std_normalization=False, \n",
    "                                             zca_whitening=False, ## val只有0.1\n",
    "                                             zca_epsilon=1e-06, \n",
    "                                             rotation_range=0.0, \n",
    "                                             width_shift_range=0.0, ## 0.2\n",
    "                                             height_shift_range=0.0, ## 0.1\n",
    "                                             brightness_range=None, ## 不要用，很爛\n",
    "                                             shear_range=0.0, \n",
    "                                             zoom_range=0.0, \n",
    "                                             channel_shift_range=0.0, \n",
    "                                             fill_mode='nearest', \n",
    "                                             cval=0.0, \n",
    "                                             horizontal_flip=False, \n",
    "                                             vertical_flip=False, ## 不要用，非常糟\n",
    "                                             rescale=None, \n",
    "                                             preprocessing_function=None, \n",
    "                                             data_format=None, \n",
    "                                             validation_split=0.0, \n",
    "                                             dtype=None)\n",
    "\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=0,\n",
    "#     width_shift_range=0.0,\n",
    "#     height_shift_range=0.0)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(x_train_trans, augment=True, rounds=2, seed=None)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "show_augmented_result_dir = './ImageDataGenerator_result'\n",
    "\n",
    "history = model.fit(datagen.flow(x_train_trans, y_train_trans, batch_size=BATCH_SIZE, shuffle=True), shuffle=True,\n",
    "                              steps_per_epoch=len(x_train_trans)/BATCH_SIZE, epochs=EPOCHS,\n",
    "                              validation_data=(x_val, y_val),validation_steps=len(x_val)/BATCH_SIZE,\n",
    "                              callbacks=callbacks_list)\n",
    "\n",
    "''' baseline of x_train: 0.5/0.27[1] ~ 0.94/0.33[4] '''\n",
    "'''\n",
    "0.（不能用，因為value是float）FancyPCA (alpha=0.1, always_apply=False, p=0.5): X\n",
    "\n",
    "1. (效果超好！)A.HueSaturationValue (hue_shift_limit=(-40./255, -40./255), sat_shift_limit=(-60./255, -60./255), \n",
    "val_shift_limit=(-40./255, -40./255), always_apply=False, p=1)\n",
    ": 0.41/0.15[1] -> 0.86/0.60[7] -> 0.99/0.66[31]\n",
    "\n",
    "2. (還行)A.ChannelDropout (channel_drop_range=(1, 1), fill_value=0, always_apply=False, p=1)\n",
    ": 0.28/0.29[1] -> 0.85/0.38[7] -> 0.95/0.40[10]\n",
    "\n",
    "3. (bad)A.Downscale (scale_min=0.1, scale_max=0.1, interpolation=cv2.INTER_NEAREST, always_apply=False, p=1)\n",
    ": 0.47/0.24[1] -> 0.94/0.18[4]\n",
    "\n",
    "4. (bad)A.Emboss (alpha=(0.2, 0.5), strength=(0.2, 0.7), always_apply=False, p=1)\n",
    ": 0.49/0.23 -> 0.95/0.25[4]\n",
    "\n",
    "5. (還不錯)A.ColorJitter (brightness=0.1, contrast=0.2, saturation=0.3, hue=0.2, always_apply=False, p=1)\n",
    ": 0.41/0.27[1] -> 0.89/0.46[6]\n",
    "\n",
    "6. (bad)A.RandomBrightnessContrast (brightness_limit=0.1, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1)\n",
    ": 0.49/0.26[1] -> 0.93/0.26[4]\n",
    "\n",
    "7. (超棒！)A.ChannelShuffle(p=1.0)\n",
    ": 0.38/0.24[1] -> 0.84/0.59[8] -> 1.0/0.72[50?]\n",
    "(有把training data存下來了('channelShuffle_data.npy')，因為怕是random channelShuffle剛好有不錯的結果)(QQ被我自己洗掉了)\n",
    "\n",
    "8.（普通）A.GridDistortion (num_steps=5, distort_limit=0.3, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=1)\n",
    ": 0.44/0.25[1] -> 0.92/0.37[6] -> 0.95/0.32[10]\n",
    "\n",
    "9. (沒用)HorizontalFlip(p=0.1)\n",
    ": 0.50/0.27[1] -> 0.92/0.29[3]\n",
    "實驗證明，flip在這個task根本沒有幫助\n",
    "\n",
    "10.（普通）A.RGBShift (r_shift_limit=(-40./255, 40./255), g_shift_limit=(-40./255, 40./255), b_shift_limit=(-40./255, 40./255), always_apply=False, p=1.0)\n",
    ": 0.9/0.37[6]\n",
    "跟shift的value有關\n",
    "\n",
    "11. (不錯呦！)A.ToGray(p=1.0)\n",
    ": 0.39/0.29[1] -> 0.86/0.55[8]\n",
    "\n",
    "12. (不錯呦！)A.ToSepia (always_apply=False, p=1.0)\n",
    ": 0.39/0.18[1] -> 0.79/0.52[6] -> 0.96/0.57[11]\n",
    "train epochs要多點, lr小點，太大會overshoot\n",
    "\n",
    "13. (bad)A.Superpixels (p_replace=0.1, n_segments=100, max_size=32, interpolation=1, always_apply=False, p=1.0)\n",
    ": 0.53/0.21[1] -> 0.97/0.29[9]\n",
    "\n",
    "# combined result:\n",
    "\n",
    "1. ((1, 7, 7, 11, 12), (5)), with data比例(1: 1: 0.5: 1: 1: 0.5)\n",
    ": 1.00/0.7374[68]\n",
    "和7.沒什麼差\n",
    "\n",
    "2. (1, 7) with (1: 1)\n",
    ": 0.99/0.70[13]\n",
    "\n",
    "3. (1.+7.)\n",
    ": 0.97/0.68[16]\n",
    "\n",
    "4. (7., 7.) with (1: 1)\n",
    ": 1.00/0.72[33]\n",
    "\n",
    "5. （很棒！）(5.+7.) \n",
    ": ?/?[1] -> 0.?/0.?[9] -> 0.?/0.?[20]\n",
    "- 用ReduceLROnPlateau的training strategy可以在acc高的時候，依舊能改善acc而不一直overshoot\n",
    "\n",
    "6. dropchannel (6-1)種?\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('./output_training_plot/densenet121_try5_current', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load('./datalab-cup4-unlearnable-datasets-cifar-10/x_test_cifar10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.argmax(model.predict(x_test, batch_size=BATCH_SIZE, verbose=1), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_csv = [i for i in range(len(y_test))]\n",
    "df = pd.DataFrame(list(zip(id_csv,y_test)), columns = ['id','label'])\n",
    "df.to_csv('./output_csv/cifar10_try5_densenet121_current.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
