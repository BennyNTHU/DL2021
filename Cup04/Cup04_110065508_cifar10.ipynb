{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a30b29cf",
   "metadata": {},
   "source": [
    "# Cup04: Unlearnable Dataset\n",
    "110065508 李丞恩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354b1014",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (32, 32, 3)\n",
    "BATCH_SIZE = 64 \n",
    "EPOCHS = 100\n",
    "PATIENCE = 10 # 如果過多少個EPOCHS沒改善就停止訓練\n",
    "CATGORICAL = 2\n",
    "LR = 0.001\n",
    "LR_FACTOR = 0.5 # new_lr = lr * factor.\n",
    "LR_PATIENCE = 4 # umber of epochs with no improvement after which learning rate will be reduced\n",
    "MODEL_NAME = 'ResNet50_cup04_cifar10'\n",
    "MODEL_PATH = './model/' + MODEL_NAME + '.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c05d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1313bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efed0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a3d4bb",
   "metadata": {},
   "source": [
    "## 一. 引入Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14c4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('./dataset/x_train_cifar10_unlearn.npy')\n",
    "y_train = np.load('./dataset/y_train_cifar10.npy')\n",
    "x_val = np.load('./dataset/x_val_cifar10.npy')\n",
    "y_val = np.load('./dataset/y_val_cifar10.npy')\n",
    "x_test = np.load('./dataset/x_test_cifar10.npy')\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_val = np.argmax(y_val, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd071252",
   "metadata": {},
   "source": [
    "## 二. Auto encoder & decoder\n",
    "### 1. 搭建自動編解碼器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "babb9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoise(Model):\n",
    "    def __init__(self):\n",
    "        super(Denoise, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=INPUT_SHAPE),\n",
    "            layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv2D(3, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee11732",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Denoise()\n",
    "autoencoder.build((None, INPUT_SHAPE[0], INPUT_SHAPE[1], INPUT_SHAPE[2]))\n",
    "autoencoder.compile(optimizer='adam', loss=tf.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a2f36ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 16, 16, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 8)           1160      \n",
      "=================================================================\n",
      "Total params: 1,608\n",
      "Trainable params: 1,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738d73c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 3)         435       \n",
      "=================================================================\n",
      "Total params: 2,187\n",
      "Trainable params: 2,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175dea83",
   "metadata": {},
   "source": [
    "### 2. 執行降噪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac85aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((x_train, x_val, x_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee9fc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 25s 12ms/step - loss: 0.0091\n",
      "Epoch 2/10\n",
      " 325/1875 [====>.........................] - ETA: 19s - loss: 0.0044"
     ]
    }
   ],
   "source": [
    "autoencoder.fit(X, X,\n",
    "                epochs=10,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0cb7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in autoencoder.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74666ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a8434",
   "metadata": {},
   "source": [
    "## 三. 資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(images, labels):\n",
    "    alpha = tf.random.uniform([], 0, 1)\n",
    "    mixedup_images = (alpha * images +\n",
    "                     (1 - alpha) * tf.reverse(images, axis=[0]))\n",
    "    return mixedup_images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d450f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_dataset(dataset):\n",
    "    mixup_ds = dataset.map(mixup, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.concatenate(mixup_ds)\n",
    "    dataset = dataset.shuffle(BATCH_SIZE*10)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f66cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "batched_dataset = mixup_dataset(train_dataset)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "batched_val_dataset = mixup_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12922426",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in batched_dataset.take(1):\n",
    "    plt.imshow(data[0][3],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168f556",
   "metadata": {},
   "source": [
    "## 三. 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6464973",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.Rescaling(scale=1/255),\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\", input_shape=INPUT_SHAPE),\n",
    "    layers.experimental.preprocessing.RandomContrast(factor = 0.5),\n",
    "    layers.experimental.preprocessing.Normalization()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5be3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    autoencoder,\n",
    "    data_augmentation,\n",
    "    ResNet50(include_top=False, weights=None, input_tensor=None, input_shape=INPUT_SHAPE), \n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(CATGORICAL, activation='softmax', name='softmax')\n",
    "],\n",
    "    name=MODEL_NAME\n",
    ")\n",
    "model.build((None, INPUT_SHAPE[0], INPUT_SHAPE[1], INPUT_SHAPE[2]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=LR)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b4593",
   "metadata": {},
   "source": [
    "## 四. 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16434803",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=MODEL_PATH, \n",
    "                             monitor='val_acc',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='auto', \n",
    "                             save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                   factor=LR_FACTOR, \n",
    "                                   patience=LR_PATIENCE, \n",
    "                                   verbose=1, \n",
    "                                   mode='auto', \n",
    "                                   min_delta=0.0001)\n",
    "early = EarlyStopping(monitor='val_acc', \n",
    "                      mode=\"auto\", \n",
    "                      patience=PATIENCE)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "history = model.fit(x=batched_dataset, \n",
    "                    validation_data=batched_val_dataset, \n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=1)\n",
    "end = datetime.datetime.now()\n",
    "CNN_time = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0f162",
   "metadata": {},
   "source": [
    "## 五. 繪製結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f0bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = len(history.history['acc'])\n",
    "train_acc = np.max(history.history['acc'])\n",
    "val_acc = np.max(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3379268",
   "metadata": {},
   "outputs": [],
   "source": [
    "localtime = time.strftime(\"%Y-%m-%d %I:%M:%S %p\", time.localtime())\n",
    "with open('./output/result_cifar10.txt',  'a') as file_obj:\n",
    "    file_obj.write('================RUN TIME: '+ localtime+ '================' + '\\n')\n",
    "    file_obj.write('model_name: '    + str(MODEL_NAME)        + '\\n')\n",
    "    file_obj.write('epoch: '         + str(epoch)             + '\\n')\n",
    "    file_obj.write('training time: ' + str(CNN_time)          + '\\n')\n",
    "    file_obj.write('train_acc: '     + str(train_acc)     + '\\n')\n",
    "    file_obj.write('val_acc: '       + str(val_acc)       + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251dd260",
   "metadata": {},
   "source": [
    "## 六. 繳交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1053002",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc915d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.argmax(model.predict(x_test, batch_size=BATCH_SIZE, verbose=1), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ce1af",
   "metadata": {},
   "source": [
    "Make submit file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fcf8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_csv = [i for i in range(len(y_test))]\n",
    "df = pd.DataFrame(list(zip(id_csv,y_test)), columns = ['id','label'])\n",
    "df.to_csv('./output/cup04_cifar10_group2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
