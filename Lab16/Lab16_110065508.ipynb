{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # this line disable pop-out window\n",
    "game = FlappyBird()\n",
    "env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n",
    "env.reset_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ff327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a dictionary whose key is action description and value is action index\n",
    "print(game.actions)\n",
    "# return a list of action index (include None)\n",
    "print(env.getActionSet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary describe state\n",
    "'''\n",
    "    player y position.\n",
    "    players velocity.\n",
    "    next pipe distance to player\n",
    "    next pipe top y position\n",
    "    next pipe bottom y position\n",
    "    next next pipe distance to player\n",
    "    next next pipe top y position\n",
    "    next next pipe bottom y position\n",
    "'''\n",
    "game.getGameState()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b439bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "MIN_EXPLORING_RATE = 0.01\n",
    "MIN_LEARNING_RATE = 0.5\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 bucket_range_per_feature,\n",
    "                 num_action,\n",
    "                 t=0,\n",
    "                 discount_factor=0.99):\n",
    "        self.update_parameters(t)  # init explore rate and learning rate\n",
    "        self.q_table = defaultdict(lambda: np.zeros(num_action))\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_action = num_action\n",
    "\n",
    "        # how to discretize each feature in a state\n",
    "        # the higher each value, less time to train but with worser performance\n",
    "        # e.g. if range = 2, feature with value 1 is equal to feature with value 0 bacause int(1/2) = int(0/2)\n",
    "        self.bucket_range_per_feature = bucket_range_per_feature\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # epsilon-greedy\n",
    "        state_idx = self.get_state_idx(state)\n",
    "        if np.random.rand() < self.exploring_rate:\n",
    "            action = np.random.choice(num_action)  # Select a random action\n",
    "        else:\n",
    "            action = np.argmax(\n",
    "                self.q_table[state_idx])  # Select the action with the highest q\n",
    "        return action\n",
    "\n",
    "    def update_policy(self, state, action, reward, state_prime):\n",
    "        state_idx = self.get_state_idx(state)\n",
    "        state_prime_idx = self.get_state_idx(state_prime)\n",
    "        # Update Q_value using Q-learning update rule\n",
    "        best_q = np.max(self.q_table[state_prime_idx])\n",
    "        self.q_table[state_idx][action] += self.learning_rate * (\n",
    "            reward + self.discount_factor * best_q - self.q_table[state_idx][action])\n",
    "\n",
    "    def get_state_idx(self, state):\n",
    "        # instead of using absolute position of pipe, use relative position\n",
    "        state = copy.deepcopy(state)\n",
    "        state['next_next_pipe_bottom_y'] -= state['player_y']\n",
    "        state['next_next_pipe_top_y'] -= state['player_y']\n",
    "        state['next_pipe_bottom_y'] -= state['player_y']\n",
    "        state['next_pipe_top_y'] -= state['player_y']\n",
    "\n",
    "        # sort to make list converted from dict ordered in alphabet order\n",
    "        state_key = [k for k, v in sorted(state.items())]\n",
    "\n",
    "        # do bucketing to decrease state space to speed up training\n",
    "        state_idx = []\n",
    "        for key in state_key:\n",
    "            state_idx.append(\n",
    "                int(state[key] / self.bucket_range_per_feature[key]))\n",
    "        return tuple(state_idx)\n",
    "\n",
    "    def update_parameters(self, episode):\n",
    "        self.exploring_rate = max(MIN_EXPLORING_RATE,\n",
    "                                  min(0.5, 0.99**((episode) / 30)))\n",
    "        self.learning_rate = max(MIN_LEARNING_RATE, min(0.5, 0.99\n",
    "                                                        ** ((episode) / 30)))\n",
    "\n",
    "    def shutdown_explore(self):\n",
    "        # make action selection greedy\n",
    "        self.exploring_rate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_action = len(env.getActionSet())\n",
    "bucket_range_per_feature = {\n",
    "  'next_next_pipe_bottom_y': 40,\n",
    "  'next_next_pipe_dist_to_player': 512,\n",
    "  'next_next_pipe_top_y': 40,\n",
    "  'next_pipe_bottom_y': 20,\n",
    "  'next_pipe_dist_to_player': 20,\n",
    "  'next_pipe_top_y': 20,\n",
    "  'player_vel': 4,\n",
    "  'player_y': 16\n",
    "}\n",
    "# init agent\n",
    "agent = Agent(bucket_range_per_feature, num_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "\n",
    "\n",
    "def make_anim(images, fps=60, true_image=False):\n",
    "    duration = len(images) / fps\n",
    "\n",
    "    def make_frame(t):\n",
    "        try:\n",
    "            x = images[int(len(images) / duration * t)]\n",
    "        except:\n",
    "            x = images[-1]\n",
    "\n",
    "        if true_image:\n",
    "            return x.astype(np.uint8)\n",
    "        else:\n",
    "            return ((x + 1) / 2 * 255).astype(np.uint8)\n",
    "\n",
    "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
    "    clip.fps = fps\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "reward_per_epoch = []\n",
    "lifetime_per_epoch = []\n",
    "exploring_rates = []\n",
    "learning_rates = []\n",
    "print_every_episode = 500\n",
    "show_gif_every_episode = 5000\n",
    "NUM_EPISODE = 40000\n",
    "for episode in range(0, NUM_EPISODE):\n",
    "\n",
    "    # Reset the environment\n",
    "    env.reset_game()\n",
    "\n",
    "    # record frame\n",
    "    frames = [env.getScreenRGB()]\n",
    "\n",
    "    # for every 500 episodes, shutdown exploration to see performance of greedy action\n",
    "    if episode % print_every_episode == 0:\n",
    "        agent.shutdown_explore()\n",
    "\n",
    "    # the initial state\n",
    "    state = game.getGameState()\n",
    "    # cumulate reward for this episode\n",
    "    cum_reward = 0  \n",
    "    t = 0\n",
    "\n",
    "    while not env.game_over():\n",
    "\n",
    "        # select an action\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # execute the action and get reward\n",
    "        # reward = +1 when pass a pipe, -5 when die\n",
    "        reward = env.act(env.getActionSet()[action])  \n",
    "\n",
    "        frames.append(env.getScreenRGB())\n",
    "\n",
    "        # cumulate reward\n",
    "        cum_reward += reward\n",
    "\n",
    "        # observe the result\n",
    "        state_prime = game.getGameState()  # get next state\n",
    "\n",
    "        # update agent\n",
    "        agent.update_policy(state, action, reward, state_prime)\n",
    "\n",
    "        # Setting up for the next iteration\n",
    "        state = state_prime\n",
    "        t += 1\n",
    "\n",
    "    # update exploring_rate and learning_rate\n",
    "    agent.update_parameters(episode)\n",
    "\n",
    "    if episode % print_every_episode == 0:\n",
    "        print(\"Episode {} finished after {} time steps, cumulated reward: {}, exploring rate: {}, learning rate: {}\".format(\n",
    "            episode,\n",
    "            t,\n",
    "            cum_reward,\n",
    "            agent.exploring_rate,\n",
    "            agent.learning_rate\n",
    "        ))\n",
    "        reward_per_epoch.append(cum_reward)\n",
    "        exploring_rates.append(agent.exploring_rate)\n",
    "        learning_rates.append(agent.learning_rate)\n",
    "        lifetime_per_epoch.append(t)\n",
    "\n",
    "    # for every 5000 episode, record an animation\n",
    "    if episode % show_gif_every_episode == 0:\n",
    "        print(\"len frames:\", len(frames))\n",
    "        clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n",
    "        display(clip.ipython_display(fps=60, autoplay=1, loop=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo():\n",
    "    # Reset the environment\n",
    "    env.reset_game()\n",
    "\n",
    "    # record frame\n",
    "    frames = [env.getScreenRGB()]\n",
    "\n",
    "    # shutdown exploration to see performance of greedy action\n",
    "    agent.shutdown_explore()\n",
    "\n",
    "    # the initial state\n",
    "    state = game.getGameState()\n",
    "\n",
    "    while not env.game_over():\n",
    "        # select an action\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # execute the action and get reward\n",
    "        reward = env.act(env.getActionSet()[action])\n",
    "\n",
    "        frames.append(env.getScreenRGB())\n",
    "\n",
    "        # observe the result\n",
    "        state_prime = game.getGameState()  # get next state\n",
    "\n",
    "        # Setting up for the next iteration\n",
    "        state = state_prime\n",
    "\n",
    "    clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n",
    "    display(clip.ipython_display(fps=60, autoplay=1, loop=1))\n",
    "\n",
    "\n",
    "demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5dfdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot life time against training episodes\n",
    "fig, ax1 = plt.subplots(figsize=(20, 5))\n",
    "plt.plot(range(len(lifetime_per_epoch)), lifetime_per_epoch)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8aa7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reward against training episodes\n",
    "fig, ax1 = plt.subplots(figsize=(20, 5))\n",
    "plt.plot(range(len(reward_per_epoch)), reward_per_epoch)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
