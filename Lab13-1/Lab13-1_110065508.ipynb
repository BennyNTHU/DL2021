{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd850f9",
   "metadata": {},
   "source": [
    "# Lab13-1 Sentiment Analysis\n",
    "110065508 李丞恩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3dfd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 14:28:31.526708: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f39a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 14:28:32.416529: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-25 14:28:32.440687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 14:28:32.441155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.852GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2021-11-25 14:28:32.441173: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-11-25 14:28:32.443323: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-11-25 14:28:32.443364: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-11-25 14:28:32.443906: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-25 14:28:32.444033: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-25 14:28:32.444598: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-11-25 14:28:32.444995: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-11-25 14:28:32.445066: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-11-25 14:28:32.445130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 14:28:32.445605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 14:28:32.446041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-11-25 14:28:32.446648: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-25 14:28:32.447105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 14:28:32.447565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.852GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2021-11-25 14:28:32.447605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 14:28:32.448061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 14:28:32.448552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-11-25 14:28:32.448573: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-11-25 14:28:32.762973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-25 14:28:32.762996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-11-25 14:28:32.763000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-11-25 14:28:32.763144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 14:28:32.763608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 14:28:32.764034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 14:28:32.764462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9986 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280852d",
   "metadata": {},
   "source": [
    "## 一. Data preprocessing\n",
    "### 1. Dataset檢查\n",
    "load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af00f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = pd.read_csv(\"./data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea5420",
   "metadata": {},
   "source": [
    "check if there is any null value in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6741d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f97380",
   "metadata": {},
   "source": [
    "show the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c1ed1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa491d1",
   "metadata": {},
   "source": [
    "show the first five data in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc8d90f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "093cf033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews[\"review\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d20c78",
   "metadata": {},
   "source": [
    "### 2. 利用正規表達式去除不必要的字元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d875e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b7425d",
   "metadata": {},
   "source": [
    "將全部的data處理完，並讀取是positive還是negative的評論。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cecd08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(movie_reviews['review'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))\n",
    "\n",
    "# replace the positive with 1, replace the negative with 0\n",
    "y = movie_reviews['sentiment']\n",
    "y = np.array(list(map(lambda x: 1 if x == \"positive\" else 0, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6326fd76",
   "metadata": {},
   "source": [
    "### 3. 拆成training set和testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3811c7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training data: 40000\n",
      "# test data: 10000\n"
     ]
    }
   ],
   "source": [
    "# Split the training dataset and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "print(\"# training data: {:d}\\n# test data: {:d}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04bba0",
   "metadata": {},
   "source": [
    "### 4. 執行分詞並轉為詞向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763def0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 100\n",
    "# padding sentences to the same length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eb077c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,  296,  140, 2854,    2,  405,  614,    1,  263,    5, 3514,\n",
       "        977,    4,   25,   37,   11, 1237,  215,   62,    2,   35,    5,\n",
       "         27,  217,   24,  189, 1430,    7, 1068,   15, 4868,   81,    1,\n",
       "        221,   63,  351,   64,   52,   24,    4, 3547,   13,    6,   19,\n",
       "        192,    4, 8148,  859, 3430, 1720,   17,   23,    4,  158,  194,\n",
       "        175,  106,    9, 1604,  461,   71,  218,    4,  321,    2, 3431,\n",
       "         31,   20,   47,   68, 1844, 4668,   11,    6, 1365,    8,   16,\n",
       "          5, 3475, 1990,   14,   59,    1, 2380,  460,  518,    2,  170,\n",
       "       2524, 2698, 1745,    4,  573,    6,   33,    1, 3750,  198,  345,\n",
       "       3812], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the preprocessed data\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce79bf3a",
   "metadata": {},
   "source": [
    "### 5. 將training set和testing set轉為tf.dataset的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb410ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 100]), TensorShape([128]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "# only reserve 10000 words\n",
    "vocab_size = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445cdafc",
   "metadata": {},
   "source": [
    "## 二. Encoder & Decoder\n",
    "### 1. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ef2a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        # vacab_size=10000, embedding_dim=256 enc_units=1024 batch_sz=64\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_activation='sigmoid',\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # x is the training data with shape == (batch_size，max_length)  -> (128, 100)\n",
    "        # which means there are batch_size sentences in one batch, the length of each sentence is max_length\n",
    "        # hidden state shape == (batch_size, units) -> (128, 1024)\n",
    "        # after embedding, x shape == (batch_size, max_length, embedding_dim) -> (128, 100, 256)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # output contains the state(in GRU, the hidden state and the output are same) from all timestamps,\n",
    "        # output shape == (batch_size, max_length, units) -> (128, 100, 1024)\n",
    "        # state is the hidden state of the last timestamp, shape == (batch_size, units) -> (128, 1024)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        # initialize the first state of the gru,  shape == (batch_size, units) -> (128, 1024)\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a65b4a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 14:28:43.460710: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (128, 100, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (128, 1024)\n",
      "tf.Tensor([ True  True  True ...  True  True  True], shape=(1024,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 14:28:43.859024: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8100\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "# the output and the hidden state of GRU is equal\n",
    "print(sample_output[-1, -1, :] == sample_hidden[-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad0756",
   "metadata": {},
   "source": [
    "### 2. LuongAttention\n",
    "令$h_i$表hidden state of the encoder，$s_t$表hidden state of the decoder，$W_a$表the trainable weights，則定義Luong Attention的score function為：\n",
    "$$\\text{score}(s_t,h_i)=s_t^TW_ah_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a0b05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values): # (query, values) = (dec_hidden, enc_output)\n",
    "        # query shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(self.W(values))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = score\n",
    "\n",
    "        # context_vector shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector = attention_weights * hidden_with_time_axis\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01cd43f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 14:28:43.976809: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (128, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (128, 100, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 14:28:44.372633: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-11-25 14:28:44.372756: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "attention_layer = LuongAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e77ae4",
   "metadata": {},
   "source": [
    "### 3. BahdanauAttention\n",
    "$$\\text{score}=(s_t,h_i)=v^T_a\\tanh(W_a[s_t;h_i])$$\n",
    "放這個也會work，滿有趣的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4293a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7eab114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (128, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (128, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2611398",
   "metadata": {},
   "source": [
    "### 3. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d9a27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        \n",
    "        # pass through four fully connected layers, the model will return \n",
    "        # the probability of the positivity of the sentence\n",
    "        self.fc_1 = tf.keras.layers.Dense(2048)\n",
    "        self.fc_2 = tf.keras.layers.Dense(512)\n",
    "        self.fc_3 = tf.keras.layers.Dense(64)\n",
    "        self.fc_4 = tf.keras.layers.Dense(1)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = LuongAttention(self.dec_units)\n",
    "        #self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        output = self.fc_1(context_vector)\n",
    "        output = self.fc_2(output)\n",
    "        output = self.fc_3(output)\n",
    "        output = self.fc_4(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5943d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (128, 1)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(units, BATCH_SIZE)\n",
    "sample_decoder_output, _ = decoder(sample_hidden, sample_output)\n",
    "print('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4d091",
   "metadata": {},
   "source": [
    "## 三. 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05152b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbef9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints/sentiment-analysis'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c68714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, _ = decoder(enc_hidden, enc_output)\n",
    "\n",
    "        loss = loss_function(targ, predictions)\n",
    "\n",
    "    # collect all trainable variables\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # calculate the gradients for the whole variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # apply the gradients on the variables\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92c689cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 14:28:45.197437: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-25 14:28:45.220270: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2592000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.6938\n",
      "Epoch 1 Batch 100 Loss 0.4136\n",
      "Epoch 1 Batch 200 Loss 0.3620\n",
      "Epoch 1 Batch 300 Loss 0.3668\n",
      "Epoch 1 Loss 0.4510\n",
      "Time taken for 1 epoch 24.096109867095947 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2778\n",
      "Epoch 2 Batch 100 Loss 0.3358\n",
      "Epoch 2 Batch 200 Loss 0.2981\n",
      "Epoch 2 Batch 300 Loss 0.3587\n",
      "Epoch 2 Loss 0.2856\n",
      "Time taken for 1 epoch 23.24446129798889 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2461\n",
      "Epoch 3 Batch 100 Loss 0.3241\n",
      "Epoch 3 Batch 200 Loss 0.2333\n",
      "Epoch 3 Batch 300 Loss 0.2168\n",
      "Epoch 3 Loss 0.2294\n",
      "Time taken for 1 epoch 22.565391778945923 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1496\n",
      "Epoch 4 Batch 100 Loss 0.1905\n",
      "Epoch 4 Batch 200 Loss 0.1298\n",
      "Epoch 4 Batch 300 Loss 0.2261\n",
      "Epoch 4 Loss 0.1828\n",
      "Time taken for 1 epoch 22.770443201065063 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1451\n",
      "Epoch 5 Batch 100 Loss 0.0964\n",
      "Epoch 5 Batch 200 Loss 0.1658\n",
      "Epoch 5 Batch 300 Loss 0.1241\n",
      "Epoch 5 Loss 0.1404\n",
      "Time taken for 1 epoch 22.547135829925537 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1075\n",
      "Epoch 6 Batch 100 Loss 0.1388\n",
      "Epoch 6 Batch 200 Loss 0.0989\n",
      "Epoch 6 Batch 300 Loss 0.2238\n",
      "Epoch 6 Loss 0.1042\n",
      "Time taken for 1 epoch 22.76311469078064 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0176\n",
      "Epoch 7 Batch 100 Loss 0.0702\n",
      "Epoch 7 Batch 200 Loss 0.0315\n",
      "Epoch 7 Batch 300 Loss 0.1047\n",
      "Epoch 7 Loss 0.0844\n",
      "Time taken for 1 epoch 22.55583119392395 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0465\n",
      "Epoch 8 Batch 100 Loss 0.0413\n",
      "Epoch 8 Batch 200 Loss 0.0867\n",
      "Epoch 8 Batch 300 Loss 0.0676\n",
      "Epoch 8 Loss 0.0662\n",
      "Time taken for 1 epoch 22.828900814056396 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0473\n",
      "Epoch 9 Batch 100 Loss 0.0287\n",
      "Epoch 9 Batch 200 Loss 0.0108\n",
      "Epoch 9 Batch 300 Loss 0.0313\n",
      "Epoch 9 Loss 0.0499\n",
      "Time taken for 1 epoch 22.956210136413574 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0527\n",
      "Epoch 10 Batch 100 Loss 0.0122\n",
      "Epoch 10 Batch 200 Loss 0.0412\n",
      "Epoch 10 Batch 300 Loss 0.0657\n",
      "Epoch 10 Loss 0.0448\n",
      "Time taken for 1 epoch 22.922815799713135 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the epochs for training\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # get the initial hidden state of gru\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "241cd29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/sentiment-analysis/ckpt-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe9bc7ada60>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ef76f",
   "metadata": {},
   "source": [
    "## 四. 預測結果\n",
    "### 1. 定義test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "521d97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(inp, enc_hidden):\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        predictions, attention_weights = decoder(enc_hidden, enc_output)\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5137d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    for batch, (inp, targ) in enumerate(test_data):\n",
    "        if len(inp) != BATCH_SIZE:\n",
    "            enc_hidden = tf.zeros((len(inp), units))\n",
    "        # make prediction\n",
    "        if batch == 0:\n",
    "            predictions, attention_weights = test_step(inp, enc_hidden)\n",
    "            predictions, attention_weights = predictions.numpy(), attention_weights.numpy()\n",
    "        else:\n",
    "            _predictions, _attention_weights = test_step(inp, enc_hidden)\n",
    "            _predictions, _attention_weights = _predictions.numpy(), _attention_weights.numpy()\n",
    "            predictions = np.concatenate((predictions, _predictions))\n",
    "            attention_weights = np.concatenate((attention_weights, _attention_weights))\n",
    "    \n",
    "    predictions = np.squeeze(predictions)\n",
    "    attention_weights = np.squeeze(attention_weights)\n",
    "    predictions[np.where(predictions < 0.5)] = 0\n",
    "    predictions[np.where(predictions >= 0.5)] = 1\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a54a8",
   "metadata": {},
   "source": [
    "### 2. 計算結果與準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "522579fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, attention_weights = evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92481c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.843\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', (y_pred == y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fb3e6",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9575d239",
   "metadata": {},
   "source": [
    "### 3. 結果視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fc3943d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: 1\n",
      "y_predict: 0\n",
      "changed it was \u001b[31mterrible\u001b[0m main event just like every match is in is \u001b[31mterrible\u001b[0m other matches on the card were razor ramon vs ted brothers vs \u001b[31mbodies\u001b[0m shawn michaels vs this was the event where shawn named his big monster of body guard vs kid hart first takes on then takes on jerry and stuff with the and was always very interesting then destroyed marty undertaker took on \u001b[31mgiant\u001b[0m in another \u001b[31mterrible\u001b[0m \u001b[31mmatch\u001b[0m \u001b[31mthe\u001b[0m smoking and took on bam bam and the and the world title against lex this match was \u001b[31mboring\u001b[0m \u001b[31mand\u001b[0m it has \u001b[31mterrible\u001b[0m ending however it deserves \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "of subject matter as are and broken in many ways on many many issues happened to see the pilot premiere in passing and just \u001b[31mhad\u001b[0m to keep in \u001b[31mafter\u001b[0m that to see if would ever get the girl \u001b[31mafter\u001b[0m seeing them all on \u001b[31mtelevision\u001b[0m \u001b[31mwas\u001b[0m delighted to see them available on dvd have to admit that it was the \u001b[31monly\u001b[0m thing that kept me sane whilst had to do \u001b[31mhour\u001b[0m night shift and developed \u001b[31minsomnia\u001b[0m farscape was the \u001b[31monly\u001b[0m thing to get me through those \u001b[31mextremely\u001b[0m long nights do yourself favour watch the pilot and see what mean farscape comet \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "destruction the first really \u001b[31mbad\u001b[0m thing is the guy steven \u001b[31mseagal\u001b[0m would have been beaten to pulp by \u001b[31mseagal\u001b[0m \u001b[31mdriving\u001b[0m but that probably would have ended the \u001b[31mwhole\u001b[0m premise for the movie it seems like they decided to make all kinds of changes in the movie plot so \u001b[31mjust\u001b[0m \u001b[31mplan\u001b[0m to enjoy the action and do \u001b[31mnot\u001b[0m expect coherent plot turn any sense of \u001b[31mlogic\u001b[0m you may have it will your chance of getting headache does give me some hope that \u001b[31msteven\u001b[0m seagal is trying to move back towards the type of characters he portrayed in his more popular movies \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "jane austen would definitely of this one paltrow does an awesome job capturing the attitude of emma she is funny without \u001b[31mbeing\u001b[0m \u001b[31msilly\u001b[0m yet elegant she puts on very convincing british accent \u001b[31mnot\u001b[0m being british myself maybe m \u001b[31mnot\u001b[0m the best judge but she fooled me she was also excellent in doors sometimes forget she american also brilliant are \u001b[31mjeremy\u001b[0m northam and \u001b[31msophie\u001b[0m \u001b[31mthompson\u001b[0m and law emma thompson sister and mother as the bates women they nearly steal the show and \u001b[31mms\u001b[0m law \u001b[31mdoesn\u001b[0m \u001b[31meven\u001b[0m have any lines highly recommended \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "reaches the point where they become \u001b[31mobnoxious\u001b[0m and simply \u001b[31mfrustrating\u001b[0m touch football puzzle family and talent shows are not how actual people behave it almost \u001b[31msickening\u001b[0m \u001b[31manother\u001b[0m big flaw is the woman carell is \u001b[31msupposed\u001b[0m to be falling for her in her first scene with steve carell is like watching stroke victim \u001b[31mtrying\u001b[0m to be what imagine is \u001b[31msupposed\u001b[0m to be unique and original in this \u001b[31mwoman\u001b[0m comes off as \u001b[31mmildly\u001b[0m \u001b[31mretarded\u001b[0m it makes me think that this movie is taking place on another planet left the theater wondering what just saw after thinking further don think it was much \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "the pace quick and energetic but most \u001b[31mimportantly\u001b[0m \u001b[31mhe\u001b[0m knows how to make comedy funny he \u001b[31mdoesn\u001b[0m the jokes and he understands that funny actors know what they re doing and he allows them to do it but \u001b[31msegal\u001b[0m \u001b[31mgoes\u001b[0m step further he gives tommy boy friendly almost nostalgic tone that both the genuinely and the critics didn like tommy boy \u001b[31mshame\u001b[0m \u001b[31mon\u001b[0m them movie \u001b[31mdoesn\u001b[0m have to be super sophisticated or intellectual to be funny god farley and spade were forced to do \u001b[31mmuted\u001b[0m comedy \u001b[31mla\u001b[0m the office this is great movie and one of my all time favorites \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "for once story of hope over the tragic reality our \u001b[31myouth\u001b[0m \u001b[31mface\u001b[0m rising draws one into scary and unfair world and shows through beautiful color and moving music \u001b[31mhow\u001b[0m one man and his \u001b[31mdedicated\u001b[0m \u001b[31mfriends\u001b[0m \u001b[31mchoose\u001b[0m \u001b[31mnot\u001b[0m to accept that world and change it through action and art an entertaining interesting emotional beautiful film showed this film to numerous high school students as well who all live in with \u001b[31mpoverty\u001b[0m and and gun violence and they were with anderson the \u001b[31mprotagonist\u001b[0m recommend this film to all ages over \u001b[31mdue\u001b[0m to subtitles and some images of death from all backgrounds \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 0\n",
      "people and \u001b[31msleeping\u001b[0m around that he kept secret from most people he feels free to have an affair with \u001b[31mquasi\u001b[0m \u001b[31mbecause\u001b[0m he kevin he figures out that he can fool some people with cards like hotel but it won get him out of those the of heaven are keeping track of him and everything he does \u001b[31mafter\u001b[0m reading all the theories on though it seems like identity is reminder of the different paths tony could ve taken in his life possibly along with the car \u001b[31mjoke\u001b[0m \u001b[31minvolving\u001b[0m that made \u001b[31mno\u001b[0m \u001b[31msense\u001b[0m to me \u001b[31motherwise\u001b[0m at that point my \u001b[31mbrain\u001b[0m out \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 1\n",
      "over again can remember how many times he said the universe is made out of tiny little strings it like they were trying to us into \u001b[31mjust\u001b[0m accepting are the best thing since bread finally the show ended \u001b[31moff\u001b[0m with an unpleasant \u001b[31msense\u001b[0m of competition between and clearly biased towards this is \u001b[31msupposed\u001b[0m \u001b[31mto\u001b[0m be an \u001b[31meducational\u001b[0m program about quantum physics \u001b[31mnot\u001b[0m about whether the us is better than europe or vice versa also felt that was part of the audiences need to see some \u001b[31mconflict\u001b[0m to \u001b[31mremain\u001b[0m interested please give me little more credit than that overall thumbs \u001b[31mdown\u001b[0m \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "the scenes involving joe character in particular the scenes in the \u001b[31mterribly\u001b[0m clich but still funny rich but \u001b[31mscrewed\u001b[0m \u001b[31mup\u001b[0m characters house where the story towards it final moments can see how was great stage play and while the film \u001b[31mmakers\u001b[0m did their best to translate this to \u001b[31mcelluloid\u001b[0m it simply \u001b[31mdidn\u001b[0m work and while laughed out loud at some of scenes and one liners think the first \u001b[31mminutes\u001b[0m my senses and expectations to such degree would have laughed at \u001b[31manything\u001b[0m \u001b[31munless\u001b[0m you re stuck for novelty coffee coaster \u001b[31mdon\u001b[0m pick this up if you see it in bargain bucket \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "for idx, data in enumerate(X_test[:10]):\n",
    "    print('y_true: {:d}'.format(y_test[idx]))\n",
    "    print('y_predict: {:.0f}'.format(y_pred[idx]))\n",
    "    \n",
    "    # get the twenty most largest attention weights\n",
    "    large_weights_idx = np.argsort(attention_weights[idx])[::-1][:10]\n",
    "    \n",
    "    for _idx in range(len(data)):\n",
    "        word_idx = data[_idx]\n",
    "        if word_idx != 0:\n",
    "            if _idx in large_weights_idx:\n",
    "                print(colored(tokenizer.index_word[word_idx], 'red'), end=' ')\n",
    "            else:\n",
    "                print(tokenizer.index_word[word_idx], end=' ')\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0ef12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
