{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e83960",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "## Requirement:\n",
    "\n",
    "### 1. should design your own model architecture. In other words, do not load the model or any pre-trained weights directly from other sources.\n",
    "### 2. use the first 100,000 images as training data, the next 20,000 as validation data, and the rest as testing data.\n",
    "### 3. Only if the whole word matches exactly does it count as correct\n",
    "### 4. predict the answer to the testing data and write them in a file.\n",
    "### 5. testing accuracy should be at least 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49926b88",
   "metadata": {},
   "source": [
    "## Procedure:\n",
    "\n",
    "### 0. Load Data:\n",
    "- load and split data into training and validation set\n",
    "\n",
    "### 1. Preprocessing:\n",
    "- create dictionary: index2char, char2index.(for annotations <-> indices)\n",
    "- find maximum length in annotations and convert annotations to index and pad to max_length\n",
    "- resize images into sizes \"based on what feature extractor you use\".\n",
    "- normalize images pixels into -1~1\n",
    "\n",
    "### 2. Design Feature Extractor:\n",
    "- \n",
    "- \n",
    "\n",
    "### 3. Design Encoder and Decoder\n",
    "- encoder, decoder, attention-based的設計和lab一樣\n",
    "\n",
    "### 4. Design Model and Training\n",
    "- 把input images丟到feature extractor得到features \n",
    "- 再把features, hidden_states, decoder_input丟到decoder\n",
    "- loss function, optimizer和lab一樣\n",
    "\n",
    "### 5. Do validation\n",
    "- \n",
    "- \n",
    "\n",
    "### 6. predict on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85191c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86e89b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 01:25:12.128181: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d5b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 01:25:13.008498: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-12-16 01:25:13.088481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 01:25:13.088967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.852GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2021-12-16 01:25:13.088985: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-12-16 01:25:13.090625: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-12-16 01:25:13.090654: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-12-16 01:25:13.091177: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-12-16 01:25:13.091296: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-12-16 01:25:13.091791: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-12-16 01:25:13.093246: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-12-16 01:25:13.093525: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-12-16 01:25:13.093713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 01:25:13.095039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 01:25:13.096134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-12-16 01:25:13.097530: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-16 01:25:13.098381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 01:25:13.099343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.852GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2021-12-16 01:25:13.099425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 01:25:13.100347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 01:25:13.101118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-12-16 01:25:13.101161: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-12-16 01:25:13.388137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-12-16 01:25:13.388165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-12-16 01:25:13.388169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-12-16 01:25:13.388332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 01:25:13.388794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 01:25:13.389222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-16 01:25:13.389638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10245 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4191391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20 # 40\n",
    "SHUFFLE_BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "IMG_HEIGHT = 448\n",
    "IMG_WIDTH = 224\n",
    "LEARNING_RATE = 5e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323f9c3",
   "metadata": {},
   "source": [
    "## 0. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6b2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = './words_captcha/'\n",
    "annotation_file = './words_captcha/spec_train_val.txt'\n",
    "\n",
    "with open(annotation_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "train_img_name = []\n",
    "val_img_name = []\n",
    "train_annotation = []\n",
    "val_annotation = []\n",
    "num = 0\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip('\\n')\n",
    "    line = line.split(' ')\n",
    "    if num < 100000:\n",
    "        train_img_name.append(line[0])\n",
    "        train_annotation.append(line[1])\n",
    "    else:\n",
    "        val_img_name.append(line[0])\n",
    "        val_annotation.append(line[1])\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725537b",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bba08a",
   "metadata": {},
   "source": [
    "### 1-(1): create dictionary: index2char, char2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30abc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {}\n",
    "idx2char = {}\n",
    "\n",
    "# add token <pad> to dictionary\n",
    "char2idx['<pad>'] = 0\n",
    "idx2char[0] = '<pad>'\n",
    "\n",
    "# only a~z appears in capcha\n",
    "for i in range(1, 27):\n",
    "    char2idx[chr(ord('a') + i - 1)] = i\n",
    "    idx2char[i] = chr(ord('a') + i - 1)\n",
    "    \n",
    "# add token <start>, <end> to dictionary\n",
    "char2idx['<start>'] = 27\n",
    "idx2char[27] = '<start>'\n",
    "char2idx['<end>'] = 28\n",
    "idx2char[28] = '<end>'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0b7aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '<start>': 27, '<end>': 28}\n",
      "{0: '<pad>', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: '<start>', 28: '<end>'}\n"
     ]
    }
   ],
   "source": [
    "print(char2idx)\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954996df",
   "metadata": {},
   "source": [
    "### 1-(2): find max_length of annotations and convert annotations to indcies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b40d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(annotations):\n",
    "    max_len = 0\n",
    "    for annotation in annotations:\n",
    "        if len(annotation) > max_len:\n",
    "            max_len = len(annotation)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cffeb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "max_len_train = max_length(train_annotation)\n",
    "max_len_val = max_length(val_annotation)\n",
    "\n",
    "max_len = max(max_len_train, max_len_val) + 2 ## '+2' for <start> and <end> indices!\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f30ae65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation_idx = []\n",
    "val_annotation_idx = []\n",
    "\n",
    "for annotation in train_annotation:\n",
    "    # pad <start> index\n",
    "    annotation_idx = [27]\n",
    "    for character in annotation:\n",
    "        annotation_idx.append(char2idx[character])\n",
    "        \n",
    "    # pad <end> index\n",
    "    annotation_idx.append(28)\n",
    "    \n",
    "    while len(annotation_idx) < max_len:\n",
    "        # pad <pad> index\n",
    "        annotation_idx.append(0)\n",
    "    train_annotation_idx.append(annotation_idx)\n",
    "    \n",
    "for annotation in val_annotation:\n",
    "    annotation_idx = [27]\n",
    "    for character in annotation:\n",
    "        annotation_idx.append(char2idx[character])\n",
    "    annotation_idx.append(28)\n",
    "    while len(annotation_idx) < max_len:\n",
    "        annotation_idx.append(0)\n",
    "    val_annotation_idx.append(annotation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0ac36bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thus\n",
      "[27, 20, 8, 21, 19, 28, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_annotation[0])\n",
    "print(train_annotation_idx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac5a67",
   "metadata": {},
   "source": [
    "### 1-(3) resize images into sizes based on VGG19 architecture, and do normalize to 1~-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0085148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f27aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_name, annotation):\n",
    "    img = tf.io.read_file(IMAGE_DIR + image_name + '.png')\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH)) # resize image(為的是要放到feature extractor裡面)\n",
    "    img = img/255 - 1. # normalize to 1~-1\n",
    "    return img, annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c31ddd",
   "metadata": {},
   "source": [
    "### create tf.dataset and make batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7217e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_img_name,train_annotation_idx))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_img_name,val_annotation_idx))\n",
    "\n",
    "train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(200)\n",
    "\n",
    "val_dataset = val_dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965908e",
   "metadata": {},
   "source": [
    "## 2. Design Feature Extractor: (based on VGG19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5dd43",
   "metadata": {},
   "source": [
    "VGG19 architecture: \n",
    "\n",
    "https://www.google.com/search?q=vgg19+architecture&tbm=isch&source=iu&ictx=1&fir=OiS_Va5y9f7f_M%252CGfR54ZAyOPXJ0M%252C_%253BgWpOWCUibA2HHM%252C-fAqcRc2_0E69M%252C_%253BBmAVVY3fiaEWSM%252Cr5RcjSElbvC9zM%252C_%253BIi4dkHHJCPBHhM%252C6SpiHVnsTHGfqM%252C_%253BN0Ee1uegeQS2MM%252CtmYWlqyrDnMDKM%252C_%253BDAYKPdDc3L8LWM%252CGfR54ZAyOPXJ0M%252C_%253Bulz740AMBJwELM%252CAQWccUmcjehYAM%252C_%253B7TyTAnvvZQlh2M%252Cz2PiMjqBWglM2M%252C_%253BfnUCv4NSRiu4sM%252CJ0XOR7dnwxmcfM%252C_%253BJCwII581R6z1lM%252CJVUs7QkfNRs0vM%252C_%253BetpWE5UJ8thKGM%252CG9LNfrtX5LmIQM%252C_%253BVZClRgHVIQzRDM%252CPLQfnY1iNdmakM%252C_%253B_r1pSVOfWcucoM%252CAzBUl5wwOfzzaM%252C_%253BMGWJupu_SmhpCM%252CJV6PdoPibOBEKM%252C_%253Bo3UI0f48t9XjTM%252CigQ3rReKeD7THM%252C_&vet=1&usg=AI4_-kRgiOWpbYylrLF6gz4Uqc-Symtxfw&sa=X&ved=2ahUKEwjY48WH0uP0AhVcs1YBHV_AC78Q9QF6BAgkEAE#imgrc=gWpOWCUibA2HHM\n",
    "\n",
    "Input: 448x224x3\n",
    "\n",
    "Output: 7x7x1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e13a7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_relu(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, size, stride):\n",
    "        super(conv_relu, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters, size, stride, padding=\"same\",\n",
    "                      kernel_initializer=tf.keras.initializers.TruncatedNormal())\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.lkrelu = tf.keras.layers.LeakyReLU(0.1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.batchnorm(x,training = training) ## batch normalization!\n",
    "        x = self.lkrelu(x) ## leaky relu!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33352dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extracter(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Feature_Extracter, self).__init__()\n",
    "        self.cr1_1 = conv_relu(64,3,1)\n",
    "        self.cr1_2 = conv_relu(64,3,1)\n",
    "        self.max_pooling1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr2_1 = conv_relu(128,3,1)\n",
    "        self.cr2_2 = conv_relu(128,3,1)\n",
    "        self.max_pooling2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr3_1 = conv_relu(256,3,1)\n",
    "        self.cr3_2 = conv_relu(256,3,1)\n",
    "        self.cr3_3 = conv_relu(256,3,1)\n",
    "        self.cr3_4 = conv_relu(256,3,1)\n",
    "        self.max_pooling3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr4_1 = conv_relu(512,3,1)\n",
    "        self.cr4_2 = conv_relu(512,3,1)\n",
    "        self.cr4_3 = conv_relu(512,3,1)\n",
    "        self.cr4_4 = conv_relu(512,3,1)\n",
    "        self.max_pooling4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr5_1 = conv_relu(512,3,1)\n",
    "        self.cr5_2 = conv_relu(512,3,1)\n",
    "        self.cr5_3 = conv_relu(512,3,1)\n",
    "        self.cr5_4 = conv_relu(512,3,1)\n",
    "        self.max_pooling5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr6_1 = conv_relu(1024,3,1)\n",
    "        self.cr6_2 = conv_relu(1024,3,1)\n",
    "        self.cr6_3 = conv_relu(1024,3,1)\n",
    "        self.cr6_4 = conv_relu(1024,3,1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.cr1_1(inputs,training)\n",
    "        x = self.cr1_2(x,training)\n",
    "        x = self.max_pooling1(x)\n",
    "        x = self.cr2_1(x,training)\n",
    "        x = self.cr2_2(x,training)\n",
    "        x = self.max_pooling2(x)\n",
    "        x = self.cr3_1(x,training)\n",
    "        x = self.cr3_2(x,training)\n",
    "        x = self.cr3_3(x,training)\n",
    "        x = self.cr3_4(x,training)\n",
    "        x = self.max_pooling3(x)\n",
    "        x = self.cr4_1(x,training)\n",
    "        x = self.cr4_2(x,training)\n",
    "        x = self.cr4_3(x,training)\n",
    "        x = self.cr4_4(x,training)        \n",
    "        x = self.max_pooling4(x)\n",
    "        x = self.cr5_1(x,training)\n",
    "        x = self.cr5_2(x,training)\n",
    "        x = self.cr5_3(x,training)\n",
    "        x = self.cr5_4(x,training)\n",
    "        x = self.max_pooling5(x)\n",
    "        x = self.cr6_1(x,training)\n",
    "        x = self.cr6_2(x,training)\n",
    "        x = self.cr6_3(x,training)\n",
    "        x = self.cr6_4(x,training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ddd3206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature__extracter\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_relu (conv_relu)        multiple                  2048      \n",
      "_________________________________________________________________\n",
      "conv_relu_1 (conv_relu)      multiple                  37184     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_relu_2 (conv_relu)      multiple                  74368     \n",
      "_________________________________________________________________\n",
      "conv_relu_3 (conv_relu)      multiple                  148096    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_relu_4 (conv_relu)      multiple                  296192    \n",
      "_________________________________________________________________\n",
      "conv_relu_5 (conv_relu)      multiple                  591104    \n",
      "_________________________________________________________________\n",
      "conv_relu_6 (conv_relu)      multiple                  591104    \n",
      "_________________________________________________________________\n",
      "conv_relu_7 (conv_relu)      multiple                  591104    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_relu_8 (conv_relu)      multiple                  1182208   \n",
      "_________________________________________________________________\n",
      "conv_relu_9 (conv_relu)      multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "conv_relu_10 (conv_relu)     multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "conv_relu_11 (conv_relu)     multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_relu_12 (conv_relu)     multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "conv_relu_13 (conv_relu)     multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "conv_relu_14 (conv_relu)     multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "conv_relu_15 (conv_relu)     multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_relu_16 (conv_relu)     multiple                  4723712   \n",
      "_________________________________________________________________\n",
      "conv_relu_17 (conv_relu)     multiple                  9442304   \n",
      "_________________________________________________________________\n",
      "conv_relu_18 (conv_relu)     multiple                  9442304   \n",
      "_________________________________________________________________\n",
      "conv_relu_19 (conv_relu)     multiple                  9442304   \n",
      "=================================================================\n",
      "Total params: 53,097,024\n",
      "Trainable params: 53,077,824\n",
      "Non-trainable params: 19,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extracter = Feature_Extracter()\n",
    "feature_extracter.build((None, IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "feature_extracter.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d3480",
   "metadata": {},
   "source": [
    "## 3. Design Encoder and Decoder\n",
    "\n",
    "和lab一樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aa35d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0728487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b42186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98f39273",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char2idx)\n",
    "num_steps = len(train_img_name) // BATCH_SIZE \n",
    "\n",
    "# Shape of the vector extracted from VGG19 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 1024\n",
    "attention_features_shape = 49 ## 因為出來是7x7x1024, reshape後變49x1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8066037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce647380",
   "metadata": {},
   "source": [
    "## 4. Design Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02dee84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252be8dc",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3db59a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/assignment/train_vgg19\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3be061eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165afd4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf39174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e179eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([char2idx['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        features = feature_extracter(img_tensor,True) ## 必須真的去extract features\n",
    "        features = tf.reshape(features,(features.shape[0], -1, features.shape[3])) ## reshape 成 7x7x1024\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1) ## training 時可以用teacher forcing（也就是把已知的training label當成input，就可以平行）\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = feature_extracter.trainable_variables + encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2af78c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 01:25:14.913092: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-16 01:25:14.936258: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2592000000 Hz\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_17734/1481518333.py:19 train_step  *\n        predictions, hidden, _ = decoder(dec_input, features, hidden)\n    /tmp/ipykernel_17734/2338771246.py:27 call  *\n        output, state = self.gru(x)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:668 __call__  **\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py:426 call\n        inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:868 _process_inputs\n        initial_state = self.get_initial_state(inputs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:650 get_initial_state\n        init_state = get_initial_state_fn(\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:1963 get_initial_state\n        return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:2998 _generate_zero_filled_state_for_cell\n        return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:3016 _generate_zero_filled_state\n        return create_zeros(state_size)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:3011 create_zeros\n        return array_ops.zeros(init_state_size, dtype=dtype)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2911 wrapped\n        tensor = fun(*args, **kwargs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2960 zeros\n        output = _constant_if_small(zero, shape, dtype, name)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2896 _constant_if_small\n        if np.prod(shape) < 1000:\n    <__array_function__ internals>:5 prod\n        \n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3030 prod\n        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87 _wrapreduction\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:867 __array__\n        raise NotImplementedError(\n\n    NotImplementedError: Cannot convert a symbolic Tensor (rnn__decoder/gru/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17734/3350017827.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {} {}/{} Train Loss {:.6f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_train_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    /tmp/ipykernel_17734/1481518333.py:19 train_step  *\n        predictions, hidden, _ = decoder(dec_input, features, hidden)\n    /tmp/ipykernel_17734/2338771246.py:27 call  *\n        output, state = self.gru(x)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:668 __call__  **\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py:426 call\n        inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:868 _process_inputs\n        initial_state = self.get_initial_state(inputs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:650 get_initial_state\n        init_state = get_initial_state_fn(\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:1963 get_initial_state\n        return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:2998 _generate_zero_filled_state_for_cell\n        return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:3016 _generate_zero_filled_state\n        return create_zeros(state_size)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:3011 create_zeros\n        return array_ops.zeros(init_state_size, dtype=dtype)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2911 wrapped\n        tensor = fun(*args, **kwargs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2960 zeros\n        output = _constant_if_small(zero, shape, dtype, name)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2896 _constant_if_small\n        if np.prod(shape) < 1000:\n    <__array_function__ internals>:5 prod\n        \n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3030 prod\n        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87 _wrapreduction\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n    /home/benny/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:867 __array__\n        raise NotImplementedError(\n\n    NotImplementedError: Cannot convert a symbolic Tensor (rnn__decoder/gru/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    ''' training part '''\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_train_loss += t_loss\n",
    "        print ('Epoch {} {}/{} Train Loss {:.6f}'.format(epoch + 1,batch+1,num_steps,total_train_loss/(batch+1)),end='\\r')\n",
    "    print('')\n",
    "    \n",
    "    ''' validation part '''\n",
    "    equal_num = 0\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    for (batch, (img_tensor, target)) in enumerate(val_dataset):\n",
    "        val_loss = 0\n",
    "        \n",
    "        hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "        dec_input = tf.expand_dims([character_to_idx['<start>']]*BATCH_SIZE, 1)\n",
    "        features = feature_extracter(img_tensor,False)\n",
    "        features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "        \n",
    "        ''' create batch大的result，第一個都是index = 27代表<start>，每decoder predict後就concat到裡面，最後在比對 '''\n",
    "        result = np.full((BATCH_SIZE, 1), 27) \n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            predicted_id = tf.argmax(predictions,axis=1).numpy() ## 機率最高的word對應的index\n",
    "            val_loss += loss_function(target[:, i], predictions)\n",
    "            result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "            dec_input = tf.expand_dims(predicted_id, 1) ## 這裡不是teacher forcing\n",
    "        target_array = target.numpy()\n",
    "        total_val_loss += (val_loss / int(target.shape[1]))\n",
    "        \n",
    "        ''' 對predict出來的結果，如果完全match ground truth 就++ '''\n",
    "        for i in range(BATCH_SIZE):\n",
    "            for j in range(max_len):\n",
    "                if result[i][j] == 28 and target_array[i][j] == 28:\n",
    "                    if (result[i][1:j] == target_array[i][1:j]).all():\n",
    "                        equal_num+=1\n",
    "                    break\n",
    "        print ('Validation Accuracy {:.6f}, Validation Loss {:.6f}'.format(float(equal_num)/((batch+1)*BATCH_SIZE),total_val_loss/(batch+1)),end='\\r')\n",
    "    \n",
    "    print('')\n",
    "\n",
    "\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    #if epoch % 5 == 0:\n",
    "    ckpt_manager.save()\n",
    "        \n",
    "    output_string = 'Epoch {} Train Loss {:.6f} Validation Accuracy {:.6f} Validation Loss {:.6f}\\n'.format(epoch + 1,\n",
    "                                                             total_loss/num_steps,float(equal_num)/20000.,total_val_loss/val_num_steps)\n",
    "    with open('./lab13-2_v4.log','a') as f:\n",
    "        f.write(output_string)\n",
    "    f.close()\n",
    "    print ('Epoch {} Train Loss {:.6f} Validation Accuracy {:.6f}'.format(epoch + 1,\n",
    "                                                             total_loss/num_steps,float(equal_num)/20000.))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615ca24",
   "metadata": {},
   "source": [
    "## 5. Do validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt.restore('./checkpoints/assignment/ckpt-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_num = 0\n",
    "total_val_loss = 0\n",
    "for (batch, (img_tensor, target)) in enumerate(val_dataset):\n",
    "    val_loss = 0\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([character_to_idx['<start>']]*BATCH_SIZE, 1)\n",
    "    features = feature_extracter(img_tensor,False)\n",
    "    features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "    result = np.full((BATCH_SIZE, 1), 27)\n",
    "    for i in range(1, target.shape[1]):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "        val_loss += loss_function(target[:, i], predictions)\n",
    "        result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "    target_array = target.numpy()\n",
    "    total_val_loss += (val_loss / int(target.shape[1]))\n",
    "    for i in range(BATCH_SIZE):\n",
    "        for j in range(max_len):\n",
    "            if result[i][j] == 28 and target_array[i][j] == 28:\n",
    "                if (result[i][1:j] == target_array[i][1:j]).all():\n",
    "                    equal_num+=1\n",
    "                break\n",
    "    print ('Validation Accuracy {:.6f}, Validation Loss {:.6f}'.format(float(equal_num)/((batch+1)*BATCH_SIZE),total_val_loss/(batch+1)),end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc3f6e",
   "metadata": {},
   "source": [
    "## 6. Predict Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54358d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_name = []\n",
    "\n",
    "for i in range(120000,140000):\n",
    "    test_img_name.append('a'+str(i))\n",
    "\n",
    "print(len(test_img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5272390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_image(image_name):\n",
    "    img = tf.io.read_file(IMAGE_DIR + image_name + '.png')\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    img = img/255 - 1.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86aa227",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_img_name)\n",
    "test_dataset = test_dataset.map(load_test_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "for batch, img_tensor in enumerate(test_dataset):\n",
    "    hidden = decoder.reset_state(batch_size=BATCH_SIZE)\n",
    "    dec_input = tf.expand_dims([character_to_idx['<start>']]*BATCH_SIZE, 1)\n",
    "    features = feature_extracter(img_tensor,False)\n",
    "    features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "    result = np.full((BATCH_SIZE, 1), 27)\n",
    "    for i in range(1, max_len):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "        result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        output_str = ''\n",
    "        num = num+1\n",
    "        hit = False\n",
    "        for j in range(1,max_len):\n",
    "            if result[i][j] == 28:\n",
    "                hit = True\n",
    "                break\n",
    "            else:\n",
    "                output_str = output_str + idx_to_character[result[i][j]]\n",
    "        if hit != True:\n",
    "            print(num)\n",
    "        with open('./Lab13-2_110062539.txt','a') as f:\n",
    "            f.write('a' + str(119999 + num) + ' ' + output_str+'\\n')\n",
    "        f.close()\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f15914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cbde30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
