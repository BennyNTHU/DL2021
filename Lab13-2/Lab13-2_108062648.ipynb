{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab13-2 Image Captioning\n",
    "110065508 李丞恩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER_PARAMETERS\n",
    "IMG_SIZE_W = 224\n",
    "IMG_SIZE_H = 448\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "BUFFER_SIZE = 5000\n",
    "\n",
    "embedding_dim = 128\n",
    "units = 64\n",
    "\n",
    "vocab_size = 29 # 26個英文字母 + <start>, <end>, <pad>\n",
    "max_length = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Reshape, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Select GPU number 1\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取spec\n",
    "---\n",
    "- 讀取圖片的Caption\n",
    "- 讀取圖片的路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "f = open(\"./spec_train_val.txt\", \"r\")\n",
    "\n",
    "captions = []\n",
    "img_path = []\n",
    "\n",
    "for line in f:\n",
    "    ss = line.strip('\\n').split(' ')\n",
    "    img_path.append(\"words_captcha\" + \"/\" + ss[0] + \".png\")\n",
    "    captions.append(ss[1])\n",
    "\n",
    "# Cloase the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割dataset為100000-20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 20000, 100000, 20000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training and validation sets using an 100000-20000 split\n",
    "img_path_train, img_path_val, captions_train, captions_val = train_test_split(img_path,\n",
    "                                                                              captions,\n",
    "                                                                              test_size=20000,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "len(img_path_train), len(img_path_val), len(captions_train), len(captions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER_PARAMETERS\n",
    "IMG_SIZE_W = 224\n",
    "IMG_SIZE_H = 448\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 5000\n",
    "\n",
    "embedding_dim = 128\n",
    "units = 64\n",
    "\n",
    "vocab_size = 29 # 26個英文字母 + <start>, <end>, <pad>\n",
    "max_length = 7\n",
    "\n",
    "num_steps = len(img_path_train) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define map_func\n",
    "---\n",
    "- 把剛剛讀的caption換成數字，a = 1，b = 2，依此類推。\n",
    "- pad   = 0\n",
    "- start = 27\n",
    "- end   = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_path, captions):\n",
    "    # load image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.dtypes.cast(img, tf.float32)\n",
    "    img = img / 255.0\n",
    "    img = tf.image.resize(img, (IMG_SIZE_W, IMG_SIZE_H))\n",
    "    \n",
    "    # process caption\n",
    "    cap = []\n",
    "    cap.append(27) # start = 27\n",
    "    for i in range(6):\n",
    "        number = 0 # empty = 0\n",
    "        if i < len(captions):\n",
    "            number = (captions[i]) - 96\n",
    "        elif i == len(captions):\n",
    "            number = 28\n",
    "\n",
    "        cap.append(np.int64(number))\n",
    "    \n",
    "    return img, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_path_train, captions_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int64]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "\n",
    "        # input = (batch_size, 224, 448, 3)\n",
    "\n",
    "        self.conv1_1 = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.conv1_2 = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.pool1   = MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        # pool1 = (batch_size, 112, 224, 32)\n",
    "\n",
    "        self.conv2_1 = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.conv2_2 = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.pool2   = MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        # pool2 = (batch_size, 56, 112, 64)\n",
    "\n",
    "        self.conv3_1 = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.conv3_2 = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.pool3   = MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        # pool3 = (batch_size, 28, 56, 128)\n",
    "\n",
    "        self.conv4_1 = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.conv4_2 = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.pool4   = MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        # pool4 = (batch_size, 14, 28, 256)\n",
    "        \n",
    "        self.conv5_1 = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.pool5   = MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        # pool5 = (batch_size, 7, 14, 512)\n",
    "\n",
    "        self.R = Reshape((-1, 256))\n",
    "        # reshape = (batch_size, 98, 512)\n",
    "        \n",
    "        self.E_1 = Dense(192          , activation='relu')\n",
    "        self.E_2 = Dense(embedding_dim, activation='relu')\n",
    "        # embedding = (batch_size, 98, embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.conv4_2(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.conv5_1(x)\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        x = self.R(x)\n",
    "        \n",
    "        x = self.E_1(x)\n",
    "        x = self.E_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer = optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img, target):\n",
    "    loss = 0\n",
    "             \n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([27] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:  \n",
    "                                \n",
    "        features = encoder(img)\n",
    "        \n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    ## 平均每個字預測的cross entrophy\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    \n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 開始train\n",
    "\n",
    "---\n",
    "- 燒顯卡的時候到了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.723897\n",
      "Time taken for 1 epoch 693.6380391120911 sec\n",
      "\n",
      "Epoch 2 Loss 1.452572\n",
      "Time taken for 1 epoch 615.5227978229523 sec\n",
      "\n",
      "Epoch 3 Loss 0.343950\n",
      "Time taken for 1 epoch 618.6903860569 sec\n",
      "\n",
      "Epoch 4 Loss 0.120944\n",
      "Time taken for 1 epoch 604.7163732051849 sec\n",
      "\n",
      "Epoch 5 Loss 0.040951\n",
      "Time taken for 1 epoch 606.2124140262604 sec\n",
      "\n",
      "Epoch 6 Loss 0.018616\n",
      "Time taken for 1 epoch 630.2027697563171 sec\n",
      "\n",
      "Epoch 7 Loss 0.016430\n",
      "Time taken for 1 epoch 588.1956033706665 sec\n",
      "\n",
      "Epoch 8 Loss 0.013663\n",
      "Time taken for 1 epoch 585.7850868701935 sec\n",
      "\n",
      "Epoch 9 Loss 0.015161\n",
      "Time taken for 1 epoch 589.6416227817535 sec\n",
      "\n",
      "Epoch 10 Loss 0.012006\n",
      "Time taken for 1 epoch 619.8796455860138 sec\n",
      "\n",
      "Epoch 11 Loss 0.008165\n",
      "Time taken for 1 epoch 619.4291632175446 sec\n",
      "\n",
      "Epoch 12 Loss 0.010550\n",
      "Time taken for 1 epoch 620.2097067832947 sec\n",
      "\n",
      "Epoch 13 Loss 0.009158\n",
      "Time taken for 1 epoch 616.1064252853394 sec\n",
      "\n",
      "Epoch 14 Loss 0.007547\n",
      "Time taken for 1 epoch 616.0037415027618 sec\n",
      "\n",
      "Epoch 15 Loss 0.016336\n",
      "Time taken for 1 epoch 621.6863689422607 sec\n",
      "\n",
      "Epoch 16 Loss 0.011969\n",
      "Time taken for 1 epoch 609.6521351337433 sec\n",
      "\n",
      "Epoch 17 Loss 0.005515\n",
      "Time taken for 1 epoch 600.8521728515625 sec\n",
      "\n",
      "Epoch 18 Loss 0.014737\n",
      "Time taken for 1 epoch 620.689553976059 sec\n",
      "\n",
      "Epoch 19 Loss 0.004883\n",
      "Time taken for 1 epoch 610.5207045078278 sec\n",
      "\n",
      "Epoch 20 Loss 0.007175\n",
      "Time taken for 1 epoch 603.1455116271973 sec\n",
      "\n",
      "Epoch 21 Loss 0.004974\n",
      "Time taken for 1 epoch 610.8635742664337 sec\n",
      "\n",
      "Epoch 22 Loss 0.007222\n",
      "Time taken for 1 epoch 605.0133905410767 sec\n",
      "\n",
      "Epoch 23 Loss 0.004843\n",
      "Time taken for 1 epoch 606.3980023860931 sec\n",
      "\n",
      "Epoch 24 Loss 0.016072\n",
      "Time taken for 1 epoch 621.6786308288574 sec\n",
      "\n",
      "Epoch 25 Loss 0.003912\n",
      "Time taken for 1 epoch 606.4353699684143 sec\n",
      "\n",
      "Epoch 26 Loss 0.004873\n",
      "Time taken for 1 epoch 602.7655863761902 sec\n",
      "\n",
      "Epoch 27 Loss 0.003715\n",
      "Time taken for 1 epoch 604.6978766918182 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_plot = []\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(img_path):\n",
    "    # load image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.dtypes.cast(img, tf.float32)\n",
    "    img = img / 255.0\n",
    "    img = tf.image.resize(img, (IMG_SIZE_W, IMG_SIZE_H))\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    \n",
    "    hidden = decoder.reset_state(batch_size = 1)\n",
    "    features = encoder(img)\n",
    "    dec_input = tf.expand_dims([27], 0)\n",
    "    result = \"\"\n",
    "    \n",
    "    for i in range(1, max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        if predicted_id >= 1 and predicted_id <= 26:\n",
    "            result = result + chr(predicted_id + 96)\n",
    "\n",
    "        if predicted_id == 28:\n",
    "            return result\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "---\n",
    "- Accuracy > 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = 0\n",
    "for (index, img_path) in enumerate(img_path_val):\n",
    "    predict = evaluate(img_path)\n",
    "    if predict == captions_val[index]:\n",
    "        match = match + 1\n",
    "    else:\n",
    "        print(predict, captions_val[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", (match / len(captions_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 輸出檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Lab13-2_108062648.txt', 'w') as f:\n",
    "    for i in range(120000, 140000):\n",
    "        name = 'a' + str(i)\n",
    "        path = './words_captcha/'+ name + '.png'\n",
    "        result = evaluate(path)\n",
    "        \n",
    "        f.write(name + ' ' + result +'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
